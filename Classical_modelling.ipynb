{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of words - tf-idf - keyword extraction techniques**"
      ],
      "metadata": {
        "id": "KTlvPwVlGBrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "DERIpveLUFWR"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQRIR7-TUYA7",
        "outputId": "2e672046-9ca7-477d-acfc-06f3454be4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "YzUrLGxZUFWT",
        "outputId": "236d5fea-39b5-465d-d983-3c990b0062e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  source_id  year  \\\n",
              "0           0             0         27  1987   \n",
              "1           1             1         63  1987   \n",
              "2           2             2         60  1987   \n",
              "3           3             3         59  1987   \n",
              "4           4             4         69  1987   \n",
              "\n",
              "                                               title  \\\n",
              "0                         Bit-Serial Neural Networks   \n",
              "1                        Connectivity Versus Entropy   \n",
              "2        The Hopfield Model with Multi-Level Neurons   \n",
              "3                               How Neural Nets Work   \n",
              "4  Spatial Organization of Neural Networks: A Pro...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  a bit serial vlsi neural network is described ...   \n",
              "1  how does the connectivity of a neural network ...   \n",
              "2  the hopfield neural network model for associat...   \n",
              "3  there is presently great interest in the abili...   \n",
              "4  the aim of this paper is to explore the spatia...   \n",
              "\n",
              "                                           full_text  \n",
              "0  bit serial neural networks alan f murray antho...  \n",
              "1  connectivity versus entropy yaser s abu mostaf...  \n",
              "2  the hopfield model with mul ti level neurons m...  \n",
              "3  alan lapedes robert farber theoretical divisio...  \n",
              "4  spatial organization of neural nen orks a prob...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f10bbb4-153e-4e61-8244-06005f45a007\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>source_id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1987</td>\n",
              "      <td>Bit-Serial Neural Networks</td>\n",
              "      <td>a bit serial vlsi neural network is described ...</td>\n",
              "      <td>bit serial neural networks alan f murray antho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>63</td>\n",
              "      <td>1987</td>\n",
              "      <td>Connectivity Versus Entropy</td>\n",
              "      <td>how does the connectivity of a neural network ...</td>\n",
              "      <td>connectivity versus entropy yaser s abu mostaf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>1987</td>\n",
              "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
              "      <td>the hopfield neural network model for associat...</td>\n",
              "      <td>the hopfield model with mul ti level neurons m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>59</td>\n",
              "      <td>1987</td>\n",
              "      <td>How Neural Nets Work</td>\n",
              "      <td>there is presently great interest in the abili...</td>\n",
              "      <td>alan lapedes robert farber theoretical divisio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>69</td>\n",
              "      <td>1987</td>\n",
              "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
              "      <td>the aim of this paper is to explore the spatia...</td>\n",
              "      <td>spatial organization of neural nen orks a prob...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f10bbb4-153e-4e61-8244-06005f45a007')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f10bbb4-153e-4e61-8244-06005f45a007 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f10bbb4-153e-4e61-8244-06005f45a007');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/ai-5/papers_with_abstract_cleaned.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOP7vVEUFWT",
        "outputId": "46954502-1d04-4794-be18-4cb07be96965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9312 entries, 0 to 9311\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Unnamed: 0    9312 non-null   int64 \n",
            " 1   Unnamed: 0.1  9312 non-null   int64 \n",
            " 2   source_id     9312 non-null   int64 \n",
            " 3   year          9312 non-null   int64 \n",
            " 4   title         9312 non-null   object\n",
            " 5   abstract      9311 non-null   object\n",
            " 6   full_text     9304 non-null   object\n",
            "dtypes: int64(4), object(3)\n",
            "memory usage: 509.4+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJK8ngHNUFWU",
        "outputId": "519e7f08-0026-4900-fdc4-c803c929b5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'TITLE:Threshold Bandits, With and Without Censored Feedback'\n",
            "('ABSTRACT:we consider the emph threshold bandit setting a variant of the '\n",
            " 'classical multi armed bandit problem in which the reward on each round '\n",
            " 'depends on a piece of side information known as a emph threshold value the '\n",
            " 'learner selects one of k actions arms this action generates a random sample '\n",
            " 'from a fixed distribution and the action then receives a unit payoff in the '\n",
            " 'event that this sample exceeds the threshold value we consider two versions '\n",
            " 'of this problem the emph uncensored and emph censored case that determine '\n",
            " 'whether the sample is always observed or only when the threshold is not met '\n",
            " 'using new tools to understand the popular ucb algorithm we show that the '\n",
            " 'uncensored case is essentially no more difficult than the classical multi '\n",
            " 'armed bandit setting finally we show that the censored case exhibits more '\n",
            " 'challenges but we give guarantees in the event that the sequence of '\n",
            " 'threshold values is generated optimistically ')\n",
            "('FULL TEXT:threshold bandit with and without censored feedback jacob '\n",
            " 'abernethy kareem amin department of computer science department of computer '\n",
            " 'science university of michigan ann arbor mi jabernet umich edu university of '\n",
            " 'michigan ann arbor mi amkareem umich edu ruihao zhu aeroastro csail mit '\n",
            " 'cambridge ma rzhu mit edu abstract we consider the threshold bandit setting '\n",
            " 'a variant of the classical multi armed bandit problem in which the reward on '\n",
            " 'each round depends on a piece of side information known as a threshold value '\n",
            " 'the learner selects one of k actions arms this action generates a random '\n",
            " 'sample from a xed distribution and the action then receives a unit payoff in '\n",
            " 'the event that this sample exceeds the threshold value we consider two '\n",
            " 'versions of this problem the uncensored and censored case that determine '\n",
            " 'whether the sample is always observed or only when the threshold is not met '\n",
            " 'using new tools to understand the popular ucb algorithm we show that the '\n",
            " 'uncensored case is essentially no more dif c')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "sample = 5657\n",
        "pprint.pprint(\"TITLE:{}\".format(df['title'][sample]))\n",
        "pprint.pprint(\"ABSTRACT:{}\".format(df['abstract'][sample]))\n",
        "pprint.pprint(\"FULL TEXT:{}\".format(df['full_text'][sample][:1000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s01B22vkUFWU"
      },
      "source": [
        "This dataset contains 5 columns: source_id, year, title,  title, abstract and full_text. We are mostly interested in the full_text which include both title and abstract."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWJVQXuwUFWV"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyrKVpstVXIQ",
        "outputId": "ff14abf9-c844-4133-80e0-649c2aeefbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "661a7f8e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "stop_words = list(stop_words.union(new_words))\n",
        "\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "    \n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df['full_text'].sample(100).apply(lambda x:pre_process(x))"
      ],
      "metadata": {
        "id": "vB5R4pWAY6vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIHF_qkvg8Gq",
        "outputId": "6535a7c1-6bbf-48ab-cb11-db01f2bd3e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9036    quadratic video interpolation xiangyu carnegie...\n",
              "5727    active learning oracle epiphany tzu kuo huang ...\n",
              "1135    competitive line linear regression vovk pepart...\n",
              "8512    loaded dice trading bias variance order score ...\n",
              "8650    complexity simplicity adaptive active subspace...\n",
              "                              ...                        \n",
              "4627    ocsvm quantile estimator high dimensional dist...\n",
              "1807    information geometric decomposition spike anal...\n",
              "1568    support vector novelty detection applied jet e...\n",
              "8014    copula high dimensional generative model vine ...\n",
              "7781    direct estimation difference causal graph yuha...\n",
              "Name: full_text, Length: 100, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of words"
      ],
      "metadata": {
        "id": "-0mn8NR4pwqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgaTfrmtSB4b",
        "outputId": "e28cd0bd-d1a6-4cb5-fdab-f562355b5a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2count = {}\n",
        "for data in docs:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "metadata": {
        "id": "q8SrGVXJpviy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o2A2mVfrqRa",
        "outputId": "3575a83b-3b9e-4e94-e429-7bd7a5f6711a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quadratic': 108,\n",
              " 'video': 185,\n",
              " 'interpolation': 82,\n",
              " 'xiangyu': 9,\n",
              " 'carnegie': 18,\n",
              " 'mellon': 18,\n",
              " 'university': 234,\n",
              " 'xuxiangyu': 1,\n",
              " 'gmail': 9,\n",
              " 'com': 113,\n",
              " 'siyao': 1,\n",
              " 'sensetime': 4,\n",
              " 'research': 233,\n",
              " 'wenxiu': 1,\n",
              " 'sun': 80,\n",
              " 'lisiyao': 1,\n",
              " 'sunwenxiu': 1,\n",
              " 'qian': 1,\n",
              " 'yin': 5,\n",
              " 'beijing': 1,\n",
              " 'normal': 92,\n",
              " 'yinqian': 1,\n",
              " 'bnu': 1,\n",
              " 'edu': 144,\n",
              " 'ming': 7,\n",
              " 'hsuan': 3,\n",
              " 'yang': 70,\n",
              " 'california': 21,\n",
              " 'merced': 1,\n",
              " 'google': 34,\n",
              " 'mhyang': 1,\n",
              " 'ucmerced': 1,\n",
              " 'abstract': 106,\n",
              " 'important': 150,\n",
              " 'problem': 1045,\n",
              " 'computer': 267,\n",
              " 'vision': 176,\n",
              " 'help': 53,\n",
              " 'overcome': 16,\n",
              " 'temporal': 111,\n",
              " 'limitation': 47,\n",
              " 'camera': 44,\n",
              " 'sensor': 34,\n",
              " 'existing': 133,\n",
              " 'method': 1354,\n",
              " 'usually': 91,\n",
              " 'assume': 228,\n",
              " 'uniform': 112,\n",
              " 'motion': 157,\n",
              " 'consecutive': 19,\n",
              " 'frame': 205,\n",
              " 'use': 699,\n",
              " 'linear': 544,\n",
              " 'model': 2262,\n",
              " 'can': 106,\n",
              " 'not': 106,\n",
              " 'well': 380,\n",
              " 'approximate': 189,\n",
              " 'complex': 139,\n",
              " 'real': 242,\n",
              " 'world': 92,\n",
              " 'address': 65,\n",
              " 'issue': 63,\n",
              " 'propose': 202,\n",
              " 'exploit': 55,\n",
              " 'acceleration': 34,\n",
              " 'information': 803,\n",
              " 'allows': 132,\n",
              " 'prediction': 286,\n",
              " 'curvilinear': 4,\n",
              " 'trajectory': 95,\n",
              " 'variable': 349,\n",
              " 'velocity': 21,\n",
              " 'generates': 30,\n",
              " 'accurate': 59,\n",
              " 'result': 802,\n",
              " 'high': 313,\n",
              " 'quality': 118,\n",
              " 'synthesis': 38,\n",
              " 'develop': 53,\n",
              " 'reversal': 16,\n",
              " 'layer': 392,\n",
              " 'estimate': 365,\n",
              " 'eld': 241,\n",
              " 'starting': 56,\n",
              " 'unknown': 105,\n",
              " 'target': 179,\n",
              " 'source': 158,\n",
              " 'addition': 92,\n",
              " 'present': 196,\n",
              " 'technique': 189,\n",
              " 'nement': 10,\n",
              " 'extensive': 26,\n",
              " 'experiment': 479,\n",
              " 'demonstrate': 84,\n",
              " 'approach': 513,\n",
              " 'performs': 61,\n",
              " 'favorably': 6,\n",
              " 'wide': 51,\n",
              " 'variety': 49,\n",
              " 'datasets': 230,\n",
              " 'introduction': 138,\n",
              " 'aim': 56,\n",
              " 'synthesize': 13,\n",
              " 'intermediate': 38,\n",
              " 'original': 171,\n",
              " 'input': 554,\n",
              " 'image': 317,\n",
              " 'temporally': 7,\n",
              " 'upsample': 2,\n",
              " 'low': 288,\n",
              " 'rate': 489,\n",
              " 'higher': 198,\n",
              " 'fundamental': 24,\n",
              " 'used': 648,\n",
              " 'numerous': 15,\n",
              " 'application': 296,\n",
              " 'deblurring': 3,\n",
              " 'editing': 1,\n",
              " 'virtual': 15,\n",
              " 'reality': 5,\n",
              " 'medical': 22,\n",
              " 'imaging': 16,\n",
              " 'state': 597,\n",
              " 'art': 119,\n",
              " 'explicitly': 47,\n",
              " 'implicitly': 13,\n",
              " 'object': 248,\n",
              " 'move': 77,\n",
              " 'along': 111,\n",
              " 'straight': 11,\n",
              " 'line': 218,\n",
              " 'constant': 305,\n",
              " 'speed': 140,\n",
              " 'adopt': 32,\n",
              " 'synthesizing': 5,\n",
              " 'however': 459,\n",
              " 'scenario': 34,\n",
              " 'non': 472,\n",
              " 'assumption': 333,\n",
              " 'may': 349,\n",
              " 'always': 85,\n",
              " 'hold': 142,\n",
              " 'often': 124,\n",
              " 'lead': 129,\n",
              " 'inaccurate': 10,\n",
              " 'moreover': 65,\n",
              " 'mainly': 24,\n",
              " 'developed': 64,\n",
              " 'based': 671,\n",
              " 'order': 519,\n",
              " 'exploited': 11,\n",
              " 'effective': 85,\n",
              " 'algorithm': 1768,\n",
              " 'additional': 118,\n",
              " 'end': 177,\n",
              " 'speci': 263,\n",
              " 'cally': 91,\n",
              " 'data': 1524,\n",
              " 'driven': 22,\n",
              " 'integrates': 9,\n",
              " 'convolutional': 143,\n",
              " 'neural': 1018,\n",
              " 'network': 1568,\n",
              " 'cnns': 46,\n",
              " 'estimation': 374,\n",
              " 'proposed': 361,\n",
              " 'aware': 20,\n",
              " 'thus': 298,\n",
              " 'equal': 133,\n",
              " 'contribution': 88,\n",
              " 'corresponding': 194,\n",
              " 'author': 81,\n",
              " 'conference': 444,\n",
              " 'processing': 347,\n",
              " 'system': 662,\n",
              " 'neurips': 59,\n",
              " 'vancouver': 26,\n",
              " 'canada': 41,\n",
              " 'exploiting': 18,\n",
              " 'leftmost': 4,\n",
              " 'sub': 79,\n",
              " 'gure': 29,\n",
              " 'show': 295,\n",
              " 'describing': 8,\n",
              " 'projectile': 1,\n",
              " 'football': 1,\n",
              " 'gures': 17,\n",
              " 'interpolated': 16,\n",
              " 'different': 610,\n",
              " 'note': 342,\n",
              " 'overlap': 15,\n",
              " 'better': 239,\n",
              " 'visualizing': 9,\n",
              " 'since': 386,\n",
              " 'assumes': 21,\n",
              " 'movement': 19,\n",
              " 'contrast': 103,\n",
              " 'neighboring': 16,\n",
              " 'generate': 102,\n",
              " 'although': 135,\n",
              " 'idea': 119,\n",
              " 'intuitive': 18,\n",
              " 'sensible': 5,\n",
              " 'task': 436,\n",
              " 'challenging': 61,\n",
              " 'need': 168,\n",
              " 'backward': 44,\n",
              " 'easily': 61,\n",
              " 'obtained': 198,\n",
              " 'effectively': 31,\n",
              " 'convert': 7,\n",
              " 'forward': 43,\n",
              " 'introduce': 119,\n",
              " 'new': 415,\n",
              " 'ltering': 64,\n",
              " 'estimated': 162,\n",
              " 'map': 244,\n",
              " 'shown': 355,\n",
              " 'pixel': 74,\n",
              " 'obtain': 127,\n",
              " 'work': 563,\n",
              " 'summarized': 20,\n",
              " 'follows': 166,\n",
              " 'first': 190,\n",
              " 'inter': 22,\n",
              " 'polation': 1,\n",
              " 'nonlinear': 77,\n",
              " 'second': 305,\n",
              " 'thereby': 21,\n",
              " 'facilitating': 6,\n",
              " 'novel': 109,\n",
              " 'ning': 19,\n",
              " 'focus': 97,\n",
              " 'function': 1522,\n",
              " 'framework': 201,\n",
              " 'general': 288,\n",
              " 'extended': 47,\n",
              " 'related': 165,\n",
              " 'typical': 44,\n",
              " 'example': 713,\n",
              " 'baker': 2,\n",
              " 'optical': 48,\n",
              " 'warping': 48,\n",
              " 'linearly': 42,\n",
              " 'liu': 101,\n",
              " 'cnn': 71,\n",
              " 'directly': 135,\n",
              " 'learn': 203,\n",
              " 'interpolating': 3,\n",
              " 'middle': 26,\n",
              " 'similarly': 71,\n",
              " 'jiang': 15,\n",
              " 'enables': 24,\n",
              " 'multi': 141,\n",
              " 'hand': 115,\n",
              " 'meyer': 3,\n",
              " 'phase': 82,\n",
              " 'combine': 40,\n",
              " 'across': 160,\n",
              " 'level': 261,\n",
              " 'scale': 281,\n",
              " 'pyramid': 9,\n",
              " 'modeled': 32,\n",
              " 'time': 1245,\n",
              " 'implicit': 19,\n",
              " 'le': 172,\n",
              " 'kernel': 265,\n",
              " 'constrained': 74,\n",
              " 'scheme': 105,\n",
              " 'handle': 39,\n",
              " 'visual': 176,\n",
              " 'closely': 37,\n",
              " 'mcallister': 2,\n",
              " 'roulier': 2,\n",
              " 'us': 117,\n",
              " 'spline': 7,\n",
              " 'preserve': 27,\n",
              " 'convexity': 66,\n",
              " 'applied': 201,\n",
              " 'dimensional': 290,\n",
              " 'solve': 98,\n",
              " 'much': 182,\n",
              " 'dimension': 207,\n",
              " 'ground': 82,\n",
              " 'truthlinear': 1,\n",
              " 'modelquadratic': 1,\n",
              " 'overview': 28,\n",
              " 'rst': 519,\n",
              " 'shelf': 9,\n",
              " 'describe': 75,\n",
              " 'process': 462,\n",
              " 'detail': 160,\n",
              " 'paper': 307,\n",
              " 'computed': 135,\n",
              " 'finally': 110,\n",
              " 'fusing': 7,\n",
              " 'inaccurately': 1,\n",
              " 'predicting': 37,\n",
              " 'scene': 74,\n",
              " 'warped': 37,\n",
              " 'warp': 7,\n",
              " 'part': 118,\n",
              " 'following': 466,\n",
              " 'section': 470,\n",
              " 'side': 66,\n",
              " 'performed': 90,\n",
              " 'compute': 167,\n",
              " 'pwc': 3,\n",
              " 'net': 110,\n",
              " 'predict': 100,\n",
              " 'reversing': 3,\n",
              " 'interpolate': 5,\n",
              " 'consider': 299,\n",
              " 'cid': 6924,\n",
              " 'denotes': 108,\n",
              " 'displacement': 3,\n",
              " 'represents': 91,\n",
              " 'set': 1182,\n",
              " 'rewritten': 11,\n",
              " 'travel': 5,\n",
              " 'take': 177,\n",
              " 'consideration': 14,\n",
              " 'correspondingly': 6,\n",
              " 'derived': 69,\n",
              " 'equivalent': 77,\n",
              " 'iiii': 2,\n",
              " 'flow': 50,\n",
              " 'estimationquadratic': 1,\n",
              " 'predictionquadratic': 1,\n",
              " 'predictionflow': 1,\n",
              " 'reversalflow': 1,\n",
              " 'reversalsynthesisti': 1,\n",
              " 'effectiveness': 47,\n",
              " 'adaptive': 166,\n",
              " 'car': 41,\n",
              " 'moving': 21,\n",
              " 'arrow': 25,\n",
              " 'direction': 145,\n",
              " 'sequence': 347,\n",
              " 'naive': 38,\n",
              " 'strategy': 117,\n",
              " 'generated': 175,\n",
              " 'represent': 89,\n",
              " 'deep': 354,\n",
              " 'respectively': 174,\n",
              " 'formulation': 96,\n",
              " 'relaxes': 2,\n",
              " 'constraint': 171,\n",
              " 'rectilinear': 1,\n",
              " 'accelerated': 62,\n",
              " 'closest': 14,\n",
              " 'whereas': 71,\n",
              " 'naturally': 38,\n",
              " 'instead': 147,\n",
              " 'simple': 216,\n",
              " 'perform': 133,\n",
              " 'around': 64,\n",
              " 'boundary': 49,\n",
              " 'importantly': 26,\n",
              " 'project': 49,\n",
              " 'corresponds': 75,\n",
              " 'next': 168,\n",
              " 'averaging': 23,\n",
              " 'projected': 42,\n",
              " 'value': 661,\n",
              " 'fall': 25,\n",
              " 'neighborhood': 26,\n",
              " 'mathematically': 10,\n",
              " 'written': 42,\n",
              " 'gaussian': 336,\n",
              " 'weight': 460,\n",
              " 'conceptually': 7,\n",
              " 'similar': 321,\n",
              " 'surface': 22,\n",
              " 'splatting': 2,\n",
              " 'graphic': 8,\n",
              " 'replaced': 27,\n",
              " 'projection': 151,\n",
              " 'training': 869,\n",
              " 'learnable': 23,\n",
              " 'parameter': 793,\n",
              " 'differentiable': 63,\n",
              " 'gradient': 692,\n",
              " 'backpropagated': 1,\n",
              " 'module': 87,\n",
              " 'whole': 50,\n",
              " 'hole': 2,\n",
              " 'mostly': 16,\n",
              " 'due': 223,\n",
              " 'visible': 10,\n",
              " 'occluded': 8,\n",
              " 'missing': 69,\n",
              " 'lled': 2,\n",
              " 'reversed': 3,\n",
              " 'ned': 252,\n",
              " 'fusion': 20,\n",
              " 'still': 122,\n",
              " 'ringing': 1,\n",
              " 'artifact': 17,\n",
              " 'edge': 135,\n",
              " 'outlier': 39,\n",
              " 'straightforward': 34,\n",
              " 'way': 200,\n",
              " 'reduce': 69,\n",
              " 'train': 160,\n",
              " 'residual': 98,\n",
              " 'connection': 122,\n",
              " 'initial': 102,\n",
              " 'practice': 106,\n",
              " 'thin': 3,\n",
              " 'streak': 1,\n",
              " 'spike': 103,\n",
              " 'removed': 22,\n",
              " 'weighted': 78,\n",
              " 'convolution': 81,\n",
              " 'affected': 8,\n",
              " 'spiky': 1,\n",
              " 'inspired': 32,\n",
              " 'median': 33,\n",
              " 'lter': 76,\n",
              " 'sample': 237,\n",
              " 'avoids': 9,\n",
              " 'adaptively': 23,\n",
              " 'removing': 39,\n",
              " 'classical': 46,\n",
              " 'involves': 30,\n",
              " 'indifferentiable': 1,\n",
              " 'operation': 145,\n",
              " 'trained': 261,\n",
              " 'learns': 52,\n",
              " 'formulate': 25,\n",
              " 'ltered': 5,\n",
              " 'learned': 210,\n",
              " 'sampling': 295,\n",
              " 'offset': 24,\n",
              " 'constrain': 8,\n",
              " 'tanh': 24,\n",
              " 'activation': 60,\n",
              " 'local': 304,\n",
              " 'receptive': 100,\n",
              " 'sparse': 171,\n",
              " 'smooth': 115,\n",
              " 'region': 146,\n",
              " 'rectify': 3,\n",
              " 'rely': 38,\n",
              " 'suitable': 44,\n",
              " 'location': 71,\n",
              " 'improvement': 107,\n",
              " 'spatially': 13,\n",
              " 'variant': 74,\n",
              " 'could': 160,\n",
              " 'seen': 77,\n",
              " 'spirit': 10,\n",
              " 'implementation': 122,\n",
              " 'presented': 113,\n",
              " 'bilinear': 26,\n",
              " 'mask': 5,\n",
              " 'fuse': 8,\n",
              " 'distance': 231,\n",
              " 'give': 175,\n",
              " 'con': 117,\n",
              " 'dence': 26,\n",
              " 'closer': 27,\n",
              " 'almost': 60,\n",
              " 'content': 15,\n",
              " 'found': 178,\n",
              " 'step': 660,\n",
              " 'manner': 54,\n",
              " 'loss': 475,\n",
              " 'combination': 92,\n",
              " 'perceptual': 7,\n",
              " 'truth': 62,\n",
              " 'conv': 90,\n",
              " 'feature': 583,\n",
              " 'extractor': 5,\n",
              " 'vgg': 14,\n",
              " 'provide': 146,\n",
              " 'including': 115,\n",
              " 'structure': 302,\n",
              " 'hyper': 12,\n",
              " 'evaluation': 127,\n",
              " 'comparison': 176,\n",
              " 'code': 72,\n",
              " 'available': 119,\n",
              " 'http': 106,\n",
              " 'site': 17,\n",
              " 'view': 253,\n",
              " 'xiangyuxu': 1,\n",
              " 'qvi': 1,\n",
              " 'nip': 122,\n",
              " 'collect': 16,\n",
              " 'internet': 13,\n",
              " 'fps': 14,\n",
              " 'collected': 35,\n",
              " 'select': 52,\n",
              " 'clip': 28,\n",
              " 'shake': 2,\n",
              " 'dynamic': 219,\n",
              " 'bene': 44,\n",
              " 'cial': 111,\n",
              " 'nal': 87,\n",
              " 'dataset': 380,\n",
              " 'consists': 89,\n",
              " 'total': 126,\n",
              " 'randomly': 72,\n",
              " 'downsampled': 23,\n",
              " 'augmentation': 21,\n",
              " 'extract': 44,\n",
              " 'overlapped': 7,\n",
              " 'group': 170,\n",
              " 'resize': 2,\n",
              " 'crop': 8,\n",
              " 'patch': 60,\n",
              " 'ipping': 10,\n",
              " 'fully': 87,\n",
              " 'utilize': 14,\n",
              " 'encoder': 57,\n",
              " 'decoder': 125,\n",
              " 'composed': 20,\n",
              " 'average': 254,\n",
              " 'pooling': 50,\n",
              " 'table': 300,\n",
              " 'quantitative': 25,\n",
              " 'gopro': 10,\n",
              " 'adobe': 7,\n",
              " 'qua': 8,\n",
              " 'without': 240,\n",
              " 'center': 72,\n",
              " 'psnr': 8,\n",
              " 'ssim': 8,\n",
              " 'dvf': 13,\n",
              " 'sepconv': 8,\n",
              " 'superslomo': 8,\n",
              " 'dowsampling': 1,\n",
              " 'upsampling': 4,\n",
              " 'add': 72,\n",
              " 'skip': 11,\n",
              " 'wise': 54,\n",
              " 'summation': 11,\n",
              " 'resolution': 39,\n",
              " 'jointly': 24,\n",
              " 'concatenation': 10,\n",
              " 'produce': 86,\n",
              " 'output': 369,\n",
              " 'feed': 15,\n",
              " 'nally': 16,\n",
              " 'xed': 147,\n",
              " 'epoch': 78,\n",
              " 'netune': 1,\n",
              " 'another': 132,\n",
              " 'adam': 40,\n",
              " 'optimizer': 6,\n",
              " 'initialize': 38,\n",
              " 'learning': 1681,\n",
              " 'decrease': 75,\n",
              " 'factor': 163,\n",
              " 'trade': 47,\n",
              " 'standard': 265,\n",
              " 'deviation': 79,\n",
              " 'report': 101,\n",
              " 'structural': 37,\n",
              " 'similarity': 66,\n",
              " 'index': 92,\n",
              " 'error': 580,\n",
              " 'root': 57,\n",
              " 'mean': 506,\n",
              " 'squared': 65,\n",
              " 'rms': 5,\n",
              " 'difference': 189,\n",
              " 'reference': 146,\n",
              " 'evaluate': 92,\n",
              " 'separable': 59,\n",
              " 'voxel': 19,\n",
              " 'retrain': 7,\n",
              " 'able': 126,\n",
              " 'publicly': 12,\n",
              " 'arbitrary': 82,\n",
              " 'evaluated': 61,\n",
              " 'conduct': 20,\n",
              " 'ucf': 10,\n",
              " 'davis': 32,\n",
              " 'performance': 582,\n",
              " 'single': 260,\n",
              " 'recorded': 25,\n",
              " 'held': 33,\n",
              " 'contain': 40,\n",
              " 'indoor': 8,\n",
              " 'outdoor': 3,\n",
              " 'length': 119,\n",
              " 'correspond': 40,\n",
              " 'discussed': 63,\n",
              " 'baseline': 141,\n",
              " 'separately': 39,\n",
              " 'score': 144,\n",
              " 'denoted': 65,\n",
              " 'consistently': 35,\n",
              " 'noticeably': 3,\n",
              " 'qualitative': 39,\n",
              " 'row': 129,\n",
              " 'clearer': 2,\n",
              " 'indicates': 70,\n",
              " 'point': 642,\n",
              " 'tracking': 24,\n",
              " 'psnrs': 1,\n",
              " 'either': 94,\n",
              " 'improve': 102,\n",
              " 'best': 277,\n",
              " 'understand': 29,\n",
              " 'visualize': 20,\n",
              " 'compare': 114,\n",
              " 'qualitatively': 18,\n",
              " 'quantitatively': 5,\n",
              " 'test': 560,\n",
              " 'classic': 12,\n",
              " 'track': 19,\n",
              " 'synthesized': 7,\n",
              " 'exclude': 3,\n",
              " 'disappear': 3,\n",
              " 'connecting': 9,\n",
              " 'red': 51,\n",
              " 'quite': 30,\n",
              " 'sharp': 17,\n",
              " 'curve': 105,\n",
              " 'sudden': 2,\n",
              " 'violent': 1,\n",
              " 'change': 182,\n",
              " 'fail': 24,\n",
              " 'approximates': 13,\n",
              " 'predicted': 31,\n",
              " 'accuracy': 184,\n",
              " 'severely': 5,\n",
              " 'blurred': 3,\n",
              " 'demonstrates': 26,\n",
              " 'shift': 30,\n",
              " 'align': 16,\n",
              " 'smaller': 93,\n",
              " 'contains': 110,\n",
              " 'severe': 6,\n",
              " 'importance': 44,\n",
              " 'strictly': 26,\n",
              " 'satisfying': 49,\n",
              " 'minor': 6,\n",
              " 'per': 185,\n",
              " 'turbations': 2,\n",
              " 'strict': 11,\n",
              " 'obvious': 16,\n",
              " 'asfp': 3,\n",
              " 'replace': 28,\n",
              " 'drastically': 9,\n",
              " 'measure': 203,\n",
              " 'metric': 163,\n",
              " 'asf': 1,\n",
              " 'position': 83,\n",
              " 'ith': 28,\n",
              " 'number': 820,\n",
              " 'signi': 163,\n",
              " 'cantly': 72,\n",
              " 'reducing': 35,\n",
              " 'resized': 4,\n",
              " 'testing': 140,\n",
              " 'setting': 360,\n",
              " 'previously': 49,\n",
              " 'triplet': 9,\n",
              " 'downsampling': 5,\n",
              " 'extracting': 16,\n",
              " 'adjacent': 24,\n",
              " 'static': 31,\n",
              " 'quintuple': 2,\n",
              " 'divided': 33,\n",
              " 'converting': 6,\n",
              " 'slightly': 51,\n",
              " 'relatively': 41,\n",
              " 'slow': 39,\n",
              " 'outperforms': 61,\n",
              " 'term': 438,\n",
              " 'ada': 5,\n",
              " 'reduces': 59,\n",
              " 'overall': 61,\n",
              " 'achieves': 80,\n",
              " 'experimental': 113,\n",
              " 'ablation': 13,\n",
              " 'study': 238,\n",
              " 'analyze': 44,\n",
              " 'component': 291,\n",
              " 'ble': 6,\n",
              " 'particular': 182,\n",
              " 'impact': 33,\n",
              " 'replacing': 17,\n",
              " 'predic': 3,\n",
              " 'tion': 88,\n",
              " 'ment': 12,\n",
              " 'simply': 77,\n",
              " 'rev': 5,\n",
              " 'degrades': 3,\n",
              " 'particularly': 31,\n",
              " 'play': 40,\n",
              " 'crucial': 20,\n",
              " 'role': 48,\n",
              " 'veri': 16,\n",
              " 'small': 291,\n",
              " 'generating': 71,\n",
              " 'full': 110,\n",
              " 'conclusion': 77,\n",
              " 'facilitates': 6,\n",
              " 'accurately': 39,\n",
              " 'favorable': 3,\n",
              " 'even': 234,\n",
              " 'cubic': 5,\n",
              " 'expect': 54,\n",
              " 'anderson': 12,\n",
              " 'gallup': 1,\n",
              " 'barron': 3,\n",
              " 'kontkanen': 1,\n",
              " 'snavely': 1,\n",
              " 'hern': 4,\n",
              " 'ndez': 5,\n",
              " 'agarwal': 13,\n",
              " 'seitz': 2,\n",
              " 'jump': 11,\n",
              " 'acm': 108,\n",
              " 'transaction': 117,\n",
              " 'tog': 3,\n",
              " 'scharstein': 1,\n",
              " 'lewis': 11,\n",
              " 'roth': 10,\n",
              " 'black': 58,\n",
              " 'szeliski': 3,\n",
              " 'database': 80,\n",
              " 'methodology': 25,\n",
              " 'ijcv': 9,\n",
              " 'bao': 2,\n",
              " 'lai': 6,\n",
              " 'zhang': 163,\n",
              " 'gao': 24,\n",
              " 'memc': 1,\n",
              " 'compensation': 14,\n",
              " 'enhancement': 6,\n",
              " 'arxiv': 451,\n",
              " 'fleet': 1,\n",
              " 'beauchemin': 1,\n",
              " 'brook': 10,\n",
              " 'blur': 4,\n",
              " 'cvpr': 135,\n",
              " 'gan': 89,\n",
              " 'lin': 50,\n",
              " 'monocular': 7,\n",
              " 'depth': 112,\n",
              " 'nity': 21,\n",
              " 'vertical': 24,\n",
              " 'label': 178,\n",
              " 'eccv': 23,\n",
              " 'gonzalez': 2,\n",
              " 'wood': 5,\n",
              " 'digital': 26,\n",
              " 'prentice': 4,\n",
              " 'hall': 15,\n",
              " 'jersey': 5,\n",
              " 'jaderberg': 5,\n",
              " 'simonyan': 16,\n",
              " 'zisserman': 19,\n",
              " 'kavukcuoglu': 11,\n",
              " 'spatial': 126,\n",
              " 'transformer': 2,\n",
              " 'jampani': 1,\n",
              " 'miller': 14,\n",
              " 'kautz': 4,\n",
              " 'super': 24,\n",
              " 'slomo': 3,\n",
              " 'multiple': 183,\n",
              " 'johnson': 27,\n",
              " 'alahi': 3,\n",
              " 'fei': 23,\n",
              " 'style': 7,\n",
              " 'transfer': 82,\n",
              " 'karargyris': 1,\n",
              " 'bourbakis': 1,\n",
              " 'reconstruction': 59,\n",
              " 'digestive': 1,\n",
              " 'wall': 3,\n",
              " 'capsule': 1,\n",
              " 'endoscopy': 3,\n",
              " 'elastic': 1,\n",
              " 'ieee': 246,\n",
              " 'kingma': 13,\n",
              " 'stochastic': 403,\n",
              " 'optimization': 439,\n",
              " 'iclr': 44,\n",
              " 'lecun': 24,\n",
              " 'bottou': 22,\n",
              " 'bengio': 50,\n",
              " 'haffner': 7,\n",
              " 'document': 124,\n",
              " 'recognition': 293,\n",
              " 'proceeding': 233,\n",
              " 'yeh': 1,\n",
              " 'tang': 22,\n",
              " 'agarwala': 1,\n",
              " 'iccv': 43,\n",
              " 'convex': 302,\n",
              " 'mathematics': 29,\n",
              " 'computation': 258,\n",
              " 'meister': 3,\n",
              " 'hur': 1,\n",
              " 'unsupervised': 121,\n",
              " 'bidirectional': 6,\n",
              " 'census': 2,\n",
              " 'wang': 138,\n",
              " 'zimmer': 1,\n",
              " 'grosse': 1,\n",
              " 'sorkine': 2,\n",
              " 'hornung': 4,\n",
              " 'aaai': 27,\n",
              " 'nah': 1,\n",
              " 'kim': 15,\n",
              " 'lee': 55,\n",
              " 'niklaus': 3,\n",
              " 'context': 124,\n",
              " 'mai': 3,\n",
              " 'via': 196,\n",
              " 'paliwal': 2,\n",
              " 'pytorch': 8,\n",
              " 'github': 33,\n",
              " 'avinashpaliwal': 1,\n",
              " 'perazzi': 2,\n",
              " 'pont': 1,\n",
              " 'tuset': 1,\n",
              " 'mcwilliams': 2,\n",
              " 'van': 106,\n",
              " 'gool': 6,\n",
              " 'gross': 26,\n",
              " 'benchmark': 56,\n",
              " 'segmentation': 45,\n",
              " 'ren': 21,\n",
              " 'cao': 7,\n",
              " 'light': 92,\n",
              " 'hybrid': 44,\n",
              " 'tip': 17,\n",
              " 'meng': 6,\n",
              " 'dehazing': 1,\n",
              " 'semantic': 73,\n",
              " 'ronneberger': 2,\n",
              " 'fischer': 4,\n",
              " 'brox': 6,\n",
              " 'biomedical': 5,\n",
              " 'shi': 16,\n",
              " 'tomasi': 2,\n",
              " 'good': 156,\n",
              " 'miccai': 2,\n",
              " 'soomro': 1,\n",
              " 'zamir': 2,\n",
              " 'shah': 9,\n",
              " 'human': 186,\n",
              " 'action': 358,\n",
              " 'class': 454,\n",
              " 'delbracio': 1,\n",
              " 'sapiro': 1,\n",
              " 'heidrich': 1,\n",
              " 'wild': 20,\n",
              " 'cost': 254,\n",
              " 'volume': 96,\n",
              " 'bovik': 1,\n",
              " 'sheikh': 2,\n",
              " 'simoncelli': 11,\n",
              " 'assessment': 6,\n",
              " 'visibility': 1,\n",
              " 'deformable': 5,\n",
              " 'denoising': 10,\n",
              " 'towards': 77,\n",
              " 'raw': 22,\n",
              " 'pan': 5,\n",
              " 'rendering': 13,\n",
              " 'portraiture': 1,\n",
              " 'beyond': 57,\n",
              " 'ster': 6,\n",
              " 'resolve': 2,\n",
              " 'blurry': 2,\n",
              " 'face': 108,\n",
              " 'text': 58,\n",
              " 'zitnick': 3,\n",
              " 'kang': 4,\n",
              " 'uyttendaele': 1,\n",
              " 'winder': 1,\n",
              " 'layered': 4,\n",
              " 'representation': 344,\n",
              " 'zwicker': 1,\n",
              " 'baar': 1,\n",
              " 'annual': 62,\n",
              " 'interactive': 10,\n",
              " 'active': 125,\n",
              " 'oracle': 188,\n",
              " 'epiphany': 76,\n",
              " 'tzu': 2,\n",
              " 'kuo': 2,\n",
              " 'huang': 40,\n",
              " 'uber': 1,\n",
              " 'advanced': 23,\n",
              " 'technology': 48,\n",
              " 'pittsburgh': 15,\n",
              " 'ara': 2,\n",
              " 'vartanian': 1,\n",
              " 'wisconsin': 6,\n",
              " 'madison': 6,\n",
              " 'saleema': 2,\n",
              " 'amershi': 2,\n",
              " 'microsoft': 13,\n",
              " 'redmond': 4,\n",
              " 'lihong': 1,\n",
              " 'xiaojin': 4,\n",
              " 'zhu': 40,\n",
              " 'theoretical': 140,\n",
              " 'analysis': 471,\n",
              " 'realistic': 26,\n",
              " 'interaction': 126,\n",
              " 'previous': 182,\n",
              " 'empirical': 177,\n",
              " 'abstaining': 1,\n",
              " 'dif': 81,\n",
              " 'cult': 45,\n",
              " 'query': 220,\n",
              " 'accumulating': 2,\n",
              " 'enough': 64,\n",
              " 'make': 232,\n",
              " 'decision': 134,\n",
              " 'formalize': 12,\n",
              " 'phenomenon': 32,\n",
              " 'complexity': 275,\n",
              " 'realizable': 12,\n",
              " 'agnos': 1,\n",
              " 'tic': 10,\n",
              " 'case': 665,\n",
              " 'possible': 206,\n",
              " 'incurs': 9,\n",
              " 'depending': 45,\n",
              " 'happens': 20,\n",
              " 'suggest': 39,\n",
              " 'principled': 18,\n",
              " 'currently': 19,\n",
              " 'gap': 130,\n",
              " 'theory': 318,\n",
              " 'omniscient': 2,\n",
              " 'given': 705,\n",
              " 'answer': 49,\n",
              " 'drawing': 12,\n",
              " 'conditional': 127,\n",
              " 'distribution': 991,\n",
              " 'motivated': 19,\n",
              " 'largely': 19,\n",
              " 'convenience': 15,\n",
              " 'mounting': 1,\n",
              " 'evidence': 133,\n",
              " 'psychology': 15,\n",
              " 'behave': 13,\n",
              " 'far': 73,\n",
              " 'abstain': 5,\n",
              " 'donmez': 3,\n",
              " 'carbonell': 3,\n",
              " 'distinct': 28,\n",
              " 'classi': 334,\n",
              " 'abstention': 4,\n",
              " 'chaudhuri': 8,\n",
              " 'yaniv': 2,\n",
              " 'wiener': 2,\n",
              " 'uenced': 14,\n",
              " 'identity': 39,\n",
              " 'newell': 2,\n",
              " 'ruth': 2,\n",
              " 'sarkar': 8,\n",
              " 'kulesza': 6,\n",
              " 'incentive': 3,\n",
              " 'zhou': 42,\n",
              " 'yet': 63,\n",
              " 'account': 61,\n",
              " 'richness': 1,\n",
              " 'behavior': 108,\n",
              " 'critical': 33,\n",
              " 'designing': 24,\n",
              " 'annotator': 2,\n",
              " 'toward': 28,\n",
              " 'bridging': 4,\n",
              " 'phe': 1,\n",
              " 'nomenon': 1,\n",
              " 'build': 39,\n",
              " 'webpage': 9,\n",
              " 'basketball': 7,\n",
              " 'sport': 4,\n",
              " 'others': 48,\n",
              " 'known': 226,\n",
              " 'matter': 15,\n",
              " 'look': 48,\n",
              " 'encounter': 8,\n",
              " 'obviously': 12,\n",
              " 'immediately': 21,\n",
              " 'decide': 6,\n",
              " 'qualify': 1,\n",
              " 'solution': 308,\n",
              " 'allow': 61,\n",
              " 'answering': 9,\n",
              " 'special': 72,\n",
              " 'know': 41,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
      ],
      "metadata": {
        "id": "VWwvl_JJqJ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for data in docs:\n",
        "    vector = []\n",
        "    for word in freq_words:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            vector.append(1)\n",
        "        else:\n",
        "            vector.append(0)\n",
        "    X.append(vector)\n",
        "X = np.asarray(X)"
      ],
      "metadata": {
        "id": "Vxze6F8xqOhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p76AJ-Atqzn6",
        "outputId": "e39e2492-4263-481f-b18f-5f29a1693123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 0, 1],\n",
              "       [1, 0, 1, ..., 1, 0, 1],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 1, 1],\n",
              "       [0, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ocXBnnUFWW"
      },
      "source": [
        "## 1.TF-IDF and Scikit-learn\n",
        "\n",
        "Based on the tutorial of [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb)\n",
        "\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score.\n",
        "\n",
        "### 1.1 CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SDrK72SUFWX",
        "outputId": "383189fd-8ba0-4735-f657-a53e048ea27a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 447 ms, sys: 5.94 ms, total: 453 ms\n",
            "Wall time: 455 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#docs = docs.tolist()\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "word_count_vector=cv.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwhK9l4PUFWY"
      },
      "source": [
        "### 1.2 TfidfTransformer to Compute Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K-OH2RwUFWZ",
        "outputId": "edcb0b00-01d7-47d2-c76a-561664fc7da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.67 ms, sys: 20 µs, total: 4.69 ms\n",
            "Wall time: 5.51 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7c5EVOHUFWZ"
      },
      "source": [
        "Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVOJPwhiUFWa"
      },
      "outputs": [],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPtNe4NOUFWa",
        "outputId": "c4ca541a-0dc2-459b-dc16-106a36b7065b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(idx, docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXp7CmTRUFWa",
        "outputId": "94691e39-e837-488b-bc04-dad5bfa12aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Bayesian Distributed Stochastic Gradient Descent\n",
            "\n",
            "=====Abstract=====\n",
            "we introduce bayesian distributed stochastic gradient descent bdsgd a high throughput algorithm for training deep neural networks on parallel clusters this algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini batch gradient computation times in a compute cluster specific manner specifically our algorithm mitigates the straggler effect in synchronous gradient based optimization by choosing an optimal cutoff beyond which mini batch gradient messages from slow workers are ignored in our experiments we show that eagerly discarding the mini batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall clock time by virtue of eliminating idleness the principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run times from a generative model of cluster worker performance improves substantially over the static cutoff prior art leading to reduced deep neural net training times on large computer clusters \n",
            "\n",
            "===Keywords===\n",
            "synaptic transmission 0.381\n",
            "fringe node 0.381\n",
            "frequency separation 0.381\n",
            "der maaten weinberger 0.35\n",
            "cutting 0.35\n",
            "basis pursuit 0.35\n",
            "system nip barcelona 0.274\n",
            "system nip long 0.257\n",
            "described section 0.236\n"
          ]
        }
      ],
      "source": [
        "idx=7429\n",
        "keywords=get_keywords(idx, docs)\n",
        "print_results(idx,keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdgJghESUFWb"
      },
      "source": [
        "## 2. Gensim implementation of TextRank summarization algorithm\n",
        "\n",
        "Gensim is a free Python library designed to automatically extract semantic topics from documents. The gensim implementation is based on the popular TextRank algorithm. \n",
        "\n",
        "[Documentation](https://radimrehurek.com/gensim/summarization/keywords.html)\n",
        "\n",
        "[Tutorial](https://rare-technologies.com/text-summarization-with-gensim/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hE1oWsUFWc"
      },
      "source": [
        "### 2.1 Small text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2AG03QhUFWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad96d7f2-83e9-46e3-ea27-b81cd9095329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['factor',\n",
              " 'convergence',\n",
              " 'rescaling',\n",
              " 'multiplicative',\n",
              " 'function',\n",
              " 'kullback',\n",
              " 'gradient',\n",
              " 'algorithm',\n",
              " 'matrix',\n",
              " 'multivariate',\n",
              " 'data',\n",
              " 'useful decomposition',\n",
              " 'update']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import gensim\n",
        "text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n",
        "\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n",
        "\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n",
        "\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n",
        "\"minimize the conventional least squares error while the other minimizes the  \" + \\\n",
        "\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n",
        "\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n",
        "\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n",
        "\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n",
        "\"rescaling factor is optimally chosen to ensure convergence.\"\n",
        "gensim.summarization.keywords(text, \n",
        "         ratio=0.5,               # use 50% of original text\n",
        "         words=None,              # Number of returned words\n",
        "         split=True,              # Whether split keywords\n",
        "         scores=False,            # Whether score of keyword\n",
        "         pos_filter=('NN', 'JJ'), # Part of speech (nouns, adjectives etc.) filters\n",
        "         lemmatize=True,         # If True - lemmatize words\n",
        "         deacc=True)              # If True - remove accentuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAqZnlRfUFWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513559aa-4a09-42a7-a85d-86365bd2b873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY:  ['Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data.', 'Two different multiplicative algorithms for NMF are analyzed.', 'They differ only slightly in the multiplicative factor used in the update rules.']\n"
          ]
        }
      ],
      "source": [
        "print(\"SUMMARY: \", gensim.summarization.summarize(text,\n",
        "                                                  ratio = 0.5,\n",
        "                                                  split = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNMQVnNNUFWd"
      },
      "source": [
        "### 2.2 Large text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UR9rOTMUFWd"
      },
      "outputs": [],
      "source": [
        "def get_keywords_gensim(idx, docs):\n",
        "    \n",
        "    keywords=gensim.summarization.keywords(docs[idx], \n",
        "                                  ratio=None, \n",
        "                                  words=10,         \n",
        "                                  split=True,             \n",
        "                                  scores=False,           \n",
        "                                  pos_filter=None, \n",
        "                                  lemmatize=True,         \n",
        "                                  deacc=True)              \n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results_gensim(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk6kCmcdUFWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d87fd3-82b3-43d8-f1f6-a37300c68a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Quadratic Video Interpolation\n",
            "\n",
            "=====Abstract=====\n",
            "video interpolation is an important problem in computer vision which helps overcome the temporal limitation of camera sensors existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation which cannot well approximate the complex motion in the real world to address these issues we propose a quadratic video interpolation method which exploits the acceleration information in videos this method allows prediction with curvilinear trajectory and variable velocity and generates more accurate interpolation results for high quality frame synthesis we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame in addition we present techniques for flow refinement extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets \n",
            "\n",
            "===Keywords===\n",
            "frame\n",
            "video\n",
            "interpolate\n",
            "modeled\n",
            "method\n",
            "motion\n",
            "proposed\n",
            "dataset\n",
            "evaluated\n",
            "estimated\n"
          ]
        }
      ],
      "source": [
        "idx=9036\n",
        "keywords=get_keywords_gensim(idx, docs)\n",
        "print_results_gensim(idx,keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJDm2DrOUFWe"
      },
      "source": [
        "The keywords highlight the main point , but still miss valuable information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzGNyTDqUFWe"
      },
      "source": [
        "## 3. Python implementation of the Rapid Automatic Keyword Extraction algorithm (RAKE) using NLTK\n",
        "\n",
        "[Documentation](https://github.com/csurfer/rake-nltk)\n",
        "\n",
        "### Setup using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXAqY-jzUFWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d016437-f8ee-4f1d-fbf4-d444f449067f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.7/dist-packages (1.0.6)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rake-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn0dasQwUFWe"
      },
      "source": [
        "### or directly from the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_BJ-byNUFWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66021f40-237a-4aec-ab5f-28e89e7a7586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rake-nltk' already exists and is not an empty directory.\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "error: package directory 'rake_nltk' does not exist\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/csurfer/rake-nltk.git\n",
        "!python rake-nltk/setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlGmUg6iUFWe"
      },
      "source": [
        "### 3.1 Small text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFJ_d4kUFWf"
      },
      "outputs": [],
      "source": [
        "text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n",
        "\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n",
        "\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n",
        "\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n",
        "\"minimize the conventional least squares error while the other minimizes the  \" + \\\n",
        "\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n",
        "\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n",
        "\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n",
        "\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n",
        "\"rescaling factor is optimally chosen to ensure convergence.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYBsPY-yUFWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c0af73-234d-4661-f474-b86ff212d22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(16.0, 'diagonally rescaled gradient descent'),\n",
              " (16.0, 'conventional least squares error'),\n",
              " (13.5, 'two different multiplicative algorithms'),\n",
              " (9.0, 'negative matrix factorization'),\n",
              " (9.0, 'auxiliary function analogous'),\n",
              " (8.0, 'multiplicative factor used'),\n",
              " (4.5, 'rescaling factor'),\n",
              " (4.0, 'useful decomposition'),\n",
              " (4.0, 'update rules'),\n",
              " (4.0, 'proving convergence')]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "from rake_nltk import Rake\n",
        "nltk.download('punkt')\n",
        "r = Rake()\n",
        "r.extract_keywords_from_text(text)\n",
        "r.get_ranked_phrases_with_scores()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP7U88AzUFWf"
      },
      "source": [
        "Wow! We see well interbretable machine learning terminology! But why diagonally rescaled gradient descent is more important than negative matrix factorization? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCKKOarRUFWf"
      },
      "source": [
        "### 3.2 Large Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzob673lUFWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b139a1-6747-4c51-a929-94b30848a294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Quadratic Video Interpolation\n",
            "\n",
            "=====Abstract=====\n",
            "video interpolation is an important problem in computer vision which helps overcome the temporal limitation of camera sensors existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation which cannot well approximate the complex motion in the real world to address these issues we propose a quadratic video interpolation method which exploits the acceleration information in videos this method allows prediction with curvilinear trajectory and variable velocity and generates more accurate interpolation results for high quality frame synthesis we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame in addition we present techniques for flow refinement extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets \n",
            "\n",
            "===Keywords===\n",
            "linear model wide variety video datasets introduction video interpolation aim synthesize intermediate frame original input image temporally upsample low frame rate video higher frame rate fundamental problem computer vision help overcome temporal limitation camera sensor used numerous application motion deblurring video editing virtual reality medical imaging state art video interpolation method explicitly implicitly assume uniform motion consecutive frame object move along straight line constant speed approach usually adopt linear model synthesizing intermediate frame however motion real scenario complex non uniform uniform assumption may always hold input video often lead inaccurate interpolation result moreover existing model mainly developed based consecutive frame interpolation higher order motion information video acceleration well exploited effective frame interpolation algorithm use additional input frame estimate higher order information accurate motion prediction end propose\n"
          ]
        }
      ],
      "source": [
        "def get_keywords_rake(idx, docs, n=10):\n",
        "    # Uses stopwords for english from NLTK, and all puntuation characters by default\n",
        "    r = Rake()\n",
        "    \n",
        "    # Extraction given the text.\n",
        "    r.extract_keywords_from_text(docs[idx][1000:2000])\n",
        "    \n",
        "    # To get keyword phrases ranked highest to lowest.\n",
        "    keywords = r.get_ranked_phrases()[0:n]\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k)\n",
        "\n",
        "idx=9036\n",
        "keywords = get_keywords_rake(idx, docs, n=10)\n",
        "print_results(idx, keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLBKzFaZUFWg"
      },
      "source": [
        "Oups! Something goes wrong! Algorithm does not work for the preprocessed text without punctuations. Let's treat the raw text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blTC5_-BUFWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79096d2d-3aa6-45b4-953e-ee59457fed80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Quadratic Video Interpolation\n",
            "\n",
            "=====Abstract=====\n",
            "video interpolation is an important problem in computer vision which helps overcome the temporal limitation of camera sensors existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation which cannot well approximate the complex motion in the real world to address these issues we propose a quadratic video interpolation method which exploits the acceleration information in videos this method allows prediction with curvilinear trajectory and variable velocity and generates more accurate interpolation results for high quality frame synthesis we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame in addition we present techniques for flow refinement extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets \n",
            "\n",
            "===Keywords===\n",
            "video datasets introduction video interpolation aims\n",
            "temporally upsample low frame rate videos\n",
            "motion deblurring video editing virtual reality\n",
            "art video interpolation methods explicitly\n",
            "approaches usually adopt linear models\n",
            "nement extensive experiments demonstrate\n",
            "implicitly assume uniform motion\n",
            "synthesizing intermediate frames however\n",
            "existing linear models\n",
            "unknown target frame\n"
          ]
        }
      ],
      "source": [
        "idx=9036\n",
        "keywords = get_keywords_rake(idx, df['full_text'], n=10)\n",
        "print_results(idx, keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eHXhzr7UFWg"
      },
      "source": [
        "Presented implementation works well on sentences, but it is not flexible enough for large text. However, those who are interested in RANK can expand the capabilities of this code to their needs. We will consider next options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUWFXldSUFWg"
      },
      "source": [
        "## 4. Yet Another Keyword Extractor (Yake)\n",
        "\n",
        "[Documentation](https://github.com/LIAAD/yake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDG71KRMUFWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5f8cda-b425-40d6-c017-46529f4b3d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-r0vm6u_e\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-r0vm6u_e\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.10)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Collecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 19.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2022.6.2)\n",
            "Building wheels for collected packages: yake, jellyfish\n",
            "  Building wheel for yake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yake: filename=yake-0.4.8-py2.py3-none-any.whl size=62600 sha256=a42aa17daab86f1bc328ad7fc64628782aec7002ecdf2afdaea67f1f7bff4572\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dsy46z4a/wheels/52/79/f4/dae9309f60266aa3767a4381405002b6f2955fbcf038d804da\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73987 sha256=052c6c069fabad30f5f22d3feeccc9dbb6aec533a05f6afa7994c89103ed1a21\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built yake jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_8r4NSEUFWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d83bb1-d4c2-4af7-b35d-3ad8b64c2718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Quadratic Video Interpolation\n",
            "\n",
            "=====Abstract=====\n",
            "video interpolation is an important problem in computer vision which helps overcome the temporal limitation of camera sensors existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation which cannot well approximate the complex motion in the real world to address these issues we propose a quadratic video interpolation method which exploits the acceleration information in videos this method allows prediction with curvilinear trajectory and variable velocity and generates more accurate interpolation results for high quality frame synthesis we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame in addition we present techniques for flow refinement extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets \n",
            "\n",
            "===Keywords===\n",
            "('Non-negative matrix factorization', 0.0041066275750552455)\n",
            "('Non-negative matrix', 0.026529705128479162)\n",
            "('matrix factorization', 0.026529705128479162)\n",
            "('multivariate data', 0.026529705128479162)\n",
            "('decomposition for multivariate', 0.03127464030655176)\n",
            "('NMF', 0.06699743201311577)\n",
            "('NMF are analyzed', 0.11148839518508058)\n",
            "('algorithms', 0.13323194577601624)\n",
            "('previously been shown', 0.1372005684192386)\n",
            "('Non-negative', 0.1484061535685674)\n"
          ]
        }
      ],
      "source": [
        "import yake\n",
        "\n",
        "def get_keywords_yake(idx, docs):\n",
        "    y = yake.KeywordExtractor(lan='en',          # language\n",
        "                             n = 3,              # n-gram size\n",
        "                             dedupLim = 0.9,     # deduplicationthresold\n",
        "                             dedupFunc = 'seqm', #  deduplication algorithm\n",
        "                             windowsSize = 1,\n",
        "                             top = 10,           # number of keys\n",
        "                             features=None)           \n",
        "    \n",
        "    keywords = y.extract_keywords(text)\n",
        "    return keywords\n",
        "\n",
        "idx=9036\n",
        "keywords = get_keywords_yake(idx, docs[idx])\n",
        "print_results(idx, keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUaIUWisUFWh"
      },
      "source": [
        "Key phrases are repeated, and the text needs pre-processing to remove stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVI7bIjUFWh"
      },
      "source": [
        "## 5. Keyphrases extraction using pke\n",
        "\n",
        "`pke` an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n",
        "\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTByEYxDUFWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3a5b96-bef4-4149-ca5b-d4933a58fb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-zq57oju5\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-zq57oju5\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.7.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.3->pke==2.0.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.3->pke==2.0.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pke==2.0.0) (2022.6.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==2.0.0) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pke==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: pke, sklearn\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160276 sha256=7abb6ae5576310ba54dc65f8206f39af0d8de0b4091fba0eece002628a136422\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fizb3r8l/wheels/fa/b3/09/612ee93bf3ee4164bcd5783e742942cdfc892a86039d3e0a33\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=419ae6255f76b044c035d63f3be7ddcfa0cb528655b8882b193d9bf98244dcfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built pke sklearn\n",
            "Installing collected packages: unidecode, sklearn, pke\n",
            "Successfully installed pke-2.0.0 sklearn-0.0 unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEylpdvoUFWh"
      },
      "outputs": [],
      "source": [
        "import pke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Zg9Ds5UFWi"
      },
      "source": [
        "### 5  SingleRank\n",
        "\n",
        "This model is an extension of the TextRank model that uses the number of co-occurrences to weigh edges in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTjbKQS-UFWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c10d523-e2aa-4de6-b894-b4003f3660fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['ner', 'textcat', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Learning under uncertainty: a comparison between R-W and Bayesian approach\n",
            "\n",
            "=====Abstract=====\n",
            "accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition to examine the underlying computational principles that guide different learning behavior in an uncertain environment we compared an r w model and a bayesian approach in a visual search task with different volatility levels both r w model and the bayesian approach reflected an individual s estimation of the environmental volatility and there is a strong correlation between the learning rate in r w model and the belief of stationarity in the bayesian approach in different volatility conditions in a low volatility condition r w model indicates that learning rate positively correlates with lose shift rate but not choice optimality inverted u shape the bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality but not lose shift rate inverted u shape in addition we showed that comparing to expert learners individuals with high lose shift rate sub optimal learners had significantly higher learning rate estimated from r w model and lower belief of stationarity from the bayesian model \n",
            "\n",
            "===Keywords===\n",
            "different multiplicative algorithms\n",
            "conventional least squares error\n",
            "multiplicative factor\n",
            "monotonic convergence\n",
            "algorithms\n",
            "auxiliary function analogous\n",
            "expectation-maximization algorithm\n",
            "multivariate data\n",
            "rescaling factor\n",
            "non-negative matrix factorization\n"
          ]
        }
      ],
      "source": [
        "# define the set of valid Part-of-Speeches\n",
        "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "\n",
        "# 1. create a SingleRank extractor.\n",
        "extractor = pke.unsupervised.SingleRank()\n",
        "\n",
        "# 2. load the content of the document.\n",
        "extractor.load_document(input=text,\n",
        "                        language='en',\n",
        "                        normalization=None)\n",
        "\n",
        "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
        "extractor.candidate_selection(pos=pos)\n",
        "\n",
        "# 4. weight the candidates using the sum of their word's scores that are\n",
        "#    computed using random walk. In the graph, nodes are words of\n",
        "#    certain part-of-speech (nouns and adjectives) that are connected if\n",
        "#    they occur in a window of 10 words.\n",
        "extractor.candidate_weighting(window=10,\n",
        "                              pos=pos)\n",
        "\n",
        "# 5. get the 10-highest scored candidates as keyphrases\n",
        "keyphrases = extractor.get_n_best(n=10)\n",
        "\n",
        "idx = 9036\n",
        "# now print the results\n",
        "print(\"\\n=====Title=====\")\n",
        "print(df['title'][idx])\n",
        "print(\"\\n=====Abstract=====\")\n",
        "print(df['abstract'][idx])\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keyphrases:\n",
        "    print(k[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4O7uVCg9G7Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vectorization"
      ],
      "metadata": {
        "id": "Tul2zjIcKmlZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.029006,
          "end_time": "2021-05-11T17:14:01.462639",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.433633",
          "status": "completed"
        },
        "tags": [],
        "id": "emerging-layer"
      },
      "source": [
        "### **Skip-gram Training Sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.599081Z",
          "iopub.status.busy": "2021-05-11T17:14:01.598127Z",
          "iopub.status.idle": "2021-05-11T17:14:01.601526Z",
          "shell.execute_reply": "2021-05-11T17:14:01.602057Z"
        },
        "papermill": {
          "duration": 0.04053,
          "end_time": "2021-05-11T17:14:01.602224",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.561694",
          "status": "completed"
        },
        "tags": [],
        "id": "imperial-glasgow",
        "outputId": "f32a359e-7676-4e12-abd4-58ac65a5e04a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in docs:\n",
        "    if token not in vocab:\n",
        "        vocab[token] = index\n",
        "        index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.667502Z",
          "iopub.status.busy": "2021-05-11T17:14:01.666697Z",
          "iopub.status.idle": "2021-05-11T17:14:01.671342Z",
          "shell.execute_reply": "2021-05-11T17:14:01.670662Z"
        },
        "papermill": {
          "duration": 0.039049,
          "end_time": "2021-05-11T17:14:01.671482",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.632433",
          "status": "completed"
        },
        "tags": [],
        "id": "statistical-exchange",
        "outputId": "491cfbe7-1211-45f0-ea61-d5d21d60c864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.738241Z",
          "iopub.status.busy": "2021-05-11T17:14:01.737386Z",
          "iopub.status.idle": "2021-05-11T17:14:01.741170Z",
          "shell.execute_reply": "2021-05-11T17:14:01.741667Z"
        },
        "papermill": {
          "duration": 0.039912,
          "end_time": "2021-05-11T17:14:01.741865",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.701953",
          "status": "completed"
        },
        "tags": [],
        "id": "parallel-guess",
        "outputId": "5a853aad-2aac-4f3e-ab7d-5a21b8542b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
          ]
        }
      ],
      "source": [
        "example_sequence = [vocab[word] for word in docs]\n",
        "print(example_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram Training Sample"
      ],
      "metadata": {
        "id": "CiS9kAy4JQKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "_Gx_QhDAHNpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.810498Z",
          "iopub.status.busy": "2021-05-11T17:14:01.809593Z",
          "iopub.status.idle": "2021-05-11T17:14:01.814187Z",
          "shell.execute_reply": "2021-05-11T17:14:01.813625Z"
        },
        "papermill": {
          "duration": 0.041861,
          "end_time": "2021-05-11T17:14:01.814332",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.772471",
          "status": "completed"
        },
        "tags": [],
        "id": "historical-prophet",
        "outputId": "ff36465a-7ec3-448e-f0c9-ec2d13228441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "vocab_size = len(vocab)\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples = 0)\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.885496Z",
          "iopub.status.busy": "2021-05-11T17:14:01.884618Z",
          "iopub.status.idle": "2021-05-11T17:14:01.888716Z",
          "shell.execute_reply": "2021-05-11T17:14:01.889515Z"
        },
        "papermill": {
          "duration": 0.042841,
          "end_time": "2021-05-11T17:14:01.889751",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.846910",
          "status": "completed"
        },
        "tags": [],
        "id": "labeled-narrative",
        "outputId": "3f8885e7-1984-40e5-dc33-3c7a0101c668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(71, 70): (metric learning temporal sequence alignment damien garreau en damien garreau en emi lajugie inria remi lajugie inria sylvain arlot cnrs sylvain arlot en francis bach inria francis bach inria abstract paper propose learn mahalanobis distance perform alignment multivariate time series learning example task time series true alignment known cast alignment problem structured prediction task propose realistic loss alignment optimization tractable provide experiment real data audio audio context learning similarity measure lead improvement performance alignment task propose use metric learning framework perform feature selection basic audio feature build combination better alignment performance introduction problem aligning temporal sequence ubiquitous application ranging bioinformat ic audio processing goal align similar time series global structure local temporal difference alignment algorithm rely similar ity measure good metric crucial especially high dimensional setting feature signal irrelevant alignment task goal paper learn similarity measure annotated example order improve relevance alignment example context music information retrieval alignment used different case audio audio alignment audio score alignment rst case goal match audio interpretation piece potentially different rythm whereas audio score alignment focus matching audio signal symbolic representation score second case attempt learn annotated data measure performing alignment joder propose generative model context keshet learn measure discriminative setting similarly keshet use discriminative loss learn measure work focus audio audio alignment context set authorized alignment much larger explicitly cast problem structured prediction task solve shelf stochastic optimization technique proper signi cant adjustment particular term loss idea alignment relevant community speech recognition since pioneering work sakoe chiba contributed equally sierra project team epartement informatique ecole normale sup erieure cnrs inria en need metric learning go far beyond unsupervised partitioning problem weinberger saul proposed margin framework learning metric nearest neighbour algorithm based set must link must link constraint lajugie proposed use margin framework learn mahalanobis metric context partitioning problem since structured svm proposed tsochantaridis taskar successfully used solve many learning problem instance learn weight graph matching metric ranking task used learn graph structure graph cut make following contribution cast learning mahalanobis metric context alignment structured prediction problem real musical datasets metric improves performance alignment algo rithms high level feature propose use metric learning framework learn combination basic audio feature get good alignment performance experimentally standard hamming loss although tractable computationnally permit learn relevant similarity measure real world setting propose new loss closer true evaluation loss alignment leading tractable learning task derive cient frank wolfe based algorithm deal new loss loss solves issue encountered hamming loss matricial formulation alignment problem notation paper consider alignment problem multivariate time series sharing dimension possibly different length namely rta rtb refer row ata btb column vector denote pair signal let rta arbitrary pairwise nity matrix associated pair encodes nity note framework extended case multivariate signal different dimension long well ned goal alignment task non decreasing sequence index length max match time index time series time maximal index time series way cid satis matching beginning matching ending type move given binary matrix every otherwise denote set matrix uniquely determined example given vertical move matrix mean signal waiting whereas horizontal mean waiting diagonal move mean move together sense time reference warped known alignment task cast following linear program set cid max goal learn form nity matrix learned alignment obtained optimization problem referred decoding model dynamic time warping given nity matrix associated pair signal nding alignment solves done ciently tatb example valid alignment encoded matrix red upper triangle grey zone corresponds area loss ab blue lower one dynamic programming algorithm often referred dynamic time warping algorithm described alg supplementary material various additional constraint may used dynamic time warping algorithm could easily add alg cardinality set huge corresponds number path rectangular grid southwest northeast corner vertical horizontal diagonal move allowed nition delannoy number noted go nity mahalanobis metric many application see pair nity matrix computed cid cid paper propose learn metric compare instead plain euclidean metric parametrized matrix set semi nite positive matrix use corresponding mahalanobis metric compute pairwise nity cid note decoding maximization linear function parameter max cid max cid joint feature map cid cid cid learning metric assume given pair training instance goal matrix predicted alignment close groundtruth example well unseen example rst loss alignment order quantify proximity alignment see necessary fully labelled instance mean pair need exact alignment partial alignment might dealt alternating metric learning constrained alignment loss alignment framework alignment encoded matrix thus interested func tions cid frobenius norm ned cid cid hamming loss simple loss matrix frobenius norm difference turn unnormalized hamming loss valued matrix matrix ned cid cid cid cid cid cid cid cid cid cid vector coordinate equal last line come fact value make hamming loss loss often used structured prediction task audio score setting keshet use modi version loss average number time difference alignment greater xed threshold loss easy optimize since linear parametrization alignement problem optimal audio audio alignment indeed major drawback hamming loss alignment xed length depends number crossing alignment path easily cid cid much closer see important notice often case length signal grows area loss natural loss computed mean distance beween path depicted matrix loss corresponds area path matrix represented grey zone formally put min area loss mean audio literature loss sometimes called mean absolute deviation loss noted ab unfortunately general alignment problem ab linear matrix context alignment sequence different nature signal reference thus index sequence ned increasing audio partition alignment problem loss linear argument precisely introduce matrix lta rta lower triangular one including diagonal write loss min cid cid lta cid lta cid cid lta cid lta cid lta kyk cid easy see lta cid cid prove loss corresponds area loss special case let alignment vertical move unique ykj ltay lta exactly area curve determined path experiment use ab evaluation training approximation area loss many real world applica tions meaningful loss ass quality alignment area loss shown experiment hamming loss suf cient simple situation allows learn metric lead good alignment performance term area loss challenging datasets work see sec due fact alignment close term area loss suffer big hamming loss thus natural extend formulation matrix start symmetrizing formulation overcome problem overpenalization vertical horizontal move couple binary matrix symmetrized area loss cid cid cid lta cid cid cid cid ltb cid cid ltb cid lta ltay cid cid ltb cid cid cid cid ltb cid ltay cid cid cid real world bach chorale dataset represented groundtruth alignment together others term hamming loss alignment far groundtruth whereas area loss structured prediction setting described sec depicted alignment called violated constraint namely output loss augmented decoding step see sec propose make loss concave convex hull denote let introduce max cid max largest eigenvalue binary matrix cid cid cid cid dtb cid cid cid lta dta dta cid dta ltb cid dtb ltay cid cid ltb cid ltb cid cid cid get concave function coincides cid empirical loss minimization recall given alignment example xed loss cid goal solve following minimization problem cid cid cid argmax cid cid min convex regularizer preventing tting cid cid margin approach section describe margin approach solve surrogate problem untractable shown decoding task maximum linear function parameter aim predicting output discrete space space potential alignment respect constraint learning thus fall structured prediction framework hinge loss convex surrogate max cid cid cid cid cid cid cid completeness experiment try set matrix minimal trace dominate cid solving semide nite program sdp report associated note matrix could chosen particular since matrix pointwise positive matrix diag cid loss concave cid tatb violated constraint hamming lossmost violated constraint lsgroundruth alignment evaluation usually referred loss augmented decoding see cid argmax elementary computation cid argmin cid cid cid cid aim solving following problem sometimes called margin rescaled problem rta cid cid cid cid cid cid min cid cid max cid hamming loss case notice joint feature map linear thus take loss linear rst argument cid instance hamming loss loss augmented decoding maximization linear function space solve ciently dynamic programming algorithm see sec supplementary material way plugging hamming loss lead convex structured prediction problem problem solved standard technique cutting plane method stochastic gradient descent block coordinate frank wolfe dual note adapted standard unconstrained optimization method setting cid cid cid cid min optimization symmetrized area loss symmetrized area loss concave rst argument thus problem min max form deriving dual straightforward detail found supplementary material plug symmetrized area loss cid sal ned problem dual following form cid cid denote convex hull set cartesian product training example set note recover similar since sal loss concave aforementioned problem convex problem quadratic program compact set thus use frank wolfe algorithm note similar proposed lacoste julien additional term due concavity loss cid experiment applied method task learning good similarity measure aligning audio sig nals eld researcher spent lot effort designing well suited meaningful feature problem combining feature aligning temporal sequence still challenging simplicity took diagonal experiment dataset kirchhoff lerch dataset description first applied method dataset kirchhoff lerch dataset pair aligned example arti cially created stretching original audio signal way groundtruth alignment known thus data fall setting precise description dataset found pair stretched along different tempo curve signal made music divided frame hopsize thus leading typical length signal setting keep feature simple implement known perform well alignment task mfcc labeled spectral atness spectral centroid spectral spread maximum envelope max power level frame pow see detail computation feature normalize feature subtracting median value dividing standard deviation median audio data subject outlier comparison performance individual feature learned metric error bar performance learned metric determined best worst perfor mance different experiment denotes learned combination method best mfcc combination experiment conducted following experiment individual feature perform alignment dynamic time warping algorithm evaluate performance single feature term loss typically used ass performance setting report result experiment plug data method hamming loss learn linear positive com bination feature reported thus combining feature dataset yield better performance considering single feature completeness conducted experiment standard rst mfccs coef cients rst second order derivative feature result competed best learned combination handcrafted feature namely term ab loss perform second note result slightly worse best single handcrafted feature better best mfcc coef cient used feature baseline compared uniform combination handcrafted feature metric identity matrix result chart ab second individual value ranging second second chorale dataset dataset bach dataset consists ten bach chorale small quadriphonic piece chorale midi reference corresponding score basically representation partition alignment midi le audio given thus converted midi le audio following classically done alignment see way fall audio audio framework technique apply piece music approximately long leading similar signal length experiment use feature sec depicted optimization hamming loss performs poorly dataset fact best individual feature performance far better performance learned thus metric learning practical hamming loss performs much worse best single feature conducted learning experiment symetrized area loss cid resulting learned parameter far better learned hamming loss get performance similar best feature note feature handcrafted reaching performance hard task training instance already challenging http music northwestern edu data bach html wmpowm scm srsfm maxssm ab performance algorithm chorale dataset left right best single feature best learned combination feature symmetrized area loss cid best combination mfcc sal obtained via sdp see footnote section best combination mfcc derivative learned cid best combination mfccs derivative learned hamming loss best combination feature hamming loss depicted learned parameter loss augmented decoding performed either area known structured svm represents violated constraint see violated constraint hamming loss lead align ment totally unrelated groundtruth alignment whereas symmetrized area loss far closer much discriminative feature selection last conducted feature selection experiment datasets starting low level feature namely leading mfccs coef cients rst derivative learn linear combination achieves good alignment performance term area loss note little musical prior knowledge put moreover either improve best handcrafted feature dataset perform similarly datasets performance learned combination handcrafted feature performed similarly combination mfccs coef cients conclusion paper presented structured prediction framework learning metric tempo ral alignment problem able combine hand crafted feature well building automat ically new state art feature basic low level information little expert knowledge technically made possible considering loss beyond usual hamming loss typically used practical within structured prediction framework linear output representation present work may extended several way main consider case partial information alignment available often case music bioinformatics application note similarly lajugie simple alternating optimiza tion metric learning constrained alignment provide simple rst solution could probably improved upon acknowledgement author acknowledge support european research council sierra project gargantua project funded mastodon program cnrs airbus foundation phd fellowship thanks piotr bojanowski helpful discussion warm thanks arshia cont philippe cuvillier sharing knowledge audio processing holger kirchhoff alexander lerch dataset ab reference aach church aligning gene expression time series time warping algorithm bioinfor matics banderier schwer delannoy number journal statistical planning inference caetano mcauley cheng smola learning graph matching ieee trans pami cont schwarz schnell raphael evaluation real time audio score alignment proc ismir cuturi vert birkenes matsui kernel time series based global alignment proc icassp volume page ieee dixon widmer match music alignment tool chest proc ismir page frank wolfe algorithm quadratic programming naval research logistics quarterly gold morgan elli speech audio signal processing processing perception speech music john wiley son hamming error detecting error correcting code bell system technical journal dannenberg tzanetakis polyphonic audio matching alignment music retrieval computer science department page joachim finley cutting plane training structural svms machine learning joder essid richard learning optimal feature polyphonic audio score alignment ieee trans audio speech language processing keshet shalev shwartz singer chazan margin algorithm speech phoneme music score alignment ieee transaction audio speech language processing kirchhoff lerch evaluation feature audio audio alignment journal new music research lacoste julien jaggi schmidt pletscher block coordinate frank wolfe optimization structural svms proc icml lajugie bach arlot margin metric learning constrained partitioning problem proc icml mcfee lanckriet metric learning rank proc icml page uller information retrieval music motion springer sakoe chiba dynamic programming algorithm optimization spoken word recognition acoustic speech signal processing ieee transaction shalev shwartz singer srebro cotter pegasos primal estimated sub gradient solver svm mathematical programming szummer kohli hoiem learning crfs graph cut proc cvpr taskar koller guestrin max margin markov network adv nip thompson plewniak poch balibase benchmark alignment database evaluation multiple alignment program bioinformatics torres cabada nieto exact formula number alignment dna sequence mitochondrial dna tsochantaridis joachim hofmann altun singer margin method structured interdependent output variable journal machine learning research weinberger saul distance metric learning margin nearest neighbor classi cation journal machine learning research, resnets provably better linear predictor department computer science applied mathematics ohad shamir weizmann institute science rehovot israel ohad shamir weizmann abstract residual network resnet standard deep neural net architecture state art performance across numerous application main premise resnets allow training layer focus tting residual previous layer output target output thus expect trained network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather getting stuck arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual unit indeed exhibit behavior sense optimization landscape contains local minimum value obtained linear predictor namely layer network notably minimal assumption precise network architecture data distribution loss function used provide quantitative analysis approximate stationary point problem finally certain tweak architecture training network standard stochastic gradient descent achieves objective value close better linear predictor introduction residual network resnets popular class arti cial neural network providing state art performance across numerous application kim xie xiong unlike vanilla feedforward neural network resnets characterized skip connection output layer directly added output following layer mathematically whereas feedforward neural network expressed stacking layer form input output pair tunable parameter function resnets built residual unit form xed function fact common let identity case unit take form intuitively mean layer training focus tting residual target given rather particular adding depth harm performance since effectively eliminate layer tuning zero function due property residual network proven effective training extremely deep network hundred layer despite widespread empirical success rigorous theoretical understanding training residual network limited recent theoretical work optimization deep learning conference neural information processing system neurips montr canada soltanolkotabi yun soudry hoffer brutzkus safran shamir lee name example focused simpler feedforward architecture capture property residual network recent result consider residual like element see discussion related work generally apply standard architecture particular aware theoretical justi cation basic premise resnets namely architecture allows adding layer without harming performance problem training neural network involves solving highly non convex problem local search procedure thus even though deeper residual network express shallower one clear training process indeed converge network better perhaps attempt train residual network gradient based method might hit poor local minimum worse error obtained shallower network question main motivation work secondary motivation several recent result yun safran shamir liang demonstrate spurious local minimum value larger global minimum exist general training neural network even fairly strong assumption thus instead aiming demonstrating minimum exist might good true realistic network perhaps consider modest goal showing minimum exist certain non trivial level set level set correspond instance optimal value attainable shallower network without additional residual layer paper study question considering competitiveness simple residual network composed arbitrarily deep nonlinear residual unit linear output layer respect linear predictor equivalently layer network speci cally consider optimization problem associated training residual network general non convex complicated structure nevertheless prove optimization landscape local minimum value higher achieved linear predictor data word run local search procedure reach local minimum assured solution worse best obtainable linear predictor importantly fairly minimal assumption residual unit assumption data distribution linear separability assumption loss function used besides smoothness convexity network output satis loss used practice addition provide quantitative analysis show every point close stationary certain direction see sec precise nition poly worse xed linear predictor result geometric nature explain later necessarily imply standard gradient based method indeed converge desirable solution example since iterates might diverge nevertheless provide algorithmic showing residual architecture changed bit standard stochastic gradient descent sgd procedure predictor similar better best linear predictor relies simple perhaps unexpected reduction setting online learning might independent interest supplementary material paper contains proof appendix discussion result generalized vector valued output appendix related work far know existing rigorous theoretical result residual network pertain linear network combine linear residual unit form although network used practice capture important aspect non convexity associated training residual network particular hardt showed linear residual network squared loss spurious local minimum namely every local minimum global recently bartlett proved convergence result gradient descent problem assuming input isotropic target linear mapping symmetric positive nite showing similar result non linear network mentioned hardt major open problem paper focus non linear residual unit consider local minimum level set term setting perhaps work closest liang considers network written cid hidden layer network arbitrary possibly deeper network technical assumption data distribution activation used network size assuming certain classi cation loss author prove training objective benign sense network corresponding local minimum zero classi cation error however author point architecture different standard resnets would require nal tunable layer combine output result provably hold architecture moreover technical assumption non trivial apply standard activation loss relu activation logistic loss require speci condition data linear separability certain low rank structure contrast study standard residual unit make minimal assumption network data distribution loss used side prove result local minimum certain level set rather point finally idea studying stationary point non convex optimization problem reference level set explored work setting quite different setting preliminary start word basic notation terminology generally use bold faced letter denote vector assumed column form capital letter denote matrix function cid cid refers euclidean norm vector spectral norm matrix unless speci otherwise cid cid matrix denotes frobenius norm always upper bound spectral norm matrix vec refers entry written long vector according canonical order given function euclidean space denotes gradient denotes hessian point domain function local minimum cid cid open neighborhood finally use standard notation hide constant let poly referto expression polynomial consider residual network architecture consisting residual unit composed linear output layer scalar output cid cid make assumption structure overall depth network computes except last layer tunable linear transformation namely matrix necessarily square parameter condition follows full pre activation structure proposed empirically found best performing residual unit architecture commonly used practice tensorflow depart structure fully tunable rather convolution facilitate simplify theoretical study assumption given network output cid cid parameterized vector matrix possibly complicated function parame terized remark bias note model easily incorporate bias namely predictor form cid cid tunable standard trick augmenting additional coordinate whose value always assuming output vector additional coordinate value since result depend data geometry speci would affected modi cation assume network trained respect data distribution average training set loss function cid network prediction target value thus consider optimization problem min cid cid cid cid see appendix discussion result generalized network vector valued output unconstrained objective main focus paper general objective convex easily spurious local minimum saddle point result make explicit assumption distribution structure loss assume throughout paper following assumption loss cid twice differentiable convex assumption mild satis standard loss logistic loss squared loss smoothed hinge loss etc note assumption twice differentiable respect particular function ned xed twice differentiable emphasize throughout paper assume necessarily differentiable respect indeed represents network non differentiable operator relu max function cannot expect differentiable everywhere considering derivative think input long vector euclidean space order speci vec vector matrix discussed introduction wish compare objective value obtained linear predictor speci cally use notation flin cid cid cid cid denote expected loss linear predictor parameterized vector assumption function convex twice differentiable finally introduce following class point behave approximately like local minimum respect term rst derivative nition sopsp let open subset domain lipschitz second order partial stationary point sopsp cid cid min importantly note local minimum must sopsp local minimum differentiable function hence cid cid min nition directly generalizes well known notion second order stationary point sosp mccormick nesterov polyak jin ned function twice differentiable parameter fact nition sopsp equivalent requiring sosp need use general nition assuming differentiable interestingly sosp general class point non convex optimization gradient based method shown converge poly iteration competitiveness linear predictor main result thm corollary proven stage first point cid cid cid lower bounded term suboptimality respect best linear predictor thm consider case point suboptimal respect best linear predictor either cid cid strictly positive min strictly negative thm thus building nition sopsp previous section point suboptimal compared linear predictor local minimum theorem point cid vector dimension cid cid cid flin cid cid cid cid cid cid cid cid cid cid theorem implies point objective value larger linear predictor flin unless partial derivative respect namely non zero cannot stationary point respect local minimum proof theorem appears supplementary material relies following key lemma shall state roughly sketch proof lemma fix cid vector size matrix cid cid cid cid vec cid flin word inner product gradient carefully chosen vector lower bounded suboptimality compared linear predictor particular point suboptimal gradient cannot zero cid cid cid vec cid cid cid proof sketch lemma cid vec cid cid cid cid cid cid cid cid vec let cid equal cid cid careful technical calculation reveals expression cid cid cid cid cid cid cid cid turn equal cid cid cid cid cid cid cid recalling nition cid noting convexity cid follows lower bounded cid cid cid cid cid cid flin cid cid cid analyze case following theorem cid cid cid cid cid cid min cid min flin cid cid cid cid cid cid min cid cid cid cid min denotes minimal eigenvalue symmetric matrix combining theorem following main theorem fix positive suppose convex open subset domain max cid cid cid cid lipschitz lipschitz lipschitz respectively cid cid sopsp min cid cid flin poly note poly term hide dependency linear individual factor see proof supplementary material exact expression discussed sec local minimum must correspond sopsp hence theorem implies point minw cid cid flin long satis lipschitz continuity assumption nite bounded subset domain since hold arrived following corollary corollary suppose bounded subset hold lipschitz continuous every cal minimum satis domain inf flin word objective spurious local minimum value smallest attainable linear predictor remark generalization vector valued output consider generalization setting network vector valued output namely cid matrix loss cid taking vector valued argument convex cross entropy loss general setting possible prove variant thm similar proof technique see appendix however clear prove analog thm hence thm leave question future research effect norm regularization thm implies sopsp must value much worse obtained linear predictor moreover discussed sec point closely related second order stationary point gradient based method known converge quickly point jin thus tempting claim method indeed network competitive linear predictor unfortunately fundamental catch bound thm depends norm point via cid cid cid cid arbitrarily bad norm suf ciently word thm guarantee point sopsp good long far away origin dynamic gradient method iterates remain bounded domain least suf ciently slowly increasing norm would issue however priori guaranteed would case since optimization problem unconstrained assuming anything structure could parameter diverge meaningful algorithmic derived thm course option dependence cid cid cid cid artifact analysis sopsp competitive linear predictor regardless norm however following example show case example fix suppose scalar squared loss dependence parameter cid objective equivalently written see leftmost plot gradient hessian equal cid cid cid cid left right contour plot superimposed constraint cid cid inside circle axis corresponds axis corresponds exhibit spurious local minimum bottom left quadrant domain best viewed color particular gradient hessian equal arbitrarily close small enough however objective value cid cid respectively point equal cid cid flin remark example gradient hessian uniformly bounded lipschitz constant euclidean space however lipschitz constant bounded numerical constant includes stationary point studied construction indicates problem indeed lie norm unbounded lipschitz constant derivative standard approach ensure iterates remain bounded add regularization namely optimize min regularization term penalizing norm unfortunately alter objective might introduce new spurious local minimum exist graphically illustrated plot example without regularization form whereas stationary point either global minimum along valley corresponding saddle point regularization created new spurious local minimum around intuitively regularization make objective value increase well valley global minimum regularization choice lead phenomenon similar issue occur impose hard constraint namely optimize min constrained domain illustrates optimization problem spurious local minimum inside constrained domain course way issue making regularization parameter suf ciently small domain suf ciently regularization come effect cid cid suf ciently however correct choice depends run problem simply xed example change time priori guarantee chosen thus clear xed choice regularization would work lead gradient based method good local minimum success sgd assuming skip connection output discussed challenge getting algorithmic previous section possible assuming architecture network changed bit concretely instead network architecture cid cid consider architecture parameterized vector new objective written cid cid cid cid cid cid cid cid cid cid architecture corresponds skip connection directly network output rather nal linear output layer similar spirit skip connection studied liang except layer nonlinear network instead linear cid component follows consider standard stochastic gradient descent sgd algorithm train network fixing step size convex parameter domain initialize point randomly data point underlying data distribution perform denote euclidean projection set cid cid cid note always differentiable respect assume simplicity differentiable respect simply cid still easily veri hold use notation flin cid arbitrary vector cid cid cid cid cid cid denote expected loss linear predictor parameterized following theorem establishes mild condition running stochastic gradient descent suf ciently many iteration result network competitive xed linear predictor theorem suppose domain satis following positive constant closed convex set clidean space namely cartesian product support data distribution cid cid cid lipschitz bounded absolute value cid cid cid cid cid suppose perform iteration stochastic gradient descent described step size satis probability least iterates min flin cid cid log cid proof relies technically straightforward perhaps unexpected reduction adversarial online learning appears supplementary material roughly speaking idea stochastic gradient descent procedure equivalent online gradient descent respect sequence function ned iterates even though iterates change unexpected complicated way strong guarantee online learning allow sequence function rather arbitrary allow obtain theorem acknowledgement thank anonymous nip reviewer helpful comment research supported part european research council erc grant reference peter bartlett david helmbold philip long gradient descent identity initialization ciently learns positive nite linear transformation deep residual network arxiv preprint arxiv alon brutzkus amir globerson eran malach shai shalev shwartz sgd learns parameterized network provably generalize linearly separable data arxiv preprint arxiv simon jason lee power parametrization neural network quadratic activation arxiv preprint arxiv simon jason lee yuandong tian barnabas poczos aarti singh gradient descent learns hidden layer cnn afraid spurious local minimum arxiv preprint arxiv rong tengyu optimization landscape tensor decomposition advance neural information processing system page rong jason lee tengyu learning hidden layer neural network landscape design arxiv preprint arxiv moritz hardt tengyu identity matter deep learning arxiv preprint arxiv elad hazan introduction online convex optimization foundation trend cid optimization kaiming xiangyu zhang shaoqing ren jian sun deep residual learning recognition proceeding ieee conference computer vision pattern recognition page kaiming xiangyu zhang shaoqing ren jian sun identity mapping deep residual network european conference computer vision page springer chi jin rong praneeth netrapalli sham kakade michael jordan escape saddle point ciently arxiv preprint arxiv jiwon kim jung kwon lee kyoung lee accurate super resolution deep convolutional network proceeding ieee conference computer vision pattern recognition page shiyu liang ruoyu sun yixuan srikant understanding loss surface neural network binary classi cation arxiv preprint arxiv garth mccormick modi cation armijo step size rule negative curvature mathematical programming yurii nesterov boris polyak cubic regularization newton method global performance mathematical programming itay safran ohad shamir spurious local minimum common layer relu neural network arxiv preprint arxiv shai shalev shwartz online learning online convex optimization foundation trend cid machine learning mahdi soltanolkotabi adel javanmard jason lee theoretical insight optimization landscape parameterized shallow neural network arxiv preprint arxiv daniel soudry elad hoffer exponentially vanishing sub optimal local minimum multilayer neural network arxiv preprint arxiv saining xie ross girshick piotr doll zhuowen kaiming aggregated residual transformation deep neural network computer vision pattern recognition cvpr ieee conference page ieee wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig microsoft conversational speech recognition system acoustic speech signal processing icassp ieee international conference page ieee chulhee yun suvrit sra ali jadbabaie critical view global optimality deep learning arxiv preprint arxiv martin zinkevich online convex programming generalized nitesimal gradient ascent proceeding international conference machine learning icml page)\n",
            "(34, 33): (discriminative topic modeling logistic lda iryna korshunova ghent university iryna korshunova ugent mateusz fedoryszak twitter mfedoryszak twitter com hanchen xiong twitter hxiong twitter com lucas theis twitter ltheis twitter com abstract despite many year research latent dirichlet allocation lda applying lda collection non categorical item still challenging yet many problem much richer data share similar structure could bene vast literature lda propose logistic lda novel discriminative variant latent dirichlet allocation easy apply arbitrary input particular model easily applied group image arbitrary text embeddings integrates well deep neural network although discriminative model logistic lda learn unlabeled data unsupervised manner exploiting group structure present data contrast recent topic model designed handle arbitrary input model sacri interpretability principled motivation lda introduction probabilistic topic model powerful tool discovering theme collection item typically collection assumed document model assign topic individual word however growing number real world problem require assignment topic much richer set item example may want assign topic tweet author twitter contain multiple sentence well image image website stored board pinterest video uploaded user youtube problem common grouped item likely thematically similar would like exploit dependency instead categorizing item based content alone topic model provide natural way achieve widely used topic model latent dirichlet allocation lda exception lda variant including supervised model generative generally assume multinomial distribution word given topic limit applicability discrete token conceptually easy extend lda continuous input modeling distribution complex data image dif cult task achieving low perplexity image example would require model many dependency pixel little use topic inference would lead inef cient model hand lot progress made accurately ciently assigning category image discriminative model convolutional neural network work goal build class discriminative topic model capable handling much richer item word time would like preserve lda extensibility interpretability particular group level topic distribution item independent given item topic topic topic distribution interact intuitive way model conference neural information processing system neurips vancouver canada achieves goal discarding generative part lda maintaining factorization conditional distribution latent variable neural network represent factor model deal arbitrary input type call model logistic lda connection lda analogous relationship logistic regression naive bayes textbook example discriminative generative approach desirable property generative model trained unsupervised manner section grouping item provides enough supervision train logistic lda otherwise unsupervised manner provide approach training model section describe mean eld variational inference used train model unsupervised semi supervised supervised manner section describe empirical risk minimization approach used optimize arbitrary loss label available topic model applied document topic associated individual word usually little interest contrast topic tweet twitter pin pinterest video youtube great interest therefore additionally introduce new annotated dataset tweet allows explore model ability infer topic item code datasets available github com lucastheis logistic lda related work latent dirichlet allocation lda latent variable model relates observed word xdn document latent topic kdn distribution topic speci following generative process document draw topic proportion dir word xdn draw topic assignment kdn cat draw word xdn cat cid kdn assume topic word represented vector hot encoding matrix row corresponds topic parametrizes categorical distribution word matrix either considered parameter model commonly latent variable dirichlet prior row dir graphical model corresponding lda provided blei used mean eld variational inference approximate intractable posterior latent variable kdn resulting closed form coordinate ascent update variational lower bound model log likelihood many method inference explored including gibbs sampling expectation propagation stochastic variant variational inference worth noting lda frequently used model word applied collection item example image viewed collection patch assigning patch discrete code word could directly apply model however example clustering simple way assign patch code word unclear choose different preprocessing approach principled manner zoo topic model many topic model built upon idea lda extended various way group method modi lda assumption regarding form kdn xdn kdn model becomes expressive example modeled sentence instead word blei jordan applied lda image extracting low level feature color texture gaussian distribution instead categorical distribution example include correlated topic model replace dirichlet distribution topic logistic normal distribution hierarchical dirichlet process enable unbounded number topic srivastava sutton pointed major challenge approach need rederive inference algorithm every change modeling assumption thus several paper kdn xdn kdn xdn kdn xdn graphical model lda supervised lda logistic lda gray circle indicate variable typically observed training proposed use neural variational inference tested approach simple categorical item word another direction extending lda incorporate extra information authorship time annotation class label feature work mainly interested inclusion class label cover wide range practical application model contrast slda disclda label interact topic proportion instead topic unlike lda label impose hard constraint topic proportion related area research model document without explicit representation topic instead generic latent variable model commonly implemented neural network sometimes referred document model distinguish topic model represent topic explicitly example document model include replicated softmax topicrnn nvdm sigmoid belief document model docnade finally non probabilistic approach topic modeling employ heuristically designed loss function example cao used ranking loss train lda inspired neural topic model alternative view lda section provide alternative derivation lda special case broader class model goal derive class model make easy handle variety data modality keep desirable inductive bias lda particular topic distribution item xdn independent given item topic kdn topic topic distribution interact intuitive way instead specifying generative model directed network assume factorization make following assumption complete conditionals cid cid cid cid cid dir kdn xdn cid exp kdn dnsoftmax xdn cid dng xdn cid rst condition requires topic distribution conditionally dirichlet distributed lda second condition express would like integrate information xdn calculate belief kdn function might neural network case simply act additional bias shared grouped item finally third condition express inference would look like knew topic word inference step akin classi cation problem label kdn exp act prior remaining factor act likelihood general arbitrary set conditional distribution guarantee corresponding joint distribution exists conditional distribution might inconsistent case joint distribution satisfy however whenever positive joint distribution exists use brook lemma form joint distribution case yield appendix cid cid cid cid cid exp cid cid dng xdn easy verify distribution satis constraint given equation furthermore posterior induced lda special case cid cid cid xdn xdn constrained cid appendix however describes larger class model share similar form posterior distribution risk relaxing assumption may prevent generalizing data interesting question therefore whether choice lead useful model particular normalized log likelihood lda lift constraint following answer question positively supervised extension many practical setting access label associated document dif cult extend model given equation supervised case however multiple way instance slda assumes class variable arises empirical frequency topic assignment kdn within document alternative would class label uence topic proportion instead motivating example latter consider case author document belong certain community tendency talk different topic thus even observing word new document knowing community provides information topic distribution slda hand belief topic distribution uenced label word xdn observed proposed supervised extension therefore assumes equation together following conditionals softmax cid appendix provides derivation corresponding joint distribution assumed document label hot vector extra scalar hyperparameter future work may want explore case number class different replaced learnable matrix weight kdn dir cid logistic lda alternative lda require cid let return question regarding possible choice interesting instead distribution word case encodes distribution topic word turn posterior discriminative classi rather posterior associated generative model word generally normalize corresponds discriminative log likelihood xdn softmaxf xdn output example dimensional logits neural network parameter note conditional distribution topic remains unchanged normalization similar logistic regression naive bayes implement linear classi er naive bayes make assumption distribution input revised model share conditional distribution topic lda longer make assumption distribution input xdn therefore refer lda type model whose take form logistic lda discriminative model typically require label training unlike discriminative model logistic lda already receives weak form supervision partitioning dataset encourages grouped item mapped topic unfortunately assumption logistic lda still slightly weak produce useful belief particular assigning topic kdn value high probability however found following regularizer enough encourage use topic allow unsupervised training cid cid cid cid exp xdn allow regularizer depend observed data otherwise affect math section control strength regularization regularizer effectively computes average distribution item topic predicted across whole dataset compare uniform distribution proposed regularizer allows discover meaningful topic logistic lda unsupervised manner although particular form regularizer may crucial make regularizer amenable stochastic approximation lower bound follows rdnk exp xdn rdnk rdnk exp xdn exp xdn cid cid cid xed rdnk evaluated lower bound gradient practice approximating denominator rdnk running average yielding estimate rdnk see appendix detail training inference mean eld variational inference approximate intractable posterior factorial distribution via mean eld variational inference minimizing kullback leibler divergence cid cid cid cid cid cid cid cid cid dkl kdn cid respect distribution assuming distribution concentrated point estimate derive following coordinate descent update variational parameter see appendix detail cid dir kdn cid pdn softmax cid pdn cid pdn softmax xdn digamma function logits neural network parameter see topic prediction word xdn computed based biased logits bias aggregate information across item group word document thus providing context individual prediction iterating equation arbitrary order implement valid inference algorithm xed note inference depend regularizer optimize neural network weight value variational parameter pdn regularization term rdn optimize divergence respect amount minimizing following cross entropy loss pdn rdn cid xdn cid cid corresponds classi cation problem soft label pdn rdn intuitively pdn try align prediction grouped item rdn try ensure topic predicted least time thus far presented general way training inference logistic lda assumed latent variable class label observed document replace training make method suitable unsupervised semi supervised supervised learning supervised training label developed discriminative training procedure discriminative training decision task associated loss function classi cation often care accuracy variational inference however approximates general property posterior ignoring task approximation going used leading suboptimal result enough label available classi cation goal therefore propose directly optimize parameter respect empirical loss instead divergence cross entropy loss cid cid cid see possible note update equation leading differen tiable operation effect unrolling mean eld update treat like layer sophisticated neural network strategy succesfully used example improve performance crfs semantic segmentation unrolling mean eld update lead training procedure given algorithm algorithm reveals training inference implemented easily even derivation needed arrive algorithm may seemed complex algorithm requires processing word document iteration appendix discus highly scalable implementation variational training inference require looking single item time useful setting many item item complex word algorithm single step discriminative training collection xdn require xdn class label uniform initial belief class niter cid cid pdn softmax cid end softmax pdn end cross entropy xdn output logits neural net experiment lot research done model related lda benchmark almost exclusively focused either document classi cation generative model perplexity however interested logistic lda ability discover topic document individual item well ability handle arbitrary type input therefore explore new benchmark first going look model ability discover topic tweet second going evaluate model ability predict category board pinterest based image connect literature topic model document classi er going logistic lda work well applied task document classi cation finally demonstrate logistic lda recover meaningful topic pinterest newsgroups unsupervised manner topic classi cation twitter collected set tweet rst set contains million tweet author author clustered based follower graph assigning author community cluster subsequently manually annotated based content typical tweet community community label thus provides idea content author likely produce second dataset contains million tweet author author label instead tweet manually annotated topic taxonomy used annotate community split rst dataset training validation test set tweet author contained set second dataset used evaluation due smaller size second dataset used fold cross validation estimate performance model training used community label uence distribution topic weight via author belonged multiple community label chosen random label noisy label used inference training tweet embedded averaging dimensional skip gram embeddings word logistic lda applied shallow mlp top embeddings trained stochastic approximation mean eld variational inference section baseline tried lda well training mlp predict community label directly predict community author mlp used majority voting across prediction tweet main difference majority voting logistic lda latter able reevaluate prediction tweet based tweet author lda extended open source implementation theis hoffman depend label manner logistic lda label bias topic proportion word tweet author combined form document frequent word corpus formed vocabulary lda predict topic kdm table show logistic lda able improve prediction purely discriminatively trained neural network author tweet level category principled inference allows improve accuracy prediction community author integrating information tweet allows improve prediction tweet topic lda performed worse task note label dataset noisy dif cult predict even human hence relatively low accuracy number tweet averaged lda belief topic word contained tweet cid table accuracy prediction annotation author tweet level author annotated community tweet topic lda refers supervised generative model model mlp individual mlp majority lda logistic lda author tweet categorization pinterest table accuracy newsgroups model test accuracy svm lstm lstm lstmp logistic lda illustrate logistic lda used image apply pinterest data board pin lda term every board would correspond document every pin pinned board word item purpose used subset pinterest dataset geng describe appendix noted however dataset contains board label thus without pin label able perform depth analysis previous section case twitter data trained logistic lda stochastic variational inference procedure comparison trained mlp predict label individual pin pin labeled category board model used embeddings mobilenetv input tuned hyperparameters validation set top image assigned different topic discovered logistic lda unsuper vised manner dog fashion architecture test accuracy predicting board label logistic lda mlp respectively mlp score obtained majority voting across pin compute board prediction trained logistic lda unsupervised manner embeddings subsequently mapped topic trained neural network logistic lda able learn coherent topic unsupervised manner example topic visualized detail result provided appendix document classi cation apply logistic lda discrimintive training section standard benchmark problem document classi cation newsgroups dataset newsgroups comprises around post partitioned almost evenly among topic various version dataset exist used preprocessed version cardoso cachopo result compared one lstm based classi er detail dataset given appendix trained logistic lda word represented dimensional glove embeddings perparameters selected based split training data listed appendix result experiment given table baseline include svm model trained idf document vector compare logistic lda lstm model document classi cation owes poorer performance instable training dif culties dealing long text issue overcome starting pretrained model intricate architecture lstm adopts former approach lstmp implement latter knowledge lstmp hold state art result newsgroups logistic lda surpass lstmp task performance compare favourably complex model remarkably achieves result lstm lstm classi initialized pretrained sequence autoencoder worth noting logistic lda us generic word embeddings lightweight model requires hour instead day train provides explicit representation topic accuracy driven benchmark interesting look performance supervised logistic lda trained loss insensitive objective described section best accuracy method signi cantly worse compared achieved logistic lda used cross entropy loss optimizing con rms usefulness optimizing inference task hand bene mean eld variational inference section allows train logistic lda unsupervised manner case model able discover topic one given table qualitative comparison srivastava sutton together npmi topic coherence score multiple model found appendix table example topic discovered unsupervised logistic lda represented top word bmw motor car honda motorcycle auto engine ford bike christianity prophet atheist religion holy scripture biblical catholic homosexual religious atheist spacecraft orbit probe ship satellite rocket surface shipping moon launch user computer microsoft monitor programmer electronic processing data app system congress administration economic accord trade criminal seriously ght responsible future discussion conclusion presented logistic lda neural topic model preserve lda inductive bias giving generative component favour discriminative approach making easier apply wide range data modality paper scratched surface may possible discriminative variant lda many inference technique developed lda could applied logistic lda example mean eld variational inference known prone local optimum trust region method able get around trained fairly simple neural network precomputed embeddings interesting question whether much deeper neural network trained weak supervision form grouped item interestingly logistic lda would considered discriminative model follow nition bishop lasserre according nition discriminative model joint distribution input label model parameter factorizes logistic lda hand admits factorization common choice marginal uence inference unlike generative model acknowledgment work done iryna korshunova intern twitter london thank ferenc husz jonas degrave guy hugot derville francisco ruiz dawen liang helpful discussion feedback manuscript reference besag spatial interaction statistical analysis lattice system journal royal statistical society series methodological bishop lasserre generative discriminative getting best world bayesian statistic blei lafferty correlated topic model science annals applied statistic blei jordan modeling annotated data proceeding annual international acm sigir conference research development informaion retrieval page blei mcauliffe supervised topic model advance neural information processing system blei jordan latent dirichlet allocation journal machine learning research cao liu novel neural topic model supervised extension aaai cardoso cachopo improving method single label text categorization phd thesis instituto superior tecnico universidade tecnica lisboa dai semi supervised sequence learning advance neural information processing system dieng wang gao paisley topicrnn recurrent neural network long range semantic dependency international conference learning representation geng zhang bian chua learning user feature recommendation social network ieee international conference computer vision page grif th steyvers finding scienti topic proceeding national academy science suppl hinton salakhutdinov replicated softmax undirected topic model advance neural information processing system hoffman blei bach online learning latent dirichlet allocation advance neural information processing system latent dirichlet allocation text image music aspect sentiment uni cation model online review analysis proceeding fourth acm international conference web search data mining wsdm page johnson zhang supervised semi supervised text categorization lstm region embeddings proceeding international conference international conference machine learning page krizhevsky sutskever hinton imagenet classi cation deep convolutional neural network commun acm lacoste julien sha jordan disclda discriminative learning dimensionality reduction classi cation advance neural information processing system lacoste julien husz ghahramani approximate inference loss calibrated bayesian proceeding fourteenth international conference arti cial intelligence statistic page lang newsweeder learning lter netnews proceeding international conference machine learning larochelle lauly neural autoregressive topic model information processing system advance neural lau newman baldwin machine reading tea leaf automatically evaluating topic coherence topic model quality proceeding conference european chapter association computational linguistics miao grefenstette blunsom discovering discrete latent topic neural variational inference proceeding international conference machine learning page miao blunsom neural variational inference text processing proceeding international conference machine learning page mikolov chen corrado dean cient estimation word representation vector space international conference learning representation mimno mccallum topic model conditioned arbitrary feature dirichlet multinomial regression uncertainty arti cial intelligence minka lafferty expectation propagation generative aspect model proceeding eighteenth conference uncertainty arti cial intelligence page mnih gregor neural variational inference learning belief network proceeding international conference international conference machine learning jordan discriminative generative classi er comparison logistic regression naive bayes advance neural information processing system mit press pandey dukkipati discriminative neural topic model arxiv preprint ab pennington socher manning glove global vector word representation empirical method natural language processing emnlp page ramage hall nallapati manning labeled lda supervised topic model credit attribution multi labeled corpus proceeding conference empirical method natural language processing page rosen zvi grif th steyvers smyth author topic model author document proceeding conference uncertainty arti cial intelligence page sandler howard zhu zhmoginov chen mobilenetv inverted residual linear bottleneck ieee cvf conference computer vision pattern recognition page srivastava sutton neural variational inference topic model bayesian deep learning workshop teh jordan beal blei hierarchical dirichlet process journal american statistical association theis hoffman trust region method stochastic variational inference application streaming data http github com lucastheis trlda wang blei fei fei simultaneous classi cation annotation ieee conference computer vision pattern recognition wang mccallum topic time non markov continuous time model topical trend proceeding acm sigkdd international conference knowledge discovery data mining page zheng jayasumana romera paredes vineet huang torr conditional random field recurrent neural network ieee international conference computer vision iccv page, mixture model system medical machine diagnosis magnus stensmo terrence sejnowski computational neurobiology laboratory salk institute biological study north torrey pine road jolla magnus terry salk edu abstract diagnosis human disease machine fault missing data problem since many variable initially unknown additional information need obtained oint probability distribution data used solve problem model mixture model whose parameter estimated algorithm give benefit missing data database handled correctly request new information refine diagnosis performed maximum utility principle since system based learning domain independent le labor intensive expert system probabilistic network example heart disease database presented introduction diagnosis process identifying disease patient disorder machine considering history symptom sign examination diagnosis common important problem proven hard automate formalize procedural description often hard attain since expert know exactly solve problem paper use information specific problem exists database magnus stensmo terrence sejnowski case disorder disease determined variable observation goal find probability distribution disorder conditioned observed diagnosis strong possible outcome differentiated others information needed inconclusive initially clue rest variable unknown additional information obtained asking question test since test may dangerous time consuming expensive generally possible desirable find answer every question unnecessary test avoided many attempt automate diagnosis early work ledley lusted realized problem always tractable due number influence exist symptom disease expert system internist system internal medicine miller rule base hard time consuming build inconsistency may arise new rule added existing database strong domain dependence knowledge base rarely reused new application bayesian probabilistic network pearl way model joint probability distribution factoring chain rule probability theory although model powerful built presently general learning method construction considerable effort needed pathfinder system lymph node pathology heckerman conditional probability assessed expert pathologist inevitable error occur number manual assessment involved approach diagnosis based domain independent machine learning alleviate problem knowledge engineering decision tree quinlan piece information used appropriate question come traversing tree mean irrelevant question avoided feedforward multilayer perceptrons diagnosis baxt classify well need full information case none method adequate way handle missing data learning classification exponentially growing number probability involved make exact diagnosis intractable simple approximation independence variable condi cid tional independence given disease naive bayes introduce error since usually dependency symptom even though system based assumption work surprisingly well correct diagnosis guaranteed paper avoid assumption mixture model mixture model diagnosis formulated probability estimation problem missing input probability disorder conditioned currently observed model joint probability distribution easy marginalize get conditional probability necessary order able handle missing data principled way ahmad tresp mixture model mclachlan basford simple closed form solution optimal regression missing data formulated algorithm method parametric statistic parameter estimation especially interesting context since formulated handle missing data mixture model system medical machine diagnosis training example dempster ghahramani jordan algorithm data underlying model assumed set dimensional vector data point assumed generated independently mixture density component zlwj mixture component denoted priori probability mixturewj model parameter estimate parameter different mixture likely linear com cid bination generated set data point use maximum likelihood estimation good method iterative expectation maximization algorithm dempster step repeated first likelihood formulated expectation computed step type model use step calculate probability certain mixture component generated data point question second step step parameter maximize expectation found found analytically model written exponential form gaussian function equation derived batch line learning update equation gaussian distribution without missing data given distribu cid tions possible binomial multinomial stensmo sejnowski detail derivation found dempster nowlan ghahramani jordan stensmo sejnowski form log likelihood data logp log iwj unfortunately analytic solution logarithm sum right hand side equation however know mixture generated data point could compute algorithm solves introducing set binary indicator variable zij zij data point generated mixture component log likelihood manipulated form contain log sum expectation current parameter value used since known directly step algorithm expected value maximized step step iterated convergence likelihood never decrease iteration dempster convergence fast compared gradient descent main motivation algorithm able handle missing value variable data set principled way complete data case introduced missing indicator variable helped solve problem missing data add missing component already missing dempster ghahramani jordan magnus stensmo terrence sejnowski gaussian mixture specialize algorithm case mixture component radial gaussian distribution mixture component mean covariance matrix form covariance matrix often constrained diagonal value diagonal corresponds axis parallel oval shaped radially symmetric gaussians respectively radial diagonal basis function function well application nowlan since several gaussians together form complex shape space fewer parameter fitting minimized radial case variance step expected value likelihood computed gaussian case becomes probability gaussian generated data point step find parameter maximize likelihood step complete data new estimate input variable missing evaluated set observed dimension missing unobserved dimension denoted update equation unchanged estimate itj set ity use variance becomes least square regression used fill missing data value classification missing variable gaussian mixture becomes approach used ahmad tresp regression outcome variable missing probability distribution disorder reduced classification comparison system picking outcome maximum estimated probability mixture model system medical machine diagnosis requesting information diagnosis process outcome probability refined step based newly acquired knowledge important select question lead minimal number necessary test generally cost associated test goal minimize total cost early work automated diagnosis ledley lusted acknowledged problem asking question possible suggested use decision analysis solution important idea field decision theory maximum expected utility principle von neuman morgenstern decision maker always choose alternative maximizes expected utility decision diagnosis cost misclassification pair outcome utility correct diagnosis incorrectly determined expectation computed know probability outcome utility value assessed manually lengthy complicated process reason simplification function suggested hecker cid man utility benign malign otherwise simplification found work well practice another complication maximum expected utility principle make intractable ideal case would evaluate every possible sequence future choice see best since size search tree possibility grows exponentially often possible simplification ahead step time nearsighted myopic approach tested practice good result gorry barnett heckerman diagnosis system system developed phase first learning phase probabilistic model built model used inference diagnosis phase learning phase joint probability distribution data modeled mixture model parameter determined database case algorithm mean algorithm used initialization input output variable case combined vector per case form set training pattern outcome nominal variable coded continuous variable interval coded diagnosis phase myopic step look ahead used utility simplified following step performed initial observation entered conditional expectation regression used fill unknown variable maximum expected utility principle used recommend next obser cid vation make stop nothing would gained observation user asked determine correct value recommended observa cid tion observation could made instead addition continue step magnus stensmo terrence sejnowski table cleveland heart disease database observation description age year age sex subject sex chest pain resting blood pressure trestbps serum cholesterol chol fasting blood sugar fbs resting electrocardiogr restecg thalach max heart rate achieved exercise induced angina exang depr induced oldpeak exercise relative rest slope peak exercise stsegment major f col ftourosc defect type description heart disease thai disorder num slope value continuous male female type continuous continuous value continuous yes continuous flat normal fixed reversible value present type example cleveland heart disease data set irvine used test system contains example type heart disease absence thirteen continuous nominally valued variable table continuous variable interval coded unit per standard deviation away mean value chosen since approximately normally distributed nominal variable coded unit per value total variable coded unit step repeated convergence iteration varying number mixture component tried previously reported result used presence absence heart disease best classification rate system incrementally built prototype gennari obtained correct classification radial gaussian mixture described performance increased number mixture component sensitive varying number mixture component training unless previous investigator pointed enough information thirteen variable data set reach gennari annotated transcript diagnosis session shown conclusion work several property model remain investigated tested several database unfortunately database typically proprietary difficult obtain future prospect medical database good since hospital computerized record system instead traditional paper based fairly easy mixture model system medical machine diagnosis leftmost number number line estimated probability heart disease followed probability type heart disease entropy defined log diagnosis given time measure decisive current conclusion completely detennined diagnosis entropy initially variable unknown starting diagnosis unconditional prior probability disorder entropy first question chest pain answer change estimated probability variable continuous answer interpreted far mean observation standard deviation decision becomes conclusive entropy decrease disorder entropy age disorder entropy oldpeak disorder entropy chol disorder entropy detennined probability heart disease case remaining spread possibility diagnosis example generate data machine diagnosis alternative way choose new question evaluate variance change output variable variable changed missing observed idea variable known certainty zero variance variable largest resulting conditional variance could selected query similar cohn important aspect automated diagnosis accompanying explanation conclusion factor important user acceptance since basis function local support since estimate probability basis function generated observed data explanation conclusion could generated instead simplified utility value expected utility calcula cid tions could learned reinforcement learning trained expert would evaluate quality diagnosis performed system followed adjustment utility value used starting value magnus stensmo terrence sejnowski acknowledgement heart disease database university california irvine repository machine learning database originates detrano cleveland clinic foundation peter dayan provided helpful comment earlier version paper reference ahmad tresp solution missing feature problem vision advance neural information processing system vol morgan kaufmann san mateo baxt use artificial neural network data analysis clinical decision cid making diagnosis acute coronary occlusion neural computation cohn ghahramani jordan active learning statistical model advance neural information processing system vol morgan kaufmann san mateo dempster laird rubin maximum likelihood incomplete data via algorithm journal royal statistical society series gennari langley fisher model incremental concept formation artificial intelligence ghahramani jordan supervised learning incomplete data via approach advance neural information processing system vol morgan kaufmann san mateo gorry barnett experience model sequential diagnosis computer biomedical research heckerman horvitz nathwani toward normative expert system part pathfinder project method information medicine ledley lusted reasoning foundation medical diagnosis science mclachlan basford mixture model inference application clustering marcel dekker inc new york miller pople myers internist experimental computer cid based diagnostic consultant general internal medicine new england journal medicine nowlan soft competitive adaptation neural network learning algorithm based fitting statistical mixture phd thesis school computer science carnegie mellon university pittsburgh pearl probabilistic reasoning intelligent system network plausible inference morgan kaufmann san mateo quinlan induction decision tree machine learning stensmo sejnowski mixture model diagnosis system tech rep inc institute neural computation university california san diego von neuman morgenstern theory game economic behavior princeton university press princeton)\n",
            "(41, 40): (effective distributed via stale synchronous parallel parameter server qirong james cipar henggang cui jin kyu kim seunghak lee phillip gibbon garth gibson gregory ganger eric xing intel lab electrical computer engineering pittsburgh phillip gibbon intel com school computer science carnegie mellon university pittsburgh carnegie mellon university pittsburgh qho jcipar jinkyuk hengganc ganger ece cmu edu seunghak garth epxing cmu edu abstract propose parameter server system distributed follows stale synchronous parallel ssp model computation maximizes time com putational worker spend useful work algorithm still provid ing correctness guarantee parameter server provides easy use shared interface read write access model value parameter vari ables ssp model allows distributed worker read older stale version value local cache instead waiting get central storage signi cantly increase proportion time worker spend com puting opposed waiting furthermore ssp model ensures algorithm correctness limiting maximum age stale value provide proof correctness ssp well empirical result demonstrating ssp model achieves faster algorithm convergence several different problem compared fully synchronous asynchronous scheme introduction modern application awaiting next generation machine intelligence system posed unprece dented scalability challenge scalability need arise least aspect massive data volume societal scale social graph hundred million node massive model size google brain deep neural network containing billion parameter although exist mean theory support reductionist approach like subsam pling data small model imperative need sound effective distributed methodology user cannot well served shortcut recent effort towards dis tributed made signi cant advancement direction leveraging existing common simple distributed system implement parallel version limited selection model shown strong theoretical guarantee parallelization scheme cyclic delay model pre partitioning lock free update bulk synchronous parallel even synchronization scheme simple implement may exploit full computing power distributed cluster building high throughput distributed architec tures algorithm implementation feature signi cant system contribution relatively le theoretical analysis graphlab spark pregel yahoolda aforementioned work signi cant contribution right naturally desirable goal distributed pursue system maximally unleash combined compu tational power cluster given size spending time useful computation le time waiting communication support inference broad collection method enjoys correctness guarantee paper explore path system idea parameter server combination shared key value store provides centralized storage model may implemented distributed fashion synchronization model reading updating model value key value store provides easy program read write access shared parameter needed worker synchronization model maximizes time worker spends useful computation versus communication server still providing algorithm correctness guarantee towards end propose parameter server stale synchronous parallel ssp model computation distributed algorithm parallelized many computational worker technically thread spread many machine ssp worker make update param eter update follow associative commutative form hence current true value sum update worker worker asks ssp model give stale delayed version excludes recent update formally worker reading iteration see effect iteration user controlled staleness threshold addition worker may get see recent update beyond iteration idea ssp system deliver many update possible without missing update older given age concept referred bounded staleness practical effect twofold worker perform computation instead waiting worker nish worker spend le time communicating parameter server time useful computation bounded staleness distinguishes ssp cyclic delay system read exible staleness bulk synchronous parallel bsp system like hadoop worker must wait end every iteration completely asynchronous system worker never wait staleness guarantee implement ssp parameter server table based interface called ssptable support wide range distributed algorithm many model application ssptable run distributed fashion order increase performance support application parameter machine moreover ssptable take advantage bounded staleness maximize algorithm performance reading parameter cache worker machine whenever possible reading parameter server ssp model requires thus worker spend le time waiting spend le time communicating parameter server furthermore ssptable help slow straggling worker catch providing system based solution last reducer problem system like hadoop note theory based solution possible ssptable run multiple server machine called shard thus dividing workload cluster manner ssptable service worker simultaneously support model cannot single machine finally ssptable server program run worker machine provides simple effective strategy allocating machine worker parameter server theoretical analysis show ssp generalizes bulk synchronous parallel bsp model stochastic gradient algorithm matrix factorization topic model ssp converge least fast cyclic delay system potentially even faster depending implementation furthermore implementation ssp ssptable support wide variety algortihms model demonstrate several popular one trix factorization stochastic gradient descent topic modeling collapsed gibbs sampling lasso regression parallelized coordinate descent experimental result model algorithm ssp yield faster convergence bsp several time faster ssp yield faster convergence fully asynchronous stal ene guarantee system explain ssptable better performance term algorithm progress per iteration quality iteration executed per unit time quantity ssptable hit sweet spot quality quantity missed bsp fully asynchronous system stale synchronous parallel model computation begin informal explanation ssp assume collection worker make additive update shared parameter regular interval called clock clock similar iteration represent unit progress algorithm every worker example parameter might topic word distribution lda factor matrix matrix decomposition update could adding removing count topic word document word table lda stochastic gradient step matrix decomposition forced wait slowest worker catch integer valued clock worker commit update end clock update may immediately visible worker trying read word worker see effect stale subset update idea staleness worker retrieve update cache machine fast instead querying parameter server network slow given user chosen staleness threshold ssp enforces following bounded staleness condition see graphical illustration slowest fastest worker must clock apart otherwise fastest worker worker clock commits update timestamped time worker clock read always see effect timestamp may see timestamp worker read writes worker always see effect update since fastest slowest worker clock apart worker reading clock see update time tamp plus possi bly empty adaptive subset update range note guaranteed range come adaptive range becomes empty exactly bulk synchronous parallel model computa tion let look ssp applies example algorithm example stochastic gradient descent matrix problem stochastic gradient descent sgd algorithm optimizes objective function plying gradient descent random subset data consider matrix completion task involves decomposing matrix low rank matrix size user speci data matrix may missing entry corresponding missing data concretely could matrix user product dij representing user rating product user rate possible product goal predict rating missing entry dab given known entry dij found low rank matrix dij known entry dij could predict dab unknown entry dab perform decomposition let minimize squared difference known entry dij prediction note loss function regularizers possible bounded staleness ssp model min data dij kxk likrkj data rst step towards sgd consider solving coordinate gradient descent omf lik dabrkb brkb dablak blak omf rkj data omf objective equal otherwise transformed sgd algorithm replacing full sum entry subsample appropriate reweighting entry dab distributed multiple worker gradient computed parallel assume tall transpose true partition row processor need shared among processor let ssp shared parameter ssp allows many worker read write minimal waiting though worker see stale value tradeoff bene cial without staleness worker must wait long time reading server experiment stale value decrease convergence progress per iteration ssp make enabling signi cantly iteration per minute compared fully synchronous system thus ssp yield convergence progress per minute faster convergence clock ssp bounded staleness clock update visible worker worker worker worker worker staleness threshold update visible worker due read writes update necessarily visible worker worker must wait read worker reached clock worker progress client process table server table server table server table server table data pending request application application thread application thread application thread thread thread thread cache thread cache thread cache cache process cache cache structure ssptable multiple server shard note ssp limited stochastic gradient matrix algorithm applied parallel collapsed sampling topic model storing word topic document topic table parallel coordinate descent lasso regression storing regression coef cients well parallel algorithm model shared parameter worker need read write access experiment ssp performs better bulk synchronous parallel asynchronous system matrix completion topic modeling lasso regression ssptable cient ssp system ideal ssp implementation would fully exploit lee way granted ssp bounded staleness property order balance time worker spend waiting read need freshness shared data section describes initial implementation ssptable parameter server conforming ssp model run many server machine dis tributed experiment ssptable implemen tation show ssp indeed improve convergence rate several model algorithm fur ther tuning cache management policy could improve performance ssptable ssptable follows distributed client server architecture client access shared parameter client library maintains machine wide process cache optional per thread thread cache latter useful improving performance reducing inter thread synchronization force worker wait client program executes multiple worker thread multiple core client machine server parameter state divided sharded multiple server machine normal con guration would include server process client machine programming ssptable follows simple table based api reading writing shared parameter example matrix sgd example section table organization ssptable support unlimited number table divided read row table row retrieve table row staleness threshold user inc table row val increase table row element val negative clock inform server current thread processor completed clock number read row inc call made call clock differ ent thread worker permitted different clock however bounded staleness requires fastest slowest thread clock apart situation ssptable force fastest thread block wait call read row slowest thread caught maintain read writes property use write back policy writes immediately committed thread cache ushed process cache server upon clock maintain bounded staleness minimizing wait time read row operation ssptable us following cache protocol let every table row thread process cache endowed clock rthread rproc respectively let every thread worker endowed clock equal number time called clock finally server clock cserver minimum thread clock thread clock request table row rst check thread cache row cached clock rthread read row otherwise check process cache next row cached clock rproc read row point network traf incurred yet however cache miss network request sent server force thread wait reply server return view table row well clock cserver fastest slowest thread clock apart thread update sent server whenever call clock returned server view always satis bounded staleness requirement row subdivided element table used store change propagated server next call clock commit outstanding inc server query individual row element assume every computation thread corresponds algorithm worker asking thread fetching row server corresponding entry thread process cache clock rthread rproc overwritten server view clock cserver bene cial consequence cache protocol slowest thread performs costly server read every clock faster thread may perform server read frequently frequently every clock consistently waiting slowest thread update distinction work per thread occur bsp wherein every thread must read server every clock thus ssp reduces overall network traf thus reducing wait time server read allows slow straggler thread avoid server read iteration hence slow thread naturally catch turn allowing fast thread proceed instead waiting manner ssp maximizes time machine spends useful computation rather waiting theoretical analysis ssp formally ssp model support operation member ring abelian operator addition multiplication operator ring context shall focus addition multiplication real vector scalar coef cients operation found update equation many inference algorithm gradient descent coordinate descent collapsed gibbs sampling follows shall informally refer system state update operation writing update assume worker write update regular time interval referred clock let update written worker clock write operation update function system state ssp model different worker see different noisy version true state let noisy state read worker clock implying function formally state bounded staleness key ssp condition bound possible value take ssp condition bounded staleness fix staleness noisy state equal pxp guaranteed pre window update guaranteed read writes update best effort window update subset update written width window range clock include update worker word noisy state consists part update made querying worker guaranteed pre window update clock worker guaranteed read writes set cover window best effort window update width window counting update worker ssp implementation try deliver many update possible may choose depending condition notice speci worker clock worker different clock observe different observe ssp generalizes bulk synchronous parallel bsp model bsp corollary zero staleness ssp reduces bsp proof implies therefore exactly consists update clock key tool convergence analysis reference sequence state informally referred true sequence different unrelated ssptable server view mod txt word sum update rst looping worker mod clock bound difference true sequence noisy view read writes self synchronization property worker always see update make property make sense self synchronization incur network cost width upper bound slowest worker fastest worker clock cmax width window cmax cmax simply update clock cmax written yet lemma assume let mod missing update extra update decomposed difference index set update missing index set extra update claim furthermore min max max proof comparing see extra update obey mod missing update obey mod mod mod rst claim immediately follows second third claim follow looking left right boundary mod lemma basically say true state noisy state differ update update cannot step away property used prove convergence bound various algorithm paper shall focus stochastic gradient descent sgd theorem sgd ssp suppose want minimizer convex function via gradient descent component rft time assume component convex let trft certain constant suitable condition lipschitz distance point xkx txt mean noisy worker view converge expectation true view measured function rate defer proof appendix noting generally follows analysis langford except place lemma involved bound similar except xed delay replaced staleness upper bound shown convergence noisy worker view rather true sequence furthermore constant factor upper bound number erroneous update ssp rate convergence potentially tighter constant factor langford xed staleness system detail appendix experiment ssp model outperforms fully synchronous model bulk synchronous parallel bsp require worker wait every iteration well asynchronous model model staleness guarantee general experimental detail computational model implementation ssp bsp asynchronous used ssptable rst bsp staleness ssp implemented asynchronous model many caching feature ssptable keep implementation comparable model parallel algorithm lda topic modeling collapsed gibbs sampling matrix fac torization stochastic gradient descent lasso regression coordinate gradient descent algorithm implemented ssptable parameter server interface ran algorithm full batch mode algorithm worker collectively touch every data point per clock well minibatch model worker touch data per clock due implementa tion limitation run lasso async model datasets topic modeling new york time token term topic matrix factorization netflix matrix nonzeros rank decomposition lasso regression synthetic dataset sample feature use static data partitioning strategy explained appendix compute cluster multi core blade server connected gbps ethernet running vmware esx use virtual machine per physical machine con gured core either ghz ghz ram running top debian linux asynchronous model used many framework yahoolda hogwild largest data size could get lasso algorithm converge ideal bsp condition convergence speed show objective time plot algorithm several machine con gurations interested long algorithm take reach given objective value corresponds drawing horizontal line plot plot curve bsp zero staleness async ssp best staleness value generally omit ssp curve reduce clutter case except topic modeling vms ssp converges given objective value faster bsp async gap ssp system increase vms smaller data batch factor lead increased network communication ssp able reduce via staleness provide scalability machine plot appendix computation time network waiting time understand ssp performs better look topic modeling algorithm spends time xed number clock row see machine con guration algorithm spends roughly amount time useful computation regardless staleness value however time spent waiting network communication drop rapidly even small increase staleness allowing ssp execute clock quickly bsp staleness furthermore ratio network compute time increase add vms use smaller data batch vms data minibatches algorithm bsp spends six time time network communication computation contrast optimal value staleness exhibit ratio communication computation hence value ssp lie allowing algorithm perform far useful computation per second compared bsp model hadoop similar observation hold lasso application graph shown space reason iteration quantity quality network compute ratio partially explains ssp behav ior need examine clock behavior get full picture row plot number clock executed per worker per unit time algorithm well objective value clock higher staleness value increase number clock executed per unit time decrease clock progress towards convergence suggested theory lasso exhibit similar behavior graph shown thus staleness tradeoff tween iteration quantity quality iteration rate exhibit diminishing return higher staleness value come point additional staleness start hurt rate convergence per time explains best staleness value given setting constant hence ssp hit sweet spot quality quantity bsp async achieve automatically nding sweet spot given problem subject future work related work discussion idea staleness explored academia analyzed con text cyclic delay architecture machine communicate central server xed schedule hence xed staleness even bulk synchronous paral lel bsp model inherently produce stale communication effect studied algorithm lasso regression topic modeling work differs ssp advocate bounded rather xed staleness allow higher computational throughput via local machine cache furthermore ssp performance degrade parameter update fre quently collide vector element unlike asynchronous lock free system note staleness informally explored industrial setting scale work provides rst attempt rigorously justifying staleness sound technique distributed platform hadoop graphlab popular scale biggest difference ssptable programming model hadoop us stateless map reduce model graphlab us stateful vertex program organized graph con trast ssptable provides convenient shared memory programming model based table matrix api making easy convert single machine parallel algorithm distributed version particular algorithm used experiment lda lasso straightforward conversion single machine algorithm hadoop bsp execution model special case ssp making ssptable general regard however hadoop provides fault tolerance distributed lesystem feature ssptable cover finally exist special purpose tool vowpal wabbit yahoolda whereas system targeted subset algorithm ssptable used algorithm tolerates stale update distributed system community typically examined staleness context consistency model tact model describes consistency along dimension numerical error order error staleness work attempt classify existing system according number vms topic modeling convergence vms vms minibatches topic modeling computation time network waiting time vms vms vms minibatches topic modeling iteration quantity quality vms minibatches vms minibatches lasso convergence vms vms matrix factorization convergence vms vms minibatches experimental result ssp bsp asynchronous parameter server running topic modeling matrix factorization lasso regression convergence graph plot objective function solution quality time topic modeling plot computation time network waiting time well staleness affect iteration clock frequency quantity objective improvement per iteration quality consistency property speci cally naming concept bounded staleness vector clock used ssptable similar fidge mattern turn inspired lamport clock however ssptable us vector clock track freshness data rather causal relationship update give informal nition ssp model motivated need reduce straggler effect compute cluster database bounded staleness applied improve update query performance lazy base allows staleness bound con gured per query basis us relaxed stale ness improve query update performance fa keep data replicated number database providing different freshness performance tradeoff data stream warehouse collect data timestamped event provide different consistency depending freshness data staleness freshness timeliness applied eld sensor network dynamic web content generation web caching information system acknowledgment qirong supported n phd fellowship star singapore work supported part nih darpa nsf ii eric xing thank member company pdl consortium acti apc emc emulex facebook fusion google hitachi huawei intel microsoft nec netapp oracle panasas samsung seagate symantec vmware western digital interest insight feedback support work supported part intel via intel science technology center cloud computing istc hardware donation intel netapp log likelihood second objective function versus time lda machine thread full data per iter bsp stale stale async log likelihood second objective function versus time lda machine thread full data per iter bsp stale stale async log likelihood second objective function versus time lda machine thread data per iter bsp stale stale async second staleness time breakdown compute network lda machine full data network waiting time compute time second staleness time breakdown compute network lda machine full data network waiting time compute time second staleness time breakdown compute network lda machine data network waiting time compute time iteration clock second quantity iteration versus time lda machine data bsp stale stale stale stale stale stale stale log likelihood iteration clock quality objective versus iteration lda machine data bsp stale stale stale stale stale stale stale objective second objective function versus time lasso machine thread bsp stale stale stale stale stale objective second objective function versus time machine thread full data per iter bsp stale stale async objective second objective function versus time machine thread full data per iter bsp stale stale async objective second objective function versus time machine thread data per iter bsp stale stale async reference agarwal duchi distributed delayed stochastic optimization decision control cdc ieee annual conference page ieee ahmed aly gonzalez narayanamurthy smola scalable inference latent variable model wsdm page alexandros labrinidis balancing performance data freshness web database server page september bouzeghoub framework analysis data freshness proceeding international workshop information quality information system iqis page bradley kyrola bickson guestrin parallel coordinate descent regularized loss minimization international conference machine learning icml june bright raschid latency recency pro le data delivery web proceeding international conference data base vldb page cipar ganger keeton morrey iii soules veitch lazybase trading freshness performance scalable database proceeding acm european conference computer system page cipar kim lee ganger gibson keeton xing solving straggler problem bounded staleness hotos usenix dean corrado monga chen devin mao ranzato senior tucker yang scale distributed deep network nip facebook www facebook com note php note january fidge timestamps message passing system preserve partial ordering au tralian computer science conference page university queensland australia gemulla nijkamp haas sismanis scale matrix factorization distributed stochastic gradient descent kdd page acm golab johnson consistency stream warehouse cidr page huang loft low overhead freshness transmission sensor network sutc page washington usa ieee computer society lamport time clock ordering event distributed system commun acm july langford strehl vowpal wabbit online learning project langford smola zinkevich slow learner fast advance neural information processing system page low joseph aapo bickson guestrin hellerstein joseph distributed graphlab framework machine learning data mining cloud pvldb malewicz austern bik dehnert horn leiser czajkowski pregel system scale graph processing proceeding international conference man agement data page acm mattern virtual time global state distributed system editor proc workshop parallel distributed algorithm page north holland elsevier niu recht wright hogwild lock free approach parallelizing stochastic gradient descent nip power piccolo building fast distributed program partitioned table proceeding usenix conference operating system design implementation osdi page ohm ohm schek schuldt fa freshness sensitive coordination middleware cluster olap component vldb page vldb endowment terry replicated data consistency explained baseball technical report msr microsoft research october yahoo http webscope sandbox yahoo com catalog php datatype vahdat design evaluation conit based continuous consistency model replicated service acm transaction computer system aug zaharia chowdhury franklin shenker stoica spark cluster computing working set proceeding usenix conference hot topic cloud computing zinkevich weimer smola parallelized stochastic gradient descent advance neural information processing system, cortical microcircuit gated recurrent neural network rui ponte costa yannis assael centre neural circuit behaviour dept computer science dept physiology anatomy genetics university oxford oxford university oxford oxford rui costa cncb deepmind london yannis assael brendan shillingford dept computer science university oxford oxford deepmind london brendan shillingford nando freitas deepmind london nandodefreitas google com tim vogels centre neural circuit behaviour dept physiology anatomy genetics university oxford oxford tim vogels cncb abstract cortical circuit exhibit intricate recurrent architecture remarkably similar across different brain area stereotyped structure suggests existence common computational principle however principle remained largely elusive inspired gated memory network namely long short term memory network lstms introduce recurrent neural network information gated inhibitory cell subtractive sublstm propose natural mapping sublstms onto known canonical excitatory inhibitory cortical microcircuit empirical evaluation across sequential classi cation language modelling task show sublstm unit achieve similar performance lstm unit result suggest cortical circuit optimised solve complex contextual problem proposes novel view computational function overall work provides step towards unifying recurrent network used machine learning biological counterpart introduction last decade neuroscience research collected enormous amount data chitecture dynamic cortical circuit unveiling complex stereotypical structure across neocortex markram harris mrsic flogel jiang prevalent feature cortical net laminar organisation high degree recurrence even level local micro circuit douglas song harris mrsic flogel jiang another key feature cortical circuit detailed tight balance excitation inhibition received growing support author contributed equally work conference neural information processing system nip long beach usa experimental froemke xue froemke theoretical level van vreeswijk sompolinsky brunel vogels abbott hennequin however computational process facilitated architecture dynamic still elusive remains fundamental disconnect underlying biophysical network emergence intelligent complex behaviour arti cial recurrent neural network rnns hand crafted perform speci computation fact rnns recently proven successful solving complex task language modelling speech recognition perceptual task graf graf sutskever van den oord assael task input data contains information across multiple timescales need ltered processed according relevance ongoing presentation stimulus make dif cult learn separate meaningful stimulus background noise hochreiter pascanu rnns particular gated rnns solve problem maintaining representation relevant input sequence needed without interference new stimulus principle protected memory conserve past input thus allow back propagation error backwards time pascanu memory property rst successful type gated rnns named long short term memory network lstms hochreiter schmidhuber note architectural feature lstms overlap closely known cortical structure important difference regard mechanistic implementation gate cortical network lstms lstms gate control memory cell multiplicative factor biological network gate inhibitory neuron act rst approximation subtractively excitatory inhibitory current cancel linearly level postsynaptic membrane potential kandel gerstner moreover subtractive inhibitory mechanism must well balanced closely match excitatory input act gate input closed state without perturbing activity much inhibition previous model explored gating subtractive excitatory inhibitory balanced network vogels abbott kremkow without clear computational role hand predictive coding rnns feature studied bastos deneve machens without clear match state art machine learning network regarding previous neuroscienti interpretation lstms suggestion lstms model working memory different brain area prefrontal cortex basal ganglion hippocampus reilly frank krueger dayan cox dean marblestone hassabis bhalla without clear interpretation individual component lstms speci mapping known circuit propose map architecture function lstms directly onto cortical circuit gating provided lateral subtractive inhibition network potential exhibit excitation inhibition balance observed experiment douglas bastos harris mrsic flogel yield simpler gradient propagation multiplicative gating study dynamic empirical evaluation showing sublstms achieve similar performance lstms penn treebank wikitext language modelling task well pixelwise sequential mnist classi cation transferring functionality lstms biologically plausible network work provides testable hypothesis recently emerging technologically advanced experiment functionality entire cortical microcircuit biological motivation architecture lstm unit general feedforward structure aided additional recurrent memory controlled lateral gate remarkably similar columnar architecture cortical circuit see detailed neocortical schematic central element lstms similar rnns memory cell hypothesise implemented local recurrent network pyramidal cell layer line previous study showing relatively high level recurrence non random connectivity pyramidal cell layer douglas thomson song furthermore layer pyramidal network display rich activity relatively long time scale vivo barth sakata harris luczak van kerkoerle slice egorov wang consistent lstm like function strong evidence persistent neuronal activity higher cortical area goldman rakic sensory area huang van kerkoerle kornblith relatively speaking sensory area visual cortex exhibit sorter timescales higher brain area prefrontal cortex would expect given different temporal requirement brain area similar behaviour expected multi area layer lstms note longer time scale present super cial layer layer goldman rakic van kerkoerle suggesting possibility memory cell per cortical microcircuit slow memory decay network may controlled short york van rossum costa long term synaptic plasticity abbott nelson senn ster gerstner zenke costa recurrent excitatory synapsis gate protect given memory lstms mapped onto lateral inhibitory input cortical circuit propose similar lstms input gate implemented inhibitory neuron layer layer lateral inhibition consistent canonical view microcircuit douglas bastos harris mrsic flogel sparse sensory evoked response layer sakata harris harris mrsic flogel brain inhibition believed originate parvalbumin basket cell providing near exact balanced inhibitory counter signal given excitatory feedforward input froemke xue froemke excitatory inhibitory input thus cancel arriving signal ignored default consequently activity within downstream memory network remains largely unperturbed unless altered targeted modulation inhibitory activity harris mrsic flogel vogels abbott letzkus similarly memory cell affect output lstm activity unaccompanied congruent inhibition mapped onto layer layer layer microcircuit known project higher brain area harris mrsic flogel see lateral inhibition turned gate open subtractive neural integration presynaptic cell re neurotransmitter released synaptic terminal neurotrans mitter subsequently bound postsynaptic receptor prompt structural change ion channel allow electrically charged ion postsynaptic cell depending receptor type ion either increase depolarise decrease hyperpolarise postsynaptic membrane potential suf ciently depolarising excitatory input provided postsynaptic potential reach threshold stereotyped action potential spike kandel behaviour formalised circuit resistance capacitance follows ohm law yield standard leaky integrate neuron model gerstner kistler riexc riinh membrane time constant iexc iinh excitatory inhibitory hyperpolarizing synaptic input current respectively action potential initiated standard model brette gerstner gerstner membrane potential hit hard threshold modelled momentary pulse subsequent reset resting potential neuronal excitation inhibition opposite effect inhibitory input act linearly subtractively membrane potential leaky integrate model approximated level ring rate rate cid iexc iinh iexc iinh cid see input output response gerstner kistler used demonstrate impact subtractive gating contrast multi plicative gating ring rate approximation form basis gated rnn model similar subtrac tive behaviour input output function bottom moreover rate formulation allows cleaner comparison lstm unit use existing machine learning optimisation method could argued different form inhibition shunting inhibition counteracts excitatory input decreasing membrane resistance characteristic multiplicative gating effect membrane potential however analysed level output ring rate effect becomes subtractive holt koch prescott koninck consistent biological arti cial gated recurrent neural network example unit simpli cortical recurrent neural network sensory downstream input arrives pyramidal cell layer layer fed onto memory cell recurrently connected pyramidal cell layer memory decay decay time constant input onto layer balanced inhibitory basket cell balance represented diagonal equal connection output memory cell gated basket cell layer within area upstream brain area implementation following similar notation lstm unit input output subtractive gate dashed connection represent potential balance excitatory inhibitory input weight set lstm recurrent neural network cell see main text detail plot bellow illustrate different gating mode simple current based noisy leaky integrate neuron capped subtractive inhibition sigmoidal activation function subtractive gating sigmoidal activation function multiplicative gating output rate represents number spike per second biological circuit approach model framed ring rate level rather level membrane potential subtractive gated long short term memory lstm unit hochreiter schmidhuber greff access memory cell controlled input gate see time forget gate control decay memory output gate control whether content memory cell transmitted rest network lstm network consists many lstm unit containing memory cell input forget output gate lstm state described unit follows dynamic given middle column note leak controlled input recurrent unit may biologically unrealistic input output cellctxt itotztxt ftxt input output cellctxt itot ztfxt inputunit junit junit joutputl inpc inl pcsmemorycellfpclayer abcsublstmlstmcortical circuit output rate input baselineweak inh strong inh inh exc inh exc closed gatebaselineweak inh strong inh baselinestrong gatesubtractive gatingmultiplicative gatingweak gate lstm sublstm rht rht tanh rht rht tanh memory cell note multiplicative control input gate denotes element wise multiplication new weighted input given input vector recurrent input lstm unit respectively overall output lstm unit computed lstm network multiple layer million parameter weight bias typically trained stochastic gradient descent supervised setting parameter multiple gate allow network adapt information depending task hand particular enable writing memory cell controlled input gate adjusting timescale memory controlled forget gate exposing memory network controlled output gate combined effect gate make possible lstm unit capture temporal contextual dependency across multiple timescales introduce study new rnn unit sublstm sublstm unit mapping lstms onto known canonical excitatory inhibitory cortical microcircuit douglas song harris mrsic flogel similarly sublstms ned however gating subtractive rather multiplicative sublstm ned memory cell transformed input input gate model use simpli notion balance gating jth unit memory forgetting consider option controlled gate lstm unit rht biologically plausible learned simple decay referred result sublstm similarly input sublstm output gated subtractive output gate see equation evaluated different activation function sigmoidal transformation highest performance key difference gated rnns subtractive inhibitory gating potential balanced excitatory input respectively see detailed comparison different gating mode subtractive versus multiplicative gating rnns key difference sublstms lstms lie implementation gating mech anism lstms typically use multiplicative factor control amplitude input signal sublstms use biologically plausible interaction excitation inhibition important consequence subtractive gating potential improved gradient backwards towards input layer illustrate compare gradient sublstms lstms simple example first review derivative loss respect various component sublstm notation based greff notation represents derivative loss note consider version sublstms forget gate lstms sublstm another simple memory decay scalar ne memory timeconstant sublstm weight could optimised model decided keep number parameter minimum simplicity ease comparison lstms respect def dloss dht error layer chain rule comparison corresponding derivative lstm unit given tanh tanh tanh sigmoid activation function overlined variable etc pre activation value gate input transformation woxt roht output gate sublstm note compared lstm sublstms provide simpler gradient fewer multiplicative factor lstms weight input transformation updated according total number temporal step ellipsis abbreviates recurrent gradient path time containing path backwards time via simplicity analysis ignore recurrent connection lstm sublstm consider depth wise path network call tth timestep depth contribution derivative lstm slight abuse notation tanh input gate cid output gate tanh cid tanh derivative tanh notice either input output gate set zero closed corresponding contribution gradient zero network subtractive gating depth derivative contribution becomes cid cid sigmoid derivative case input output gate present subtractive gate sublstms directly impair error propagation result aim work fold first inspired cortical circuit aimed propose biological plausible implementation lstm unit would allow better understand cortical architecture dynamic compare performance sublstm unit lstms rst compared learning dynamic subtractive multiplicative network mathematically second step empirically compared sublstm sublstm lstm network task sequential mnist classi cation word level language modelling penn treebank marcus wikitext merity network weight initialised glorot initialisation glorot bengio lstm unit initial forget gate bias selected number unit sublstm number parameter held constant across experiment facilitate fair comparison lstms sublstms sequential mnist sequential mnist digit classi cation task digit mnist dataset presented rnn sequence pixel decompose mnist image pixel sequence step network optimised rmsprop momentum tieleman hinton learning rate hidden layer hidden unit result sublstms achieves similar result lstms result comparable previous result task rnns comparison lstm sublstm network sequential pixel pixel mnist hidden unit sample mnist dataset converted matrix pixel temporal sequence timesteps classi cation accuracy test set sublstm xed learned forget gate language modelling language modelling represents challenging task rnns short long term dependency rnn language model rnn lm model probability text autoregressively predicting sequence word timestep trained predict following word word model word sequence product conditional multinoulli distribution evaluate rnn lm measuring perplexity ned sequence word perplexity rst used penn treebank ptb dataset train model word level language modelling training validation test word vocabulary word rnns tested hidden layer backpropagation truncated step batch size optimise network used rmsprop momentum performed hyperparameter search validation set input output update dropout rate learning rate weight decay hyperparameter search done google vizier performs black box optimisation gaussian process bandit transfer learning table resulting hyperparameters table report perplexity test set golovin understand sublstms scale network size varied number hidden unit tested wikitext language modelling dataset based wikipedia article dataset twice ptb dataset training validation test word lstmsublstmfix sublstm testing accuracy seq feature larger vocabulary word therefore well suited evaluate model performance longer term dependency reduces likelihood tting datasets result sublstms achieve perplexity similar lstms table interestingly biological plausible version sublstm simple decay forget gate achieves performance similar better sublstms penn treebank ptb test perplexity wikitext test perplexity size sublstm sublstm lstm size sublstm sublstm lstm table language modelling word level test set perplexity penn treebank wikitext model layer sublstm us xed learned forget gate unit number unit sublstm chosen number parameter sub lstm facilitate fair comparison size indicates number unit number hidden unit sublstm selected number parameter lstm sublstm facilitating fair comparison conclusion future work cortical microcircuit exhibit complex stereotypical network architecture support rich dynamic computational power dynamic yet properly understood known excitatory inhibitory neuron type interact closely process sensory information great accuracy making sense interaction beyond scope contemporary experimental approach lstms hand well understood powerful tool contextual task structure map intriguingly well onto stereotyped connectivity cortical circuit analysed biologically constrained lstms sublstms could perform similarly well indeed model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table penn treebank hyperparameters model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table wikitext hyperparameters subtractively gated excitation inhibition recurrent neural network promise compared lstms benchmark sequence classi cation word level language modelling notable sublstms could outperform traditional counterpart yet hope work serve platform discus develop idea cortical function establish link relevant experimental work role excitatory inhibitory neuron contextual learning froemke froemke poort pakan kuchibhotla future work interesting study additional biological detail may affect performance next step aim include dale principle given neuron make either excitatory inhibitory connection stratum harvey naturally focus perplexing diversity inhibitory cell type markram behaviour shunting inhibition mixed subtractive divisive control doiron mejias boustani sur seybold overall given success multiplicative gated lstms insightful understand biological trick cortical network may give lstms performance boost acknowledgement would like thank everton agnes glar ehre gabor melis jake stroud helpful comment discussion supported sir henry dale fellowship wellcome trust royal society supported epsrc research council rcuk supported clarendon fund reference abbott nelson synaptic plasticity taming beast nature neuroscience assael shillingford whiteson freitas lipnet sentence level lipreading arxiv preprint arxiv barth curto luczak marguet harris population coding tone stimulus auditory cortex dynamic rate vector analysis european journal neuroscience bastos usrey adam mangun fry friston canonical microcircuit predictive coding neuron bhalla dendrite deep learning sequence hippocampus hippocampus although focus comparison lstms similar point would apply gated rnns gated recurrent unit chung brette gerstner adaptive exponential integrate model effective description neuronal activity journal neurophysiology brunel dynamic sparsely connected network excitatory inhibitory spiking neuron journal computational neuroscience chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling arxiv org costa froemke sjostrom van rossum uni pre postsynaptic long term plasticity enables reliable exible learning elife costa mizusaki sjostrom van rossum functional consequence pre postsynaptic expression synaptic plasticity philosophical transaction royal society london series biological science costa padamsey amour emptage froemke vogels synaptic transmission optimization predicts expression locus long term plasticity neuron costa sjostrom van rossum probabilistic inference short term synaptic plasticity neocortical microcircuit frontier computational neuroscience cox dean neural network neuroscience inspired computer vision current biology deneve machens cient code balanced network nature neuroscience doiron longtin berman maler subtractive divisive inhibition effect voltage dependent inhibitory conductance noise neural computation douglas koch mahowald martin suarez recurrent excitation neocortical circuit science douglas martin whitteridge canonical microcircuit neocortex neural computation egorov hamam frans hasselmo alonso graded persistent activity entorhinal cortex neuron nature boustani sur response dependent dynamic cell speci inhibition cortical network vivo nature communication froemke plasticity cortical excitatory inhibitory balance annual review neuroscience froemke merzenich schreiner synaptic memory trace cortical receptive eld plasticity nature gerstner kistler spiking neuron model single neuron population plasticity cambridge university press gerstner kistler naud paninski neuronal dynamic single neuron network model cognition cambridge university press glorot bengio understanding dif culty training deep feedforward neural network proceeding thirteenth international conference arti cial intelligence statistic page goldman rakic cellular basis working memory neuron golovin solnik moitra kochanski karro sculley google vizier service black box optimization proceeding acm sigkdd international conference knowledge discovery data mining page acm graf generating sequence recurrent neural network arxiv org graf mohamed hinton speech recognition deep recurrent neural network arxiv preprint arxiv greff srivastava koutn steunebrink schmidhuber lstm search space odyssey arxiv org harris mrsic flogel cortical connectivity sensory coding nature hassabis kumaran summer eld botvinick neuroscience inspired arti cial intelligence neuron hennequin agnes vogels inhibitory plasticity balance control codependence annual review neuroscience hennequin vogels gerstner optimal control transient dynamic balanced network support generation complex movement neuron hochreiter bengio frasconi schmidhuber gradient recurrent net dif culty learning long term dependency hochreiter schmidhuber long short term memory neural computation holt koch shunting inhibition divisive effect ring rate neural computation huang matysiak heil nig brosch king persistent neural activity auditory cortex related auditory working memory human nonhuman primate elife jiang shen cadwell berens sinz ecker patel tolias principle connectivity among morphologically ned cell type adult neocortex science aac aac kandel schwartz jessell siegelbaum principle neural science kornblith quiroga koch fried mormann persistent single neuron activity working memory human medial temporal lobe current biology kremkow aertsen kumar gating signal propagation spiking neural network balanced correlated excitation inhibition journal neuroscience krueger dayan flexible shaping learning small step help cognition kuchibhotla gill lindsay papadoyannis field sten miller froemke parallel processing cortical inhibition enables context dependent behavior nature neuroscience jaitly hinton simple way initialize recurrent network rectus linear unit arxiv org letzkus wolff thi disinhibition circuit mechanism associative learning memory neuron luczak mcnaughton harris packet based communication cortex nature review neuroscience marblestone wayne kording toward integration deep learning neuroscience frontier computational neuroscience marcus marcinkiewicz santorini building annotated corpus english penn treebank computational linguistics markram toledo rodriguez wang gupta silberberg interneurons neocortical inhibitory system nature review neuroscience mejias kappen longtin torres short term synaptic plasticity heterogeneity neural system merity xiong bradbury socher pointer sentinel mixture model arxiv org reilly frank making working memory work computational model learning prefrontal cortex basal ganglion neural computation pakan lowe dylda keemink currie coutts rochefort mrsic flogel behavioral state modulation inhibition context dependent cell type speci mouse visual cortex elife pascanu mikolov bengio dif culty training recurrent neural network arxiv org ster gerstner triplet spike model spike timing dependent plasticity journal neuroscience poort khan pachitariu nemri orsolic krupic bauza sahani keller mrsic flogel hofer learning enhances sensory multiple non sensory representation primary visual cortex neuron prescott koninck gain control ring rate shunting inhibition role synaptic noise dendritic saturation proc natl acad sci usa sakata harris laminar structure spontaneous sensory evoked population activity auditory cortex neuron senn markram tsodyks algorithm modifying neurotransmitter release probability based pre postsynaptic spike timing neural computation seybold phillips schreiner hasenstaub inhibitory action uni network integration neuron song str reigl nelson chklovskii highly nonrandom feature synaptic connectivity local cortical circuit plo biology stratum harvey dale principle brain research bulletin sutskever vinyals sequence sequence learning neural network arxiv org thomson west wang bannister synaptic connection small circuit involving excitatory inhibitory neuron layer adult rat cat neocortex triple intracellular recording biocytin labelling vitro cerebral cortex new york tieleman hinton lecture rmsprop divide gradient running average recent magnitude coursera neural network machine learning van den oord kalchbrenner vinyals espeholt graf kavukcuoglu conditional generation pixelcnn decoder arxiv org van kerkoerle self roelfsema layer speci city effect attention working memory activity primary visual cortex nature communication van vreeswijk sompolinsky chaos neuronal network balanced excitatory inhibitory activity science vogels abbott gating multiple signal detailed balance excitation inhibition spiking network nature neuroscience wang markram goodman berger goldman rakic heterogeneity pyramidal network medial prefrontal cortex nature publishing group xue atallah scanziani equalizing excitation inhibition ratio across visual cortical neuron nature york van rossum recurrent network short term synaptic depression journal computational neuroscience zenke agnes gerstner diverse synaptic plasticity mechanism orchestrated form retrieve memory spiking neural network nature communication)\n",
            "(53, 55): (robust portfolio optimization huitong qiu department biostatistics john hopkins university baltimore hqiu jhu edu fang han department biostatistics john hopkins university baltimore fhan jhu edu han liu brian caffo department biostatistics john hopkins university baltimore bcaffo jhsph edu department operation research financial engineering princeton university princeton hanliu princeton edu abstract propose robust portfolio optimization approach based quantile statistic proposed method robust extreme event asset return accommo date portfolio limited historical data speci cally risk estimated portfolio converges oracle optimal risk parametric rate weakly dependent asset return theory rely higher der moment assumption thus allowing heavy tailed asset return moreover rate convergence quanti size portfolio management allowed scale exponentially size historical data empirical effectiveness proposed method demonstrated syn thetic real stock data work extends existing one achieving robustness high dimension allowing serial dependence introduction markowitz mean variance analysis set basis modern portfolio optimization theory however mean variance analysis criticized sensitive estimation error mean covariance matrix asset return compared covariance matrix mean asset return uential harder estimate therefore many study focus global minimum variance gmv formulation involves estimating covariance matrix asset return estimating covariance matrix asset return challenging due high dimensionality heavy tailedness asset return data speci cally number asset management usually much larger size exploitable historical data hand extreme event typical nancial asset price leading heavy tailed asset return overcome curse dimensionality structured covariance matrix estimator proposed asset return data considered estimator based factor model observable factor studied covariance matrix estimator based latent factor model proposed shrink covariance matrix towards highly structured covariance matrix including identity matrix order autoregressive covariance matrix factor based covariance matrix estimator estimator commonly based covariance matrix sub gaussian tail assumption required guarantee consistency heavy tailed data robust estimator covariance matrix desired classic robust covariance matrix estimator include estimator minimum volume ellipsoid mve minimum covari ance determinant mcd estimator estimator estimator based data outlyingness depth estimator speci cally designed data low dimension size generalizing robust estimator high dimension proposed orthogo nalized gnanadesikan kettenring ogk estimator extends estimator estimating eigenvalue studied shrinkage estimator based tyler estimator however though ogk computationally tractable high dimension consistency guaranteed xed dimension shrunken tylor estimator involves iteratively inverting matrix moreover consistency guaranteed dimension order sam ple size aforementioned robust estimator analyzed independent data point performance time series data questionable paper build quantile based scatter matrix estimator propose robust portfolio optimization approach contribution aspect first proposed method accommodates high dimensional data allowing dimension scale exponentially size secondly verify consistency proposed method achieved without tail condition thus allowing heavy tailed asset return data thirdly consider weakly dependent time series demonstrate degree dependence affect consistency proposed method background section introduce notation system provide review gross exposure con strained portfolio optimization exploited paper notation let dimensional real vector mjk matrix mjk entry cid vector norm let matrix cid max norm cid cid max maxjk mjk frobenius norm cid cid let random vector write identically distributed use denote vector every entry gross exposure constrained gmv formulation gmv formulation found imposing short sale constraint improves portfolio ciency relaxed short sale constraint gross exposure constraint showed portfolio ciency improved let random vector asset return portfolio characterized vector investment allocation among asset gross exposure constrained gmv portfolio optimization formulated cid vector norm cid cid maxd cid cid cid cid cid cid cid covariance matrix budget constraint cid cid gross exposure constraint called gross exposure constant control percentage long short position allowed portfolio optimization problem converted quadratic programming problem solved standard software method section introduce quantile based portfolio optimization approach let random variable distribution function sequence observation constant quantiles min cid cid inf min cid scatter matrix ned matrix proportional covariance matrix constant order statistic say unique unique exists unique following estimator population quantile based scale exists unique say cid cid independent copy based cid robust scat cid cid cid detail let random vector ter matrix asset return representing return asset sequence observation xtd population quantile based scatter matrix qne cid cid entry cid given cid cid xtj cid cid cid cid xtj xtk cid cid since cid computed log time computational complexity cid log since cid practice cid computed almost ciently xtj xtk covariance matrix complexity let vector investment allocation among asset matrix risk function wtmw covariance matrix var wtx variance portfolio return wtx employed objected function gmv formulation however estimating dif cult due heavy tail asset return paper adopt robust alternative moment based risk metric consider following oracle portfolio optimization problem cid cid cid cid gross exposure constraint introduced section unknown estimated convexity risk function project cid onto cone practice wopt argmin positive nite matrix cid argminr cid cid cid cid cid max minid cid cid maxid min max set lower upper bound eigenvalue cid optimization algorithm supplementary material cid formulate empirical robust portfolio problem solved projection contraction algorithm summarize optimization cid wopt argmin cid cid cid remark robust portfolio optimization approach involves parameter min max empirically setting min max prof work well typically provided investor controlling percentage short position data driven choice desired refer cross validation based approach remark rationale behind positive nite projection lie aspect first order portfolio optimization convex well conditioned positive nite matrix lower bounded eigenvalue needed guaranteed setting min secondly pro jection robust compared ogk estimate ogk induces positive niteness estimating eigenvalue variance principal component robustness lost data possibly containing outlier projected onto principal direction estimating principal component remark adopt quantile nitions cid achieve breakdown point however note methodology theory carry replaced absolute constant theoretical property section provide theoretical analysis proposed portfolio optimization approach optimized portfolio cid wopt based estimate next lemma show error risk cid wopt wopt essentially related estimation error lemma let cid wopt solution arbitrary matrix cid cid min cid wopt wopt cid cid max wopt solution oracle portfolio optimization problem gross exposure constant next derive rate convergence cid wopt relates rate convergence cid cid cid max end rst introduce dependence condition asset return series nition let stationary process denote led generated respectively mixing coef cient ned sup process mixing limn condition stationary process cid xtj xtj xtk xtj xtk mixing process satisfying constant parameter determines rate decay characterizes degree dependence next introduce identi ability condition distribution function asset return condition let cid cid cid independent copy cid let distribution function cid cid cid cid cid assume exist constant inf cid condition guarantee identi ability quantiles standard literature quantile statistic based condition present rate convergence theorem let absolutely continuous stationary process satisfying condition suppose log enough probability smaller cid cid rate convergence ned max cid log log cid cid cid max cid cid cid cid cid cid cid max log log max cid cid cid max max cid cid moreover ned implication theorem follows parameter max scale rate convergence reduces cid log thus number asset management allowed scale ically approach cid exponentially size compared similar rate convergence obtained covariance based estimator require moment tail condition thus accommodating heavy tailed asset return data effect serial dependence rate convergence characterized specif increase towards nity ating allowed scale log rate convergence inversely related lower bound marginal density function around quantiles small distribu tion function around quantiles making population quantiles harder estimate combining lemma theorem obtain rate convergence cid wopt theorem let absolutely continuous stationary process satisfying condition suppose log enough cid wopt wopt ned gross exposure constant theorem show risk estimated portfolio converges oracle optimal risk parametric rate number asset allowed scale exponentially size moreover rate convergence rely tail condition distribution asset return rest section build connection proposed robust portfolio opti mization moment based counterpart speci cally consistent elliptical model nition random vector follows elliptical distribution location scatter exist nonnegative random variable matrix rank random vector independent uniformly distributed dimensional sphere aat rank denote ecd called generating variate commonly used elliptical distribution include gaussian distribution distribution elliptical distribution widely used modeling nancial return data since naturally capture many stylized property including heavy tail tail dependence next theorem relates moment based counterpart elliptical model theorem let ecd absolutely continuous elliptical random vector cid cid cid independent copy constant depending distribution moreover mqs cov covariance matrix constant given cqr cid cid cid cid cid cid var var cid cid cid cid var cid last inequality hold var var theorem elliptical model minimizing robust risk metric equiv alent minimizing standard moment based risk metric thus robust portfolio optimization equivalent moment based counterpart population level plug ging lead following theorem theorem let absolutely continuous stationary process satisfying condition suppose ecd follows elliptical distribution covariance matrix log cid wopt wopt gross exposure constant ned ned thus elliptical model optimal portfolio cid wopt obtained robust portfolio optimization lead parametric rate convergence standard moment based risk experiment section investigate empirical performance proposed portfolio optimization approach section demonstrate robustness proposed approach synthetic heavy tailed data section simulate portfolio management standard poor stock index data proposed portfolio optimization approach qne compared competitor competitor constructed replacing covariance matrix commonly used variance scatter matrix estimator ogk orthogonalized gnanadesikan kettenring estimator construct pilot scatter matrix estimate robust estimator scale estimate eigenvalue variance principal component factor principal factor estimator iteratively solves speci variance factor loading shrink shrinkage estimator shrinkage covariance matrix towards factor covariance estimator synthetic data following construct covariance matrix asset return factor model return stock bjk loading stock factor idiosyncratic noise independent factor model covariance matrix stock return given diag bjk matrix consisting factor loading covariance matrix factor variance noise adopt covariance simulation following generate factor loading trivariate normal distribution mean covariance speci table factor loading generated xed parameter throughout simulation covariance matrix factor given table standard deviation idiosyncratic noise generated independently truncated gamma distribution shape scale restricting support standard deviation xed parameter generated according parameter obtained tting factor model year daily return data industry portfolio may aug covariance matrix xed throughout simulation since interested risk optimization set mean asset return dimension stock consideration xed given covariance matrix generate asset return data following distribu tions multivariate gaussian distribution table parameter generating covariance matrix equation parameter factor loading parameter factor return gaussian multivariate elliptical log normal gaussian multivariate elliptical log normal portfolio risk selected number stock matching rate oracle optimal port folio multivariate distribution degree freedom covariance matrix elliptical distribution log normal generating variate log covariance trix distribution generate asset return series half year estimate covariance scatter matrix qne competitor plug optimize portfolio allocation solve true covariance matrix obtain oracle optimal portfolio benchmark range gross exposure constraint result based simulation cid denotes cardinality set show portfolio risk cid matching rate optimized portfolio cid oracle optimal portfolio matching rate ned follows portfolio let corresponding set selected asset asset weight non zero matching rate ned note observation estimator lead comparable portfolio risk gaussian model however heavy tailed distribution qne achieves lower portfolio risk matching rate qne stable across model higher competing method heavy tailed distribution thus conclude qne robust heavy tail risk minimization asset selection real data section simulate portfolio management stock collect adjusted daily closing price stock stayed index january due cid regularization gross exposure constraint solution generally sparse adjusted closing price account corporate action including stock split dividend right offering gross exposure constant riskoracleqneogkfactorshrink gross exposure constant riskoracleqneogkfactorshrink gross exposure constant riskoracleqneogkfactorshrink gross exposure constant matching rateqneogkfactorshrink gross exposure constant matching rateqneogkfactorshrink gross exposure constant matching rateqneogkfactorshrink table annualized sharpe ratio return risk competing approach index data sharpe ratio return risk qne ogk factor shrink december closing price obtain daily return daily growth rate price manage portfolio consisting stock january december day optimize portfolio allocation past month stock return data point hold portfolio day evaluate portfolio return day way obtain portfolio return repeat process method comparison range gross exposure constant since true covariance matrix stock return unknown adopt sharpe ratio evaluating performance portfolio table summarizes annualized sharpe ratio mean return empirical risk standard deviation portfolio return observe qne achieves largest sharpe ratio value gross exposure constant indicating lowest risk return equivalently highest return risk discussion paper propose robust portfolio optimization framework building quantile based scatter matrix obtain non asymptotic rate convergence scatter matrix estimator risk estimated portfolio relation proposed framework moment based counterpart well understood main contribution robust portfolio optimization approach lie robustness heavy tail high dimension heavy tail present unique challenge high dimension compared low dimension example asymptotic theory estimator guarantee consistency rate increasing however cid statistical error may scale rapidly dimension thus stringent tail condition subgaussian condition required guarantee consistency moment based estimator high dimension paper based quantile statistic achieve consistency portfolio risk without assuming tail condition allowing scale nearly exponentially another contribution work lie theoretical analysis serial dependence may affect consistency estimation measure degree serial dependence mixing coef cient effect serial dependence rate convergence cid even non gaussian data cid statistical error diminishes rapidly summarized parameter characterizes size cid drop data avoid nancial crisis stock price likely violate stationary assumption imposes upper bound percentage short position practice percentage short position usually strictly controlled much lower reference harry markowitz portfolio selection journal finance michael best robert grauer sensitivity mean variance cient portfolio change asset mean analytical computational result review financial study vijay kumar chopra william ziemba effect error mean variance covariance optimal portfolio choice journal portfolio management robert merton estimating expected return market exploratory investigation journal jarl kallberg william ziemba mi speci cation portfolio selection problem risk financial economics capital page springer jianqing fan yingying fan jinchi high dimensional covariance matrix estimation factor model journal econometrics james stock mark watson forecasting principal component number predictor journal american statistical association jushan bai kunpeng statistical analysis factor model high dimension annals statistic jianqing fan yuan liao martina mincheva covariance estimation thresholding principal orthogonal complement journal royal statistical society series statistical methodology olivier ledoit michael wolf improved estimation covariance matrix stock return application portfolio selection journal empirical finance olivier ledoit michael wolf well conditioned estimator dimensional covariance matri ce journal multivariate analysis olivier ledoit michael wolf honey shrunk covariance matrix journal portfolio management peter huber robust statistic wiley ricardo maronna ruben zamar robust estimate location dispersion high dimensional datasets technometrics ramanathan gnanadesikan john kettenring robust estimate residual outlier detection multiresponse data biometrics yilun chen ami wiesel alfred hero robust shrinkage estimation high dimensional covariance matrix ieee transaction signal processing romain couillet matthew mckay dimensional analysis optimization robust shrinkage covariance matrix estimator journal multivariate analysis ravi jagannathan risk reduction portfolio imposing wrong constraint help journal finance jianqing fan jingjin zhang vast portfolio selection gross exposure constraint journal american statistical association peter rousseeuw christophe croux alternative median absolute deviation journal american statistical association shao solving matrix nearness problem maximum norm applying projection contraction method advance operation research alexandre belloni victor chernozhukov cid penalized quantile regression high dimensional sparse model annals statistic lan wang yichao runze quantile regression analyzing heterogeneity ultra high dimension journal american statistical association peter bickel elizaveta levina covariance regularization thresholding annals statistic man hall tony cai cun hui zhang harrison zhou optimal rate convergence covariance matrix estimation annals statistic kai tai fang samuel kotz kai wang symmetric multivariate related distribution chap harry joe multivariate model dependence concept chapman hall rafael schmidt tail dependence elliptically contoured distribution mathematical method erations research svetlozar todorov rachev handbook heavy tailed distribution finance elsevier svetlozar rachev christian menn frank fabozzi fat tailed skewed asset return distribu tions implication risk management portfolio selection option pricing wiley kevin dowd measuring market risk wiley torben gustav andersen handbook financial time series springer jushan bai shuzhong shi estimating high dimensional covariance matrix application annals economics finance sara van geer van geer empirical process estimation cambridge university alastair hall generalized method moment oxford university press oxford peter uhlmann sara van geer statistic high dimensional data method theory press cambridge application springer, mean field approach probabilistic model information retrieval bin michael wong department physic hong kong university science technology clear water bay hong kong phwbd ust phkywong ust david bodoff department ismt hong kong university science technology clear water bay hong kong dbodoff ust abstract study explicit parametric model document query rel evancy assessment information retrieval mean eld method applied analyze model derive cient practical algorithm estimate parameter problem hyperparameters timated fast approximate leave cross validation procedure based cavity method algorithm evaluated several benchmark database comparing standard algorithm introduction area information retrieval study representation organization access information information repository advent boom internet espe cially world wide web www information available shared online search internet becomes increasingly popular respect probabilistic model become useful empowering information search fact information search contain rich information recorded fruitfully used improve performance subsequent retrieval exten sion process relevance feedback incorporates relevance assessment supplied user construct new representation query procedure user interactive document retrieval process feedback information help query continuously effect pertain particular retrieval session hand objective representation document query help relevancy data subsequent retrieval session bene ted based fuhr buckley meta structure relating document query relevancy assessment recently proposed probabilistic model object described explicit parametric distribution function facilitating construction likelihood function whose maximum used characterize document query rather relying heuristic many previous work proposed model provides uni formal framework following task hoc information retrieval query given goal return list ranked document according similarity query document routing document given goal categorize list ranked query according sim ilarities document assume model category represented query paper report recent progress putting new theoretical approach empirical test since document query represented high dimensional vec tor vector space model mean eld approach adopted mean eld method commonly used study magnetic system statistical physic thanks ability deal high dimensional system increasingly applied many ea information processing recently present context mean eld treatment implies particular component document query vector analyzed component vector considered background eld satisfying appropriate average property correlation statistical uctuations background vector neglected introducing parametric model section mean eld approach used step first section true representation document query estimated maximizing total probability observation result set mean eld equation solved fast iterative algorithm respectively esti mated true document query used hoc information retrieval document routing secondly model depends hyperparameters conventionally deter mined cross validation method described section mean eld proach used accelerate otherwise tedious leave cross validation procedure given set hyperparameter value enables carry sys temwide iteration rather repeating left document query leave estimation document query representation obtained version mean eld theory called cavity method section compare model standard idf latent semantic indexing lsi benchmark test collection shall see validity model well supported superior performance paper concluded section uni probabilistic model dimensional vector vector denoted referred true meaning document query model consists following component work motivated fuhr buckley conceptual model assume set document query available vector space model document query represented document really observe distributed around true document vector according probability distribution difference resulting word document similarly query query vector according probability distribution distribution document containing term ideally represent meaning document generated true meaning user actually submits distributed around true relation document query called relevancy assessment pair document query denote relation binary variable say document relevant query document user want otherwise document irrelevant query suppose according distribution true representation document relevancy relation document query historical record expert etc hypothesize true document query distributed query satisfy relevancy relation summarize idea probabilistic meta structure shown data unknown parameter probabilistic meta structure data order complete model need hypothesize form distribution func tions paper restrict document query hypersphere since usually cosine angle document query used determine similarity document query hence assume following distribution function distribution observed document given true location distribution observed query given true location prior distribution document query given relevance relation tween dirac function respectively hence independent normalization constant assume observation document query independent obtain total probability observing document query given relevancy relation energy function appealing correspondence denotes hyperparameters present model spin model statistical physic observed familiar partition function maximizing probability obtain estimation true docu ments used hoc retrieval similarity function vector cosine angle rank similarity instead new query determine whether document retrieved byproduct obtain estimation true query turn used document routing new document compared termine whether belongs category model give unifying procedure hoc retrieval routing parameter estimation section derive fast iterative algorithm parameter estimation first replace function fourier transform mean eld theory work limit well approximated taking saddle point derivative respect formula changed integration imaginary axis integration obtained equating partial writing zero yielding written set equation referred mean eld equation since uctuations around mean value parameter neglected due simple form solved iterative scheme though studied theoretical convergence iterative scheme effectiveness seen following argument replace saddle point iteration process becomes linear eq differ linear iteration problem scale factor respectively hence eq problem equivalent rescaling length iterated respective value vector back hypersphere ned operation linear iteration rescaling back hypersphere make stable algorithm complexity algorithm linear number document query empirically converges ten step alternatively may use whose convergence alternate guaranteed computationally complex augmented lagrangian method saddle point hyperparameter estimation model parameter uence parameter estimation described section refer hyperparameters chosen model performs optimally new query raised retrieve document new document routed determine shape distribution example hyperparameters chosen one give optimal performance standard method hyperparameter estimation machine learning leave example training model cross validation suppose time pick data validation set train model rest averaged rameters need train model exact leave cross validation tedious especially multiple hyperpa time combination hyper parameter model propose approximate leave procedure based cavity method suppose trained model data obtain test example satis steady state equation left training set query cavity estimation estimation query satisfy equation subtracting assuming approximately get difference hoc retrieval eliminate lution simpli mean eld argument change induced decoupled hence neglect removing query obtain set linear equation diagonal term yielding document estimated used predict leave hoc retrieval performance model equation document routing derived analogously known systemwide training similarity note note need train model leave estimation docu ments query obtained step algorithm extremely fast amaz ingly give reasonable estimation hyperparameters shown following experiment remark mean eld technique applied distribution document query relevance feedback described eq present case specti eq model similar gaussian model spherical replaced spherical gaussian prior though leave cross validation done exactly gaussian model involves inversion matrix hand mean eld estimation greatly simpli process neglecting diagonal element constraint experimental result applied proposed method hoc retrieval routing test collection cran eld cisi treat task identically use evaluation criterion recall precision curve average retrieval precision run version algorithm original dimension observed document query represented original idf weight reduced dimension original vector reduced singular value decomposition svd lsi fig recall precision curve optimal hyperparameters mean eld estimate compared baseline result lsi clear method give signi cant gain retrieval precision comparison original mension cran eld collection shown due space limitation yield equally satisfactory result lsi lsi recall recall recall precision curve mean eld estimation baseline lsi hoc retrieval document routing cisi reduced dimension hyperparameter estimation compare mean eld result exact leave cross validation reduced dimension since computation exact one still feasible plotted average precision versus hyper parameter computed method similar contour although uniform displacement value demonstrates usefulness mean eld approximation hyperparameter estimation table obtain value optimal hyperparameters mean eld leave method average precision exact leave computed optimal hyperparameters compared result exact leave listed table hyperparameter estimation original dimension exact leave available since tedious instead compare hyperparameters one fold cross validation whether compare mean eld exact leave fold cross validation optimal hperparameters comparable case discrepancy observe average precision essentially average retrieval precision versus hyperparameters hoc retrieval reduced dimension cisi mean eld leave peaked exact leave peaked table average retrieval precision leave cross validation reduced mension mean eld versus exact cisi cran eld lsi mean field exact lsi mean field exact average precision average precision hoc retrieval document routing conclusion considered probabilistic model document query relevancy ass ments fast algorithm derived parameter hyperparameter estimation signif icant improvement achieved hoc retrieval routing compared idf lsi another paper compared model heuristic meth od rocchio heuristic bartell multidimensional scaling mean eld method still outperforms success illustrate potential mean eld approach especially suitable system high dimension numerous mutually interacting component hence anticipate mean eld method increasing application many probabilistic model acknowledgment thank jin interesting discussion work supported grant hkust research grant council hong kong reference cohn hofmann missing link probabilistic model doc ument content hypertext connectivity advance neural information process ing system leen dietterich tresp ed mit press cambridge jaakola siegelmann active information retrieval advance neural information processing system dietterich becker ghahra mani ed mit press cambridge rocchio relevance feedback information retrieval smart retrieval system experiment automatic document processing salton prentice hall englewood cliff chapter fuhr buckley probabilistic learning approach document indexing acm transaction information system bodoff enabe kanbil simon yukhimets uni maximumn likelihood approach document retrieval journal american society information science technology opper saad ed advanced mean field method mit press cambridge wong fast parameter estimation green function advance neural information processing system dietterich becker ghahramani ed mit press cambridge salton mcgill introduction modern information retrieval mcgraw hill new york deerwester dumais furnas landauer harshman indexing latent semantic analysis journal american society informa tion science nocedal wright numerical optimization springer berlin bishop neural network pattern recognition clarendon press oxford bodoff wong relevance feedback meet maximum likelihood preprint bartell cottrell belew latent semantic indexing optimal special case multidimensional scaling proceeding ternational acm sigir conference research development information retrieval)\n",
            "(86, 88): (warped gaussian process edward snelson cid carl edward rasmusseny zoubin ghahramani cid cid gatsby computational neuroscience unit university college london queen square london fsnelson zoubing gatsby ucl ymax planck institute biological cybernetics spemann stra ubingen germany carl tuebingen mpg abstract generalise gaussian process framework regression learning nonlinear transformation output allows non gaussian process non gaussian noise learning algo rithm chooses nonlinear transformation transformed data well modelled seen including preprocessing transformation integral part probabilistic modelling problem rather hoc step demonstrate several real regression problem learning transformation lead signi cantly better performance regular xed transformation introduction gaussian process extremely concise simple way placing prior function done gps used basis nonlinear nonparametric regression classi cation showing excellent performance wide variety datasets importantly allow full bayesian predictive distribution obtained rather merely point prediction however simplest form gps limited nature simplicity assume target data distributed multivariate gaussian gaussian noise individual point simplicity enables prediction made easily matrix manipulation course predictive distribution gaussian often unreasonable assume form data obtained noise gaussian data well modelled example observation may posi tive quantity varying many order magnitude make little sense model quantity directly assuming homoscedastic gaussian noise situation standard practice statistic literature take log data modelling pro ceeds assuming transformed data gaussian noise better modelled log particular transformation could done con tinuum transformation could applied observation space bring data form well modelled making transformation really full part probabilistic modelling seems strange rst make hoc transformation use principled bayesian probabilistic model paper transformation warping observation space made entirely automatically fully encompassed probabilistic framework warped make transformation latent space observation data best modelled latent space viewed generalisation since observation space non gaussian process non gaussian asymmetric noise general however non gaussian noise model see section discussion excellent review gaussian process regression classi cation see follow notation throughout paper present brief summary regression section section toy real data warped signi cantly improve predictive performance variety measure especially regard whole predictive distribution rather single point prediction mean median transformation found give insight property data nonlinear regression gaussian process suppose given dataset consisting pair input vector cid real valued target cid ftngn wish predict value observation given new input vector rather distribution assume underlying function trying model observation lie noisily around place prior directly space function assuming nite selection point give rise multivariate gaussian dis tribution corresponding function value covariance function value point modelled covariance function usually assumed simple parametric form noise model taken gaussian distribution observation gaussian entry covariance matrix given cmn cid cid mng cid cid parameterises covariance function noise model cid kro necker delta function often noise model taken input independent covariance function taken gaussian function difference input vector stationary covariance function although many possibility exist see gps input dependent noise paper consider popular choice case entry covariance matrix given cmn exp cid cid cid width parameter expressing scale typical function vary dth dimension size parameter expressing typical size overall process space noise variance observation cid rdg simple predictive distribution new point given observed data jtn gaussian calculation mean variance distribution involves matrix inversion covariance matrix training input standard exact method incurs computational cost order learning training usually achieved nding local maximum likelihood conjugate gradient method respect hyperparameters cid covariance matrix negative log likelihood given cid log jxn cid log det cid log cid evaluation gradient respect cid involve computing inverse covariance matrix incurring order cost rather nding estimate cid prior cid included map estimate cid map even better cid numerically integrated computing example hybrid monte carlo method warping observation space section present method warping observation space nonlinear monotonic function latent space whilst retaining full probabilistic framework enable learning prediction take place consistently let consider vector latent target suppose vector modelled cid log jxn cid log det cid log cid make transformation true observation space latent space mapping observation monotonic function cid cid parameterises transformation require monotonic mapping whole real line otherwise probability measure conserved transformation induce valid distribution target including jacobian term take transformation account negative log likelihood cid log jxn cid cid becomes log det cid cid training warped log cid cid cid cid log cid learning extended model achieved simply taking derivative negative log likelihood function respect cid cid parameter vector conjugate gradient method compute parameter value way form covariance matrix nonlinear transformation learnt simultaneously probabilistic framework since computational limiter inverting covariance matrix adding extra parameter likelihood really costing anything require derivative easy compute respect cid introduce many extra parameter problem tting course prior cid cid may included compute map estimate fact parameter integrated hybrid monte carlo method prediction warped particular setting covariance function hyperparameters cid example cid cid map latent variable space predictive distribution new point regular gaussian whose mean variance calculated mentioned section cid cid cid cid cid cid distribution observation space pas gaussian nonlinear warping function giving cid cid exp cid cid cid cid cid cid cid shape distribution depends form warping function general may asymmetric multimodal require point prediction made rather whole distribution value predict depends loss function loss function absolute error median distribution predicted whereas loss function squared error mean distribution standard predictive distribution gaussian median mean lie point warped general different point median particularly easy calculate tmed cid notice need compute inverse warping function general unlikely analytical form cid parameterised function opposite direction however since access derivative iteration newton raphson good enough starting point enough often useful give indication shape range distribution giving position various percentile example may want know position cid either side median say approximately density lie bound point observation space calculated exactly way median simply pas value inverse function tmed cid cid cid cid cid calculate mean need integrate density rewriting integral back latent space get dzf cid cid cid simple dimensional integral gaussian density gauss hermite quadrature may used accurately compute weighted sum small number evaluation inverse function cid appropriate place choosing monotonic warping function wish design warping function allow complex transformation must constrain function monotonic various way obvious neural net style sum tanh function cid tanh cid cid produce series smooth step controlling size step controlling steepness position course number regression task dotted line true generating distribution dashed line prediction solid line warped prediction triplet line represent median cid percentile case predictive probability density cid cid cross section solid grey line step need set depend complex function want derivative function respect either warping parameter cid easy compute spirit sum error function sum logistic function would produce similar series step could used instead problem stand bounded inverse function cid exist value outside range bound explained earlier lead proper density space density space gaussian cover whole real line instead cid tanh cid linear trend away tanh step restricted making warping function cid size covariance function free vary effective gradient made arbitrarily small simply making range data latent space arbitrarily big exible system linear trend may made including addition neural net style function function form cid function effectively splice straight line gradient smoothly together curvature parameter cid position sign cid determines whether join convex concave cid log cid cid cid cid cid cid simple regression task simple regression task created situation warped perform signi cantly better standard point regularly spaced cid cid cid axis generated gaussian noise sine function point warped function arrive dataset shown dot sine creep abalone aileron warping function learnt regression task carried paper plot made range observation data tmin tmax warped trained independently dataset conjugate gradi ent minimisation procedure randomly initialised parameter obtain maximum like lihood parameter warped warping function used tanh function model covariance matrix used hybrid monte carlo implemented integrate parameter warping parameter much faster since matrix inversion required step dataset real datasets section signi cant difference found prediction warped made parameter point regularly spaced range prediction made median cid percentile case plotted triplet line prediction warped found much closer true generating distri bution standard especially regard cid line mean line computed found lie close slightly skewed median line emphasis point warped nd shape whole predictive distribution much better median mean plot particular point axis chosen cid cid predictive density warped plotted alongside true density written analytically note standard must necessarily predict symmetrical gaussian density even density point generated highly asymmetrical case show warping function learnt regression task tanh function adjusted mimic nonlinearity range obser vation space thus inverting transformation imposed generating data result real datasets surprising method work well toy dataset section since generated known nonlinear warping smooth function gaussian noise demonstrate nonlinear transformation help real data set run warped comparing prediction ordinary regression problem datasets summarised following table show range target tmin tmax number input dimension size training test set ntrain ntest used dataset creep abalone aileron cid cid cid cid cid cid mpa yr tmin mpa tmax ntrain ntest dataset creep abalone model log warped log warped absolute error aileron warped cid cid cid cid cid cid cid cid squared error cid log table result testing warped log transform real datasets unit absolute error squared error original data dataset creep material science set objective predict creep rup ture stress mpa steel given chemical composition input abalone aim predict age abalone various physical input aileron simulated control problem aim predict control action aileron aircraft datasets creep abalone consist positive observation standard practice may model log data datasets compared model directly data xed log transformed data warped directly data predictive point density always compared original data space accounting jacobian log warped transforms model run task parameter estimate covariance matrix warping function tanh function result obtain datasets shown table measure performance independent test set mean absolute error mean squared error mean negative log predictive density evaluated test point nal measure included give idea well model predicts entire density point prediction set warped always performs signi cantly better standard creep abalone xed log transform clearly work well partic ularly case creep warped learns better transformation show warping function learnt indeed clearly log like character hand aileron set exponential like show warped able exibly handle different type datasets shape learnt warp ing function found robust random initialisation parameter finally warped make better job predicting distribution shown difference value negative log density conclusion extension related work shown warped useful extension standard regression capable nding extra structure data transformation learns another viewpoint allows standard preprocessing transforms log discov ered automatically improved rather applied hoc manner demonstrated improvement performance regular several datasets course datasets well modelled already applying warped model simply result linear warping function found datasets censored many observation edge range lie single point cause warped problem warping function attempt model censoring pushing point far away rest data suffers performance especially learning deal properly censorship model required extension might consider warping input space nonlinear fash ion context geostatistics actually dealt hagan transformation made input space non stationary non isotropic covariance structure latent space usual condition sta tionarity isotropy hold gaussian process classi er thought warping output mapping onto probability interval however observation classi cation discrete point warped continuous space therefore likelihood different diggle consider various xed nonlinear transformation output emphasised presented method bene cial situation noise variance depends output value gaussian process noise variance depends input examined form non gaussianity directly depend output value heavy tailed noise cap tured method proposed propose current method used conjunction method targeted directly issue force method powerful yet easy computationally cheap apply acknowledgement many thanks david mackay useful discussion suggestion warping function datasets try cer supported german research council dfg grant reference williams rasmussen gaussian process regression touret zky mozer hasselmo editor advance neural information processing system mit press rasmussen evaluation gaussian process method non linear gression phd thesis university toronto gibbs bayesian gaussian process regression classi cation phd thesis cambridge university mackay introduction gaussian process bishop editor neural network machine learning nato asi series page kluwer academic press paul goldberg christopher williams christopher bishop regression input dependent noise gaussian process treatment advance neural information pro cessing system mit press radford neal monte carlo implementation gaussian process model bayesian gression classi cation technical report university toronto material algorithm project map program data library http www msm cam map entry html cole martin moran sheard bhadeshia mackay mod elling creep rupture strength ferritic steel weld science technology welding joining blake merz uci repository machine learning database http www ic uci edu mlearn mlrepository html torgo http www liacc ltorgo regression camacho inducing model human control skill phd thesis university porto hagan schmidt bayesian inference nonstationary spatial covariance struc ture via spatial deformation technical report university shef eld diggle tawn moyeed model based geostatistics applied statistic, seeing wind visual wind speed prediction coupled convolutional recurrent neural network department mechanical engineering department mechanical engineering jennifer cardona stanford university stanford jcard stanford edu michael howland stanford university stanford mhowland stanford edu john dabiri graduate aerospace laboratory galcit mechanical engineering california institute technology pasadena jodabiri caltech edu abstract wind energy resource quanti cation air pollution monitoring weather fore casting rely rapid accurate measurement local wind condition visual observation effect wind swaying tree apping ag example encode information regarding local wind condition potentially leveraged visual anemometry inexpensive ubiquitous demonstrate coupled convolutional neural network recurrent neural network architecture extract wind speed encoded visually recorded structure interaction tree naturally occurring wind prediction wind speed ranging showed agreement measurement cup anemometer site root mean squared error approaching natural wind speed variability due atmospheric turbulence generalizability network demonstrated successful prediction wind speed based recording ag eld controlled wind tunnel test furthermore physic based scaling apping dynamic accurately predicts dependence network performance video frame rate duration introduction ability accurately measure wind speed important across several application including locating optimal site wind turbine estimating pollution dispersion storm tracking currently taking measurement requires installing physical instrument exact location interest cost prohibitive case unfeasible knowledge wind resource city particular interest urbanization draw larger portion world population area driving need distributed energy generation closer densely populated region burgeoning interest use drone delivery would greatly bene instantaneous knowledge local wind condition minimize energy consumption ensure safety demonstrate technique enables wind speed measurement made video apping swaying tree facilitates visual anemometry pre existing feature environment would non intrusive cost effective wind mapping conference neural information processing system neurips vancouver canada structure interaction object wind encodes information wind speed neural network potentially used decode information leverage machine learning predict wind speed based interaction general approach current problem us convolutional neural network cnn feature extractor frame sequence followed recurrent neural network rnn taking feature extracted time series frame input algorithm second video clip sequence image output wind speed prediction meter per second visual anemometry technique potential signi cantly decrease cost time required mapping wind resource installing anemometer monitor single location typically cost even offer measurement location installed ag tree eld site collect initial training test data application method would occur pre existing structure environment interest therefore cost method camera recording device standard camera phone provides suf cient resolution hence cost method dramatically lower per measurement point barrier posed time labor required install anemometer location removed related work innovation proposed study use video observing structure interaction make wind speed prediction without classical meteorological measurement input enables wind speed prediction much broader range physical environment especially meteorological sensor would expensive impractical install classic method visual cue estimate wind speed include beaufort scale provides rough estimate based human perception surrounding environment griggs putnam index relies angle plant growth estimate annual average speed recent advance machine learning present work seek extend capability visual wind speed measurement provide automated quantitative real time measurement extracting physical quantity video become increasingly prevalent several study used image video estimate material property object speci cally cloth video input used predict dynamic object physical property uid ows estimating model parameter physical simulation similarity comparison video data shown promise approximating static dynamic property hanging cloth mass colliding object recently wind velocity material property given apping success type parameter estimation speaks potential computer vision used determining physical quantity long interest use neural network predicting future wind speed based historical measurement much work area focused wind forecasting time series measurement existing instrumentation weather forecast data input deep learning recently employed meteorological precipitation nowcasting author used spatiotemporal radar echo data make short term rainfall intensity prediction convolutional lstm recent work shown success classifying action motion video clip deep network present work aimed wind speed regression video draw inspiration previous study aimed classifying video deep learning era number approach video classi cation arisen deep network notable strategy taken hold include convolutional network short clip stream network aiming extract context motion information separate stream combination convolutional network subsequent recurrent layer treat multiple video frame time series carreia performed comparison different approach video classi cation study employ strategy cnn followed long short term memory lstm layer approach leverage transfer learning cnn extract feature related instantaneous state rnn analyze wind induced apping motion discussion architecture choice found section example cropped video frame input training validation adjacent test training validation tree tunnel test dataset main dataset used training validation consisted video taken eld site lancaster course day august video hour used order ensure daylight condition video captured motion standard checkerboard aspect ratio size mounted height well canopy young southern magnolia tree magnolia grandi os approximately height video recorded frame per second video subsequently separated second clip sequential frame formatted rgb image cropped pixel thus used model input consists time series image total size ground truth minute average wind speed label provided anemometer site height minute averaging time chosen instead shorter averaging time highly turbulent variable anemometer spatially separated instantaneous measurement made anemometer correspond exact instantaneous speed experienced sequence matched wind speed label timestamp rst series measured minute averaged wind speed ranged natural distribution wind speed uniform time period data collection many sample middle speed range tail since desired output model prediction broad range wind speed uniform distribution preferable achieve ground truth wind speed binned increment bin excess sample excluded retain even distribution range wind speed clip chosen random training validation split resulting training set contained clip clip tree clip validation set tree contained clip respectively wind speed distribution dataset provided supplementary material document ass generalizability network test set collected containing video new ag additional location eld test set called adjacent test set wind tunnel test set called tunnel test set adjacent flag test set adjacent test set comprised clip identical used training validation test located training validation clip taken time clip validation set allow direct comparison validation result test result new although timestamps wind speed label identical validation set adjacent test set precise wind madjacent flag test croptrain val flag crop mtunnel flag crop cmtrain val tree crop schematic model architecture cnn pre trained resnet architecture lstm many architecture layer containing hidden unit condition corresponding motion differ turbulence chaotic variable space tunnel test set tunnel test set consists clip taken smaller checkered mounted wind tunnel tunnel aspect ratio ag discussed size wind tunnel run speed speed second clip recorded frame per second yielding tunnel test sample although video recorded monochrome camera converted channel image repeating grayscale pixel value channel use model nal datasets used work available http purl stanford edu method feature extraction resnet analyzing video clip time series individual frame fed cnn extract relevant feature resnet architecture chosen cnn proven accuracy previous task relatively low computational cost pre trained weight resnet used current implementation take advantage transfer learning available mathworks deep learning toolbox since purpose cnn feature extraction rather classi cation last layer resnet fully connected layer softmax output layer removed resulting output frame feature map since activation function nal layer rectus linear unit relu max many resulting feature zero therefore reduce memory constraint output feature fed additional maximum pooling layer lter size stride act downsample feature reduces number zero dataset reducing feature map size attened resulted output second frame clip used input recurrent network lstm without mean subtracted input rnn selected order learn temporal feature video apping ag broadband containing wide range spectral scale typically ag located turbulent atmospheric boundary layer length scale govern vary order kilometer order micrometer associated time scale range cnncnncnnlstmlstmlstmlstmlstmlstm table final hyperparameter choice lstm network hyperparameter lstm layer hidden unit per lstm layer learning rate chosen value millisecond minute therefore architecture chosen application adapt variable spectral composition eld captured motion apping long short term memory lstm network chosen application shown lstm capability learn interaction range scale series well advantage training longer time series making suitable choice application generic lstm cell computed input forget output gate gate cid cid tanh cell hidden state computed equation respectively cid cid cid tanh weight matrix contains learnable parameter sigmoid function given tanh lstm easily trained long sequence compared standard rnns susceptible problem vanishing gradient arises due successive multiplication backpropagation standard rnn lstm network cell state allows uninterrupted gradient memory cell backpropagation requires multiplication rather discussed section input lstm network obtained feature extracted pre trained resnet network since wind condition uenced diurnal cycle weather condition plausible model could use feature present video clip motion object position sun presence cloud study avoid artifact tting background condition experiment run network architecture hyperparameters trained different input referred lstm short mean lstm raw respectively lstm experiment temporal mean feature frame clip subtracted input avoid tting background feature mean subtracted feature map served input lstm network lstm raw second model trained raw feature extracted resnet model without mean subtraction main purpose experiment identify whether removing temporal mean sequence bene cial model generalizability new location con fact object motion used prediction lstm architecture used many since series image fed lstm network regression prediction made schematic overall architecture shown hyperparameters chosen based value used spatiotemporal task similar model architecture literature nal size lstm network chosen layer hidden unit per layer smaller model considered layer hidden unit layer hidden unit model suffered high bias tting training set summary chosen hyperparameters shown table implementation detail problem framed regression regression output layer allows model predict wind speed opposed speci class mean squared error used loss function ned cid number training example wind speed label predicted wind speed label given training example mean squared error chosen mean absolute error heavily penalize outlier particularly undesirable application related wind energy due cubic dependence wind power wind speed stochastic gradient descent momentum used optimization typical momentum parameter algorithm implemented matlab deep learning toolbox lstm network trained epoch minibatches sample amount training suf cient training set within limit natural wind speed variability due turbulence early stopping employed regularization computation performed single cpu result discussion table error metric validation test case overall rmse indicates rmse wind speed measurable range rmse refers rmse wind speed ranging described section lstm raw rsme lstm rmse dataset flag validation set tree validation set adjacent flag test set tunnel test set overall measurable range overall measurable range lstm validation result show mean wind speed prediction bin plotted true minute average wind speed label validation set vertical error bar represent standard deviation prediction bin horizontal error bar range wind speed captured bin eld datasets accuracy anemometer measurement tunnel set overall root mean squared error rmse tree validation set respectively approach natural wind variability due atmospheric turbulence discussed section mean prediction bin show good agreement true label model tends predict high wind speed predict low wind speed although error low wind speed might due lack training example range see training distribution supplementary material document discussed detail next section found reduced accuracy lowest highest wind speed could predicted based knowledge physic structure interaction well video duration temporal resolution measurement limitation high low wind speed lowest highest wind speed tested increased prediction error explained inability current dataset capture relevant physic necessary measure wind speed mean lstm model prediction function true wind speed label validation set test set perfect model would carry ratio indicated unity line overlaid plot dashed black line vertical error bar indicate standard deviation horizontal error bar indicate range wind speed represented mark based binning eld datasets measurement uncertainty anemometer used tunnel test set wind speed outside measurable range due clip duration frame rate shown shaded red yellow shaded region respectively look speci cally apping eld illustration pertinent physic captured frequency scale order frequency corresponding uid element passing length wind speed high wind speed measurement capability limited sampling rate nyquist frequency ned yquist highest frequency signal still observed without effect aliasing nyquist frequency upper bound characteristic frequency corresponding critical velocity high calculated high lfn yquist case given frame rate yield nyquist frequency yquist length xed applying value equation give high wind speed exceeding value characteristic frequency would measurable without aliasing appears manifest prediction high wind speed value low wind speed duration clip limiting factor lowest frequency fully observed critical velocity given low given clip length second low wind speed lower would fundamental frequency low fully observe known limitation model performance speed rmse range hereafter referred measurable wind speed range reported addition overall rmse validation set rmse within range result summarized table red yellow shaded region indicate wind speed outside measurable range due duration sampling rate respectively comparison turbulent fluctuation evaluating model performance important consider natural variation wind speed due turbulence variation expected rmse model prediction least standard deviation turbulence uctuations denoted calculated minute averaging time uctuating velocity cid given equation respectively cid cid cid instantaneous velocity time averaged velocity calculate eld site minute average wind speed measurement used mean velocity second average used represent instantaneous velocity second averaging time instantaneous measurement chosen order match duration video clip used model input thus prediction network seen comparable instantaneous measurement ideal case standard deviation predicted value match standard deviation instantaneous anemometer measurement wind speed determine range wind speed measurement binned increment mean natural variability due turbulence calculated bin represented gray band analysis allows comparison prediction anemometer data rst comparison minute average wind speed represent site given time shown marker compared dashed black line representing unity good agreement second comparison standard deviation prediction size error bar representing standard deviation prediction approaching size gray band shown indicates model performance approach best possible accuracy given natural wind variability lstm test result model generalizability model prediction adjacent test set tunnel test set plotted true label discussed section adjacent test set serf direct comparison validation set although model still capture increasing trend prediction high wind speed prediction low wind speed pronounced validation set visible atter shape curve shown tunnel test set result shown orange mark similarly adjacent test set prediction tunnel test set capture correct qualitative increasing trend although highest wind speed case appears predicted plausible explanation test set prediction lie narrower range validation set prediction tunnel test set length shorter mean model may limited physic even lower speed section narrower range prediction corrected adjacent test set suggests model may partially speci tree trained relying partly speci feature object effect tting may become le signi cant training set expanded include diverse set ag tree test set prediction still monotonically increasing increasing true wind speed suggesting model capability partially limited factor frame rate tting test set rmse measurable wind speed range close validation set table indicates current model potential make prediction ag trained ag exist new location suggests possibility generalizability type model new setting potential broader use mapping wind speed lstm raw result effect mean subtraction discussed section lstm raw experiment used model architecture hyperparameters lstm experiment used raw input rather input temporal mean subtracted input lstm raw model performed similarly lstm model validation set table however performed notably worse test set adjacent test set lstm raw model gave rmse measureable range compared lstm model tunnel test set lstm raw model gave rmse compared lstm decrease performance test set lstm raw model indicates without temporal mean subtraction model unable make accurate prediction new location different background conclusion future work coupled cnn rnn resnet lstm architecture trained successfully predict wind speed within range video apping ag swaying tree prediction error approaching minimum expected error due turbulence uctuations validation set model performance test set consisting new ag additional location suggests model may generalize could therefore prove useful measuring wind speed new environment data driven approach visual anemometry could offer signi cant bene application mapping complex wind eld urban environment could cut time cost required measure wind speed several location currently done installing instrument point interest although study focused speci cally video clip checkered ag magnolia tree approach wind speed measurement potentially generalize type object including type ag natural vegetation interact surrounding wind addition training broader dataset including variety object con dence potential model generalization improved given deeper understanding relevant physic model prediction showed measurement capability model limited fundamental frequency future work focus understanding physic uid structure interaction extracted model necessary accurate prediction acknowledgement author acknowledge kelyn wood assisted setup wind tunnel test set funded brit alex arbeloff stanford graduate fellowship funded national science foundation graduate research fellowship grant dge stanford graduate fellowship reference silas alben michael shelley flapping state inviscid uid bistability transition chaos physical review letter moez baccouche franck mamalet christian wolf christophe garcia atilla baskurt sequential deep learning human action recognition international workshop human behavior understanding berlin heidelberg springer thanasis barbounis john theocharis mina alexiadis petros dokopoulos long term wind speed power forecasting local recurrent neural network model ieee transaction energy conversion kanna bhaskar singh awnn assisted wind power forecasting feed forward neural network ieee transaction sustainable energy kiran bhat christopher twigg jessica hodgins pradeep khosla zoran popovi steven seitz estimating cloth simulation parameter video proceeding acm siggraph eurographics symposium computer animation sca page katherine bouman bei xiao peter battaglia william freeman estimating material property fabric video proceeding ieee international conference computer vision page erasmo cadenas wilfrido rivera wind speed forecasting different region mexico hybrid arima ann model renewable energy carreira andrew zisserman quo vadis action recognition new model kinetics dataset ieee conference computer vision pattern recognition page francesco castellani massimiliano burlando samad taghizadeh davide astol emanuele piccioni wind energy forecast complex site hybrid neural network cfd based method energy procedia aiguo dai clara deser diurnal semidiurnal variation global surface wind divergence eld journal geophysical research abe davis katherine bouman justin chen michael rubinstein oral durand william freeman visual vibrometry estimating material property small motion video ieee transaction pattern analysis machine intelligence jeff donahue lisa hendricks sergio guadarrama marcus rohrbach venugopalan sub hashini kate saenko trevor darrell long term recurrent convolutional network visual recognition description ieee conference computer vision pattern recognition page zhenhai guo weigang zhao haiyan jianzhou wang multi step forecasting wind speed modi emd based arti cial neural network model renewable energy kaiming xiangyu zhang shaoqing ren jian sun deep residual learning recognition ieee conference computer vision pattern recognition page sepp hochreiter vanishing gradient problem learning recurrent neural net problem solution international journal uncertainty fuzziness knowledge based system sepp hochreiter urgen schmidhuber long short term memory neural computation mathworks inc deep learning toolbox http www mathworks com product deep learning html mathworks inc deeplearning ref resnet html resnet http www mathworks com help shuiwang wei ming yang kai convolutional neural network human action recognition ieee transaction pattern analysis machine intelligence daniel kammen deborah sunter city integrated renewable energy urban sustainability science andrej karpathy justin johnson fei fei visualizing understanding recurrent network arxiv preprint arxiv andrej karpathy george toderici sanketh shetty thomas leung rahul sukthankar fei fei scale video classi cation convolutional neural network proceed ings ieee conference computer vision pattern recognition page gong jing shi comparing arti cial neural network wind speed forecasting applied energy hui liu xiwei yanfei smart multi step deep learning model wind speed forecasting based variational mode decomposition singular spectrum analysis lstm network elm energy conversion management abhimitra meka maxim maximov michael zollhofer avishek chatterjee han seidel christian richardt christian theobalt lime live intrinsic material estimation proceed ings ieee computer society conference computer vision pattern recognition page mohamed mohandes sha qur rehman talal halawan neural network approach wind speed prediction renewable energy roozbeh mottaghi hessam bagherinezhad mohammad rastegari ali farhadi newtonian understanding unfolding dynamic object static image proceeding ieee computer society conference computer vision pattern recognition page david rumelhart geoffrey hinton ronald williams learning internal repre sentations error propagation ic california univ san diego jolla inst cognitive science tom runia kirill gavrilyuk cees snoek arnold smeulders flow perception ned physic simulation arxiv preprint arxiv hidetomo sakaino fluid motion estimation method based physical property wave ieee conference computer vision pattern recognition page xingjian shi zhourong chen hao wang dit yan yeung wai kin wong wang chun woo convolutional lstm network machine learning approach precipitation nowcasting advance neural information processing system page karen simonyan andrew zisserman stream convolutional network action recognition video advance neural information processing system page lisa spencer mubarak shah water video analysis international conference processing icip volume page ieee joshuah stolaroff constantine samara emma neill alia lubers alexandra mitchell daniel ceperley energy use life cycle greenhouse gas emission drone commercial package delivery nature communication tran lubomir bourdev rob fergus lorenzo torresani manohar paluri learn ing spatiotemporal feature convolutional network proceeding ieee international conference computer vision page john wade wendell hewson tree local climatic wind indicator journal applied meteorology jiajun joseph lim hongyi zhang joshua tenenbaum william freeman physic learning physical object property unlabeled video bmvc volume jiajun ilker yildirim joseph lim william freeman joshua tenenbaum galileo perceiving physical object property integrating physic engine deep learning advance neural information processing system page yang junbang liang ming lin learning based cloth material recovery proceeding ieee international conference computer vision page video haiyang zhihai shuqin wang yunpeng wang xiaolei spatiotemporal recurrent convolutional network traf prediction transportation network sensor joe yue hei matthew hausknecht sudheendra vijayanarasimhan oriol vinyals rajat monga george toderici beyond short snippet deep network video classi cation ieee conference computer vision pattern recognition page)\n"
          ]
        }
      ],
      "source": [
        "for target, context in positive_skip_grams[:5]:\n",
        "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:13:52.778537Z",
          "iopub.status.busy": "2021-05-11T17:13:52.777813Z",
          "iopub.status.idle": "2021-05-11T17:13:52.781753Z",
          "shell.execute_reply": "2021-05-11T17:13:52.782316Z"
        },
        "papermill": {
          "duration": 0.035869,
          "end_time": "2021-05-11T17:13:52.782489",
          "exception": false,
          "start_time": "2021-05-11T17:13:52.746620",
          "status": "completed"
        },
        "tags": [],
        "id": "thorough-daisy"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:01.975964Z",
          "iopub.status.busy": "2021-05-11T17:14:01.974747Z",
          "iopub.status.idle": "2021-05-11T17:14:02.015851Z",
          "shell.execute_reply": "2021-05-11T17:14:02.015074Z"
        },
        "papermill": {
          "duration": 0.094053,
          "end_time": "2021-05-11T17:14:02.016035",
          "exception": false,
          "start_time": "2021-05-11T17:14:01.921982",
          "status": "completed"
        },
        "tags": [],
        "id": "preceding-fight",
        "outputId": "8430d642-d067-44e9-b659-666ef778bbe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target word: metric learning temporal sequence alignment damien garreau en damien garreau en emi lajugie inria remi lajugie inria sylvain arlot cnrs sylvain arlot en francis bach inria francis bach inria abstract paper propose learn mahalanobis distance perform alignment multivariate time series learning example task time series true alignment known cast alignment problem structured prediction task propose realistic loss alignment optimization tractable provide experiment real data audio audio context learning similarity measure lead improvement performance alignment task propose use metric learning framework perform feature selection basic audio feature build combination better alignment performance introduction problem aligning temporal sequence ubiquitous application ranging bioinformat ic audio processing goal align similar time series global structure local temporal difference alignment algorithm rely similar ity measure good metric crucial especially high dimensional setting feature signal irrelevant alignment task goal paper learn similarity measure annotated example order improve relevance alignment example context music information retrieval alignment used different case audio audio alignment audio score alignment rst case goal match audio interpretation piece potentially different rythm whereas audio score alignment focus matching audio signal symbolic representation score second case attempt learn annotated data measure performing alignment joder propose generative model context keshet learn measure discriminative setting similarly keshet use discriminative loss learn measure work focus audio audio alignment context set authorized alignment much larger explicitly cast problem structured prediction task solve shelf stochastic optimization technique proper signi cant adjustment particular term loss idea alignment relevant community speech recognition since pioneering work sakoe chiba contributed equally sierra project team epartement informatique ecole normale sup erieure cnrs inria en need metric learning go far beyond unsupervised partitioning problem weinberger saul proposed margin framework learning metric nearest neighbour algorithm based set must link must link constraint lajugie proposed use margin framework learn mahalanobis metric context partitioning problem since structured svm proposed tsochantaridis taskar successfully used solve many learning problem instance learn weight graph matching metric ranking task used learn graph structure graph cut make following contribution cast learning mahalanobis metric context alignment structured prediction problem real musical datasets metric improves performance alignment algo rithms high level feature propose use metric learning framework learn combination basic audio feature get good alignment performance experimentally standard hamming loss although tractable computationnally permit learn relevant similarity measure real world setting propose new loss closer true evaluation loss alignment leading tractable learning task derive cient frank wolfe based algorithm deal new loss loss solves issue encountered hamming loss matricial formulation alignment problem notation paper consider alignment problem multivariate time series sharing dimension possibly different length namely rta rtb refer row ata btb column vector denote pair signal let rta arbitrary pairwise nity matrix associated pair encodes nity note framework extended case multivariate signal different dimension long well ned goal alignment task non decreasing sequence index length max match time index time series time maximal index time series way cid satis matching beginning matching ending type move given binary matrix every otherwise denote set matrix uniquely determined example given vertical move matrix mean signal waiting whereas horizontal mean waiting diagonal move mean move together sense time reference warped known alignment task cast following linear program set cid max goal learn form nity matrix learned alignment obtained optimization problem referred decoding model dynamic time warping given nity matrix associated pair signal nding alignment solves done ciently tatb example valid alignment encoded matrix red upper triangle grey zone corresponds area loss ab blue lower one dynamic programming algorithm often referred dynamic time warping algorithm described alg supplementary material various additional constraint may used dynamic time warping algorithm could easily add alg cardinality set huge corresponds number path rectangular grid southwest northeast corner vertical horizontal diagonal move allowed nition delannoy number noted go nity mahalanobis metric many application see pair nity matrix computed cid cid paper propose learn metric compare instead plain euclidean metric parametrized matrix set semi nite positive matrix use corresponding mahalanobis metric compute pairwise nity cid note decoding maximization linear function parameter max cid max cid joint feature map cid cid cid learning metric assume given pair training instance goal matrix predicted alignment close groundtruth example well unseen example rst loss alignment order quantify proximity alignment see necessary fully labelled instance mean pair need exact alignment partial alignment might dealt alternating metric learning constrained alignment loss alignment framework alignment encoded matrix thus interested func tions cid frobenius norm ned cid cid hamming loss simple loss matrix frobenius norm difference turn unnormalized hamming loss valued matrix matrix ned cid cid cid cid cid cid cid cid cid cid vector coordinate equal last line come fact value make hamming loss loss often used structured prediction task audio score setting keshet use modi version loss average number time difference alignment greater xed threshold loss easy optimize since linear parametrization alignement problem optimal audio audio alignment indeed major drawback hamming loss alignment xed length depends number crossing alignment path easily cid cid much closer see important notice often case length signal grows area loss natural loss computed mean distance beween path depicted matrix loss corresponds area path matrix represented grey zone formally put min area loss mean audio literature loss sometimes called mean absolute deviation loss noted ab unfortunately general alignment problem ab linear matrix context alignment sequence different nature signal reference thus index sequence ned increasing audio partition alignment problem loss linear argument precisely introduce matrix lta rta lower triangular one including diagonal write loss min cid cid lta cid lta cid cid lta cid lta cid lta kyk cid easy see lta cid cid prove loss corresponds area loss special case let alignment vertical move unique ykj ltay lta exactly area curve determined path experiment use ab evaluation training approximation area loss many real world applica tions meaningful loss ass quality alignment area loss shown experiment hamming loss suf cient simple situation allows learn metric lead good alignment performance term area loss challenging datasets work see sec due fact alignment close term area loss suffer big hamming loss thus natural extend formulation matrix start symmetrizing formulation overcome problem overpenalization vertical horizontal move couple binary matrix symmetrized area loss cid cid cid lta cid cid cid cid ltb cid cid ltb cid lta ltay cid cid ltb cid cid cid cid ltb cid ltay cid cid cid real world bach chorale dataset represented groundtruth alignment together others term hamming loss alignment far groundtruth whereas area loss structured prediction setting described sec depicted alignment called violated constraint namely output loss augmented decoding step see sec propose make loss concave convex hull denote let introduce max cid max largest eigenvalue binary matrix cid cid cid cid dtb cid cid cid lta dta dta cid dta ltb cid dtb ltay cid cid ltb cid ltb cid cid cid get concave function coincides cid empirical loss minimization recall given alignment example xed loss cid goal solve following minimization problem cid cid cid argmax cid cid min convex regularizer preventing tting cid cid margin approach section describe margin approach solve surrogate problem untractable shown decoding task maximum linear function parameter aim predicting output discrete space space potential alignment respect constraint learning thus fall structured prediction framework hinge loss convex surrogate max cid cid cid cid cid cid cid completeness experiment try set matrix minimal trace dominate cid solving semide nite program sdp report associated note matrix could chosen particular since matrix pointwise positive matrix diag cid loss concave cid tatb violated constraint hamming lossmost violated constraint lsgroundruth alignment evaluation usually referred loss augmented decoding see cid argmax elementary computation cid argmin cid cid cid cid aim solving following problem sometimes called margin rescaled problem rta cid cid cid cid cid cid min cid cid max cid hamming loss case notice joint feature map linear thus take loss linear rst argument cid instance hamming loss loss augmented decoding maximization linear function space solve ciently dynamic programming algorithm see sec supplementary material way plugging hamming loss lead convex structured prediction problem problem solved standard technique cutting plane method stochastic gradient descent block coordinate frank wolfe dual note adapted standard unconstrained optimization method setting cid cid cid cid min optimization symmetrized area loss symmetrized area loss concave rst argument thus problem min max form deriving dual straightforward detail found supplementary material plug symmetrized area loss cid sal ned problem dual following form cid cid denote convex hull set cartesian product training example set note recover similar since sal loss concave aforementioned problem convex problem quadratic program compact set thus use frank wolfe algorithm note similar proposed lacoste julien additional term due concavity loss cid experiment applied method task learning good similarity measure aligning audio sig nals eld researcher spent lot effort designing well suited meaningful feature problem combining feature aligning temporal sequence still challenging simplicity took diagonal experiment dataset kirchhoff lerch dataset description first applied method dataset kirchhoff lerch dataset pair aligned example arti cially created stretching original audio signal way groundtruth alignment known thus data fall setting precise description dataset found pair stretched along different tempo curve signal made music divided frame hopsize thus leading typical length signal setting keep feature simple implement known perform well alignment task mfcc labeled spectral atness spectral centroid spectral spread maximum envelope max power level frame pow see detail computation feature normalize feature subtracting median value dividing standard deviation median audio data subject outlier comparison performance individual feature learned metric error bar performance learned metric determined best worst perfor mance different experiment denotes learned combination method best mfcc combination experiment conducted following experiment individual feature perform alignment dynamic time warping algorithm evaluate performance single feature term loss typically used ass performance setting report result experiment plug data method hamming loss learn linear positive com bination feature reported thus combining feature dataset yield better performance considering single feature completeness conducted experiment standard rst mfccs coef cients rst second order derivative feature result competed best learned combination handcrafted feature namely term ab loss perform second note result slightly worse best single handcrafted feature better best mfcc coef cient used feature baseline compared uniform combination handcrafted feature metric identity matrix result chart ab second individual value ranging second second chorale dataset dataset bach dataset consists ten bach chorale small quadriphonic piece chorale midi reference corresponding score basically representation partition alignment midi le audio given thus converted midi le audio following classically done alignment see way fall audio audio framework technique apply piece music approximately long leading similar signal length experiment use feature sec depicted optimization hamming loss performs poorly dataset fact best individual feature performance far better performance learned thus metric learning practical hamming loss performs much worse best single feature conducted learning experiment symetrized area loss cid resulting learned parameter far better learned hamming loss get performance similar best feature note feature handcrafted reaching performance hard task training instance already challenging http music northwestern edu data bach html wmpowm scm srsfm maxssm ab performance algorithm chorale dataset left right best single feature best learned combination feature symmetrized area loss cid best combination mfcc sal obtained via sdp see footnote section best combination mfcc derivative learned cid best combination mfccs derivative learned hamming loss best combination feature hamming loss depicted learned parameter loss augmented decoding performed either area known structured svm represents violated constraint see violated constraint hamming loss lead align ment totally unrelated groundtruth alignment whereas symmetrized area loss far closer much discriminative feature selection last conducted feature selection experiment datasets starting low level feature namely leading mfccs coef cients rst derivative learn linear combination achieves good alignment performance term area loss note little musical prior knowledge put moreover either improve best handcrafted feature dataset perform similarly datasets performance learned combination handcrafted feature performed similarly combination mfccs coef cients conclusion paper presented structured prediction framework learning metric tempo ral alignment problem able combine hand crafted feature well building automat ically new state art feature basic low level information little expert knowledge technically made possible considering loss beyond usual hamming loss typically used practical within structured prediction framework linear output representation present work may extended several way main consider case partial information alignment available often case music bioinformatics application note similarly lajugie simple alternating optimiza tion metric learning constrained alignment provide simple rst solution could probably improved upon acknowledgement author acknowledge support european research council sierra project gargantua project funded mastodon program cnrs airbus foundation phd fellowship thanks piotr bojanowski helpful discussion warm thanks arshia cont philippe cuvillier sharing knowledge audio processing holger kirchhoff alexander lerch dataset ab reference aach church aligning gene expression time series time warping algorithm bioinfor matics banderier schwer delannoy number journal statistical planning inference caetano mcauley cheng smola learning graph matching ieee trans pami cont schwarz schnell raphael evaluation real time audio score alignment proc ismir cuturi vert birkenes matsui kernel time series based global alignment proc icassp volume page ieee dixon widmer match music alignment tool chest proc ismir page frank wolfe algorithm quadratic programming naval research logistics quarterly gold morgan elli speech audio signal processing processing perception speech music john wiley son hamming error detecting error correcting code bell system technical journal dannenberg tzanetakis polyphonic audio matching alignment music retrieval computer science department page joachim finley cutting plane training structural svms machine learning joder essid richard learning optimal feature polyphonic audio score alignment ieee trans audio speech language processing keshet shalev shwartz singer chazan margin algorithm speech phoneme music score alignment ieee transaction audio speech language processing kirchhoff lerch evaluation feature audio audio alignment journal new music research lacoste julien jaggi schmidt pletscher block coordinate frank wolfe optimization structural svms proc icml lajugie bach arlot margin metric learning constrained partitioning problem proc icml mcfee lanckriet metric learning rank proc icml page uller information retrieval music motion springer sakoe chiba dynamic programming algorithm optimization spoken word recognition acoustic speech signal processing ieee transaction shalev shwartz singer srebro cotter pegasos primal estimated sub gradient solver svm mathematical programming szummer kohli hoiem learning crfs graph cut proc cvpr taskar koller guestrin max margin markov network adv nip thompson plewniak poch balibase benchmark alignment database evaluation multiple alignment program bioinformatics torres cabada nieto exact formula number alignment dna sequence mitochondrial dna tsochantaridis joachim hofmann altun singer margin method structured interdependent output variable journal machine learning research weinberger saul distance metric learning margin nearest neighbor classi cation journal machine learning research, context word: resnets provably better linear predictor department computer science applied mathematics ohad shamir weizmann institute science rehovot israel ohad shamir weizmann abstract residual network resnet standard deep neural net architecture state art performance across numerous application main premise resnets allow training layer focus tting residual previous layer output target output thus expect trained network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather getting stuck arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual unit indeed exhibit behavior sense optimization landscape contains local minimum value obtained linear predictor namely layer network notably minimal assumption precise network architecture data distribution loss function used provide quantitative analysis approximate stationary point problem finally certain tweak architecture training network standard stochastic gradient descent achieves objective value close better linear predictor introduction residual network resnets popular class arti cial neural network providing state art performance across numerous application kim xie xiong unlike vanilla feedforward neural network resnets characterized skip connection output layer directly added output following layer mathematically whereas feedforward neural network expressed stacking layer form input output pair tunable parameter function resnets built residual unit form xed function fact common let identity case unit take form intuitively mean layer training focus tting residual target given rather particular adding depth harm performance since effectively eliminate layer tuning zero function due property residual network proven effective training extremely deep network hundred layer despite widespread empirical success rigorous theoretical understanding training residual network limited recent theoretical work optimization deep learning conference neural information processing system neurips montr canada soltanolkotabi yun soudry hoffer brutzkus safran shamir lee name example focused simpler feedforward architecture capture property residual network recent result consider residual like element see discussion related work generally apply standard architecture particular aware theoretical justi cation basic premise resnets namely architecture allows adding layer without harming performance problem training neural network involves solving highly non convex problem local search procedure thus even though deeper residual network express shallower one clear training process indeed converge network better perhaps attempt train residual network gradient based method might hit poor local minimum worse error obtained shallower network question main motivation work secondary motivation several recent result yun safran shamir liang demonstrate spurious local minimum value larger global minimum exist general training neural network even fairly strong assumption thus instead aiming demonstrating minimum exist might good true realistic network perhaps consider modest goal showing minimum exist certain non trivial level set level set correspond instance optimal value attainable shallower network without additional residual layer paper study question considering competitiveness simple residual network composed arbitrarily deep nonlinear residual unit linear output layer respect linear predictor equivalently layer network speci cally consider optimization problem associated training residual network general non convex complicated structure nevertheless prove optimization landscape local minimum value higher achieved linear predictor data word run local search procedure reach local minimum assured solution worse best obtainable linear predictor importantly fairly minimal assumption residual unit assumption data distribution linear separability assumption loss function used besides smoothness convexity network output satis loss used practice addition provide quantitative analysis show every point close stationary certain direction see sec precise nition poly worse xed linear predictor result geometric nature explain later necessarily imply standard gradient based method indeed converge desirable solution example since iterates might diverge nevertheless provide algorithmic showing residual architecture changed bit standard stochastic gradient descent sgd procedure predictor similar better best linear predictor relies simple perhaps unexpected reduction setting online learning might independent interest supplementary material paper contains proof appendix discussion result generalized vector valued output appendix related work far know existing rigorous theoretical result residual network pertain linear network combine linear residual unit form although network used practice capture important aspect non convexity associated training residual network particular hardt showed linear residual network squared loss spurious local minimum namely every local minimum global recently bartlett proved convergence result gradient descent problem assuming input isotropic target linear mapping symmetric positive nite showing similar result non linear network mentioned hardt major open problem paper focus non linear residual unit consider local minimum level set term setting perhaps work closest liang considers network written cid hidden layer network arbitrary possibly deeper network technical assumption data distribution activation used network size assuming certain classi cation loss author prove training objective benign sense network corresponding local minimum zero classi cation error however author point architecture different standard resnets would require nal tunable layer combine output result provably hold architecture moreover technical assumption non trivial apply standard activation loss relu activation logistic loss require speci condition data linear separability certain low rank structure contrast study standard residual unit make minimal assumption network data distribution loss used side prove result local minimum certain level set rather point finally idea studying stationary point non convex optimization problem reference level set explored work setting quite different setting preliminary start word basic notation terminology generally use bold faced letter denote vector assumed column form capital letter denote matrix function cid cid refers euclidean norm vector spectral norm matrix unless speci otherwise cid cid matrix denotes frobenius norm always upper bound spectral norm matrix vec refers entry written long vector according canonical order given function euclidean space denotes gradient denotes hessian point domain function local minimum cid cid open neighborhood finally use standard notation hide constant let poly referto expression polynomial consider residual network architecture consisting residual unit composed linear output layer scalar output cid cid make assumption structure overall depth network computes except last layer tunable linear transformation namely matrix necessarily square parameter condition follows full pre activation structure proposed empirically found best performing residual unit architecture commonly used practice tensorflow depart structure fully tunable rather convolution facilitate simplify theoretical study assumption given network output cid cid parameterized vector matrix possibly complicated function parame terized remark bias note model easily incorporate bias namely predictor form cid cid tunable standard trick augmenting additional coordinate whose value always assuming output vector additional coordinate value since result depend data geometry speci would affected modi cation assume network trained respect data distribution average training set loss function cid network prediction target value thus consider optimization problem min cid cid cid cid see appendix discussion result generalized network vector valued output unconstrained objective main focus paper general objective convex easily spurious local minimum saddle point result make explicit assumption distribution structure loss assume throughout paper following assumption loss cid twice differentiable convex assumption mild satis standard loss logistic loss squared loss smoothed hinge loss etc note assumption twice differentiable respect particular function ned xed twice differentiable emphasize throughout paper assume necessarily differentiable respect indeed represents network non differentiable operator relu max function cannot expect differentiable everywhere considering derivative think input long vector euclidean space order speci vec vector matrix discussed introduction wish compare objective value obtained linear predictor speci cally use notation flin cid cid cid cid denote expected loss linear predictor parameterized vector assumption function convex twice differentiable finally introduce following class point behave approximately like local minimum respect term rst derivative nition sopsp let open subset domain lipschitz second order partial stationary point sopsp cid cid min importantly note local minimum must sopsp local minimum differentiable function hence cid cid min nition directly generalizes well known notion second order stationary point sosp mccormick nesterov polyak jin ned function twice differentiable parameter fact nition sopsp equivalent requiring sosp need use general nition assuming differentiable interestingly sosp general class point non convex optimization gradient based method shown converge poly iteration competitiveness linear predictor main result thm corollary proven stage first point cid cid cid lower bounded term suboptimality respect best linear predictor thm consider case point suboptimal respect best linear predictor either cid cid strictly positive min strictly negative thm thus building nition sopsp previous section point suboptimal compared linear predictor local minimum theorem point cid vector dimension cid cid cid flin cid cid cid cid cid cid cid cid cid cid theorem implies point objective value larger linear predictor flin unless partial derivative respect namely non zero cannot stationary point respect local minimum proof theorem appears supplementary material relies following key lemma shall state roughly sketch proof lemma fix cid vector size matrix cid cid cid cid vec cid flin word inner product gradient carefully chosen vector lower bounded suboptimality compared linear predictor particular point suboptimal gradient cannot zero cid cid cid vec cid cid cid proof sketch lemma cid vec cid cid cid cid cid cid cid cid vec let cid equal cid cid careful technical calculation reveals expression cid cid cid cid cid cid cid cid turn equal cid cid cid cid cid cid cid recalling nition cid noting convexity cid follows lower bounded cid cid cid cid cid cid flin cid cid cid analyze case following theorem cid cid cid cid cid cid min cid min flin cid cid cid cid cid cid min cid cid cid cid min denotes minimal eigenvalue symmetric matrix combining theorem following main theorem fix positive suppose convex open subset domain max cid cid cid cid lipschitz lipschitz lipschitz respectively cid cid sopsp min cid cid flin poly note poly term hide dependency linear individual factor see proof supplementary material exact expression discussed sec local minimum must correspond sopsp hence theorem implies point minw cid cid flin long satis lipschitz continuity assumption nite bounded subset domain since hold arrived following corollary corollary suppose bounded subset hold lipschitz continuous every cal minimum satis domain inf flin word objective spurious local minimum value smallest attainable linear predictor remark generalization vector valued output consider generalization setting network vector valued output namely cid matrix loss cid taking vector valued argument convex cross entropy loss general setting possible prove variant thm similar proof technique see appendix however clear prove analog thm hence thm leave question future research effect norm regularization thm implies sopsp must value much worse obtained linear predictor moreover discussed sec point closely related second order stationary point gradient based method known converge quickly point jin thus tempting claim method indeed network competitive linear predictor unfortunately fundamental catch bound thm depends norm point via cid cid cid cid arbitrarily bad norm suf ciently word thm guarantee point sopsp good long far away origin dynamic gradient method iterates remain bounded domain least suf ciently slowly increasing norm would issue however priori guaranteed would case since optimization problem unconstrained assuming anything structure could parameter diverge meaningful algorithmic derived thm course option dependence cid cid cid cid artifact analysis sopsp competitive linear predictor regardless norm however following example show case example fix suppose scalar squared loss dependence parameter cid objective equivalently written see leftmost plot gradient hessian equal cid cid cid cid left right contour plot superimposed constraint cid cid inside circle axis corresponds axis corresponds exhibit spurious local minimum bottom left quadrant domain best viewed color particular gradient hessian equal arbitrarily close small enough however objective value cid cid respectively point equal cid cid flin remark example gradient hessian uniformly bounded lipschitz constant euclidean space however lipschitz constant bounded numerical constant includes stationary point studied construction indicates problem indeed lie norm unbounded lipschitz constant derivative standard approach ensure iterates remain bounded add regularization namely optimize min regularization term penalizing norm unfortunately alter objective might introduce new spurious local minimum exist graphically illustrated plot example without regularization form whereas stationary point either global minimum along valley corresponding saddle point regularization created new spurious local minimum around intuitively regularization make objective value increase well valley global minimum regularization choice lead phenomenon similar issue occur impose hard constraint namely optimize min constrained domain illustrates optimization problem spurious local minimum inside constrained domain course way issue making regularization parameter suf ciently small domain suf ciently regularization come effect cid cid suf ciently however correct choice depends run problem simply xed example change time priori guarantee chosen thus clear xed choice regularization would work lead gradient based method good local minimum success sgd assuming skip connection output discussed challenge getting algorithmic previous section possible assuming architecture network changed bit concretely instead network architecture cid cid consider architecture parameterized vector new objective written cid cid cid cid cid cid cid cid cid cid architecture corresponds skip connection directly network output rather nal linear output layer similar spirit skip connection studied liang except layer nonlinear network instead linear cid component follows consider standard stochastic gradient descent sgd algorithm train network fixing step size convex parameter domain initialize point randomly data point underlying data distribution perform denote euclidean projection set cid cid cid note always differentiable respect assume simplicity differentiable respect simply cid still easily veri hold use notation flin cid arbitrary vector cid cid cid cid cid cid denote expected loss linear predictor parameterized following theorem establishes mild condition running stochastic gradient descent suf ciently many iteration result network competitive xed linear predictor theorem suppose domain satis following positive constant closed convex set clidean space namely cartesian product support data distribution cid cid cid lipschitz bounded absolute value cid cid cid cid cid suppose perform iteration stochastic gradient descent described step size satis probability least iterates min flin cid cid log cid proof relies technically straightforward perhaps unexpected reduction adversarial online learning appears supplementary material roughly speaking idea stochastic gradient descent procedure equivalent online gradient descent respect sequence function ned iterates even though iterates change unexpected complicated way strong guarantee online learning allow sequence function rather arbitrary allow obtain theorem acknowledgement thank anonymous nip reviewer helpful comment research supported part european research council erc grant reference peter bartlett david helmbold philip long gradient descent identity initialization ciently learns positive nite linear transformation deep residual network arxiv preprint arxiv alon brutzkus amir globerson eran malach shai shalev shwartz sgd learns parameterized network provably generalize linearly separable data arxiv preprint arxiv simon jason lee power parametrization neural network quadratic activation arxiv preprint arxiv simon jason lee yuandong tian barnabas poczos aarti singh gradient descent learns hidden layer cnn afraid spurious local minimum arxiv preprint arxiv rong tengyu optimization landscape tensor decomposition advance neural information processing system page rong jason lee tengyu learning hidden layer neural network landscape design arxiv preprint arxiv moritz hardt tengyu identity matter deep learning arxiv preprint arxiv elad hazan introduction online convex optimization foundation trend cid optimization kaiming xiangyu zhang shaoqing ren jian sun deep residual learning recognition proceeding ieee conference computer vision pattern recognition page kaiming xiangyu zhang shaoqing ren jian sun identity mapping deep residual network european conference computer vision page springer chi jin rong praneeth netrapalli sham kakade michael jordan escape saddle point ciently arxiv preprint arxiv jiwon kim jung kwon lee kyoung lee accurate super resolution deep convolutional network proceeding ieee conference computer vision pattern recognition page shiyu liang ruoyu sun yixuan srikant understanding loss surface neural network binary classi cation arxiv preprint arxiv garth mccormick modi cation armijo step size rule negative curvature mathematical programming yurii nesterov boris polyak cubic regularization newton method global performance mathematical programming itay safran ohad shamir spurious local minimum common layer relu neural network arxiv preprint arxiv shai shalev shwartz online learning online convex optimization foundation trend cid machine learning mahdi soltanolkotabi adel javanmard jason lee theoretical insight optimization landscape parameterized shallow neural network arxiv preprint arxiv daniel soudry elad hoffer exponentially vanishing sub optimal local minimum multilayer neural network arxiv preprint arxiv saining xie ross girshick piotr doll zhuowen kaiming aggregated residual transformation deep neural network computer vision pattern recognition cvpr ieee conference page ieee wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig microsoft conversational speech recognition system acoustic speech signal processing icassp ieee international conference page ieee chulhee yun suvrit sra ali jadbabaie critical view global optimality deep learning arxiv preprint arxiv martin zinkevich online convex programming generalized nitesimal gradient ascent proceeding international conference machine learning icml page\n",
            "tf.Tensor([16  4 40  8], shape=(4,), dtype=int64)\n",
            "['memory oriented decoder light field salient object detection miao zhang jingjing wei yongri piao huchuan dalian university technology china miaozhang dlut edu lijingjing jiwei mail dlut edu yrpiao lhchuan dlut edu abstract light eld data demonstrated favor many task computer vision existing work light eld saliency detection still rely hand crafted feature paper present deep learning based method novel memory oriented decoder tailored light eld saliency detection goal deeply explore comprehensively exploit internal correlation focal slice accurate prediction designing feature fusion integration mechanism success method demonstrated achieving state art datasets present problem way accessible member community provide scale light eld dataset facilitates comparison across algorithm code dataset made publicly available http github com oiplab dut molf introduction salient object detection sod ability identify visually distinctive object despite substantial appearance similarity scene fundamental task attracted lot interest due importance various application visual tracking object recognition segmentation retrieval robot navigation existing method categorized rgb rgb light eld saliency detection based input data type method achieved great success long dominant eld saliency detection however saliency detection method may suffer false positive come challenging scene shown reason twofold first traditional method underlie many prior knowledge violation highly pose risk complex scene second deep learning based method subject feature extracted limited rgb data containing much special information rgb data light eld data saliency detection attracted lot attention depth map providing scene layout improve saliency accuracy extent however mediocre quality depth map heavily jeopardize accuracy saliency detection light eld provides image scene array viewpoint spread extent lens aperture different view used produce stack focal slice containing abundant spatial parallax information well accurate depth information object scene furthermore focusness strongest information allowing human observer instantly understand order object arranged along depth scene light eld data demonstrated favor many application computer vision depth estimation super resolution material recognition due denotes equal contribution prof piao corresponding author conference neural information processing system neurips vancouver canada left challenging scene similar foreground background complex back ground transparent object low intensity environment right light eld data focal slice focus different depth level green box red dot represents different focus position observation bene cial cient foreground background separation show model saliency result mean ground truth unique property light eld shown promising prospect saliency detection however deep learning based light eld method missing contemporary study saliency detection strong reason believe introducing cnn framework light eld saliency detection important aspect method sod order incorporate cnn framework light eld accurate sod key issue needed considered first solve ciency training data second effectively properly fuse light eld feature generated different focal slice third comprehensively integrate multi level feature paper leverage idea light eld confront challenge better adapt network fuse feature focal slice may neither want ignore contribution corresponding focal slice salient object happens focus destroy spatial correlation different focal slice therefore propose novel memory oriented spatial fusion module sfm resemble memory mechanism human fuse information understand scene going piece information emphasizing relevant one hand integration fused feature used higher cognitive processing therefore propose sophisticated multi level integration mechanism top manner high level feature used guide low level feature selection namely memory oriented feature integration module fim previous information referred memory used channel attention update current light eld feature important unnecessary feature distinguishable summary main contribution follows introduce scale light led saliency dataset sample con tains focus focal stack focal slice depth map corresponding ground truth genuinely hoping could pave way light eld sod enable advanced research development propose novel memory oriented decoder tailored light eld sod feature fusion mechanism sfm feature integration mechanism fim enable accurate prediction work best knowledge rst exploitation unique focal slice light eld data deep learning based saliency detection extensive experiment light eld datasets method achieves consis tently superior performance state art approach related work salient object detection early work saliency detection mainly rely hand crafted feature prior knowledge color contrast background prior recently utilization cnns sod achieved appealing performance adopt cnn extract multi scale feature predict saliency super pixel wang propose cnns integrate local super pixel estimation global search sod zhao utilize independent cnns extract global local context lee combine low level distant map high level semantic feature deep cnns sod rgb complex scenario method achieve better performance suffer time consuming computation injure spatial information input image afterwords liu han rst generate coarse saliency map detail step step hou introduce short connection multiple side output based hed architecture zhang integrate multi level feature multiple resolution combine accurate prediction luo propose simpli cnn combine local global information design loss penalize boundary error zhang liu introduce attention mechanism guide feature integration deng design residual nement block learn complementary saliency information intermediate prediction transfer contour knowledge saliency detection without manual saliency mask detailed survey sod found sod depth image uent spatial information act complementary cue saliency detection peng regard depth data channel input feed multi stage saliency detection model feng present saliency method based anisotropic center surround difference local background enclosure zhu propose center dark channel prior rgb sod use hand crafted feature train cnn achieve better performance tradition method stream model used process rgb depth map separately cross modal feature combined jointly make prediction due limited training set trained stage wise manner chen design progressive fusion network fuse cross modal multi level feature predict saliency map chen propose stream network extract rgb feature use attention mechanism adaptively select complement zhu use scale rgb datasets pre train prior model employ depth induced feature enhance network previous work light eld sod shown promising prospect especially complex scenario report saliency detection approach light eld data propose rst light eld saliency dataset lfsd zhang propose saliency method based depth contrast focusness based background prior effectiveness superiority light eld property introduce weighted sparse coding structure handling heterogenous type input data zhang integrate multiple visual cue light eld image detect salient region however deep learning based light eld method still infancy many issue yet explored light field dataset remedy data ciency problem introduce scale light eld saliency dataset selected high quality sample captured lytro illum camera decode light eld format lytro desktop light eld consists focus focal stack focal slice focusing different depth depth corresponding manually labeled ground truth focal stack resembles human perception eye eye dynamically refocus different focal slice determine saliency show sample light eld proposed dataset observation bene cial cient foreground background separation annotation volunteer asked draw rectangle attractive object collect sample choosing image consensus manually label salient object focus commonly used segmentation tool supplying easy understand dataset hope promote research make sod problem accessible familiar eld proposed light eld saliency dataset provides unique focal slice used support training need deep neural network dataset consists indoor outdoor scene captured surrounding environment daily life ce supermarket campus street besides dataset contains many challenging scene shown similar foreground background complex background transparent object multiple object low intensity environment proposed network overall architecture adopt widely utilized vgg net backbone architecture drop last pooling layer fully connected layer reserve convolutional block better task overall architecture proposed network contains encoder memory oriented decoder shown encoder rgb fed stream generate raw rgb feature focal slice fed another stream generate light eld feature abundant spacial information simplicity illustrate single encoder represents stream simultaneously suggested conv block block might shallow make reliable prediction hereby perform decoder deeper layer block block speci cally given rgb focal slice size denote output last block represents feature generated rgb stream represents index focal slice represents last convolution block memory oriented spatial fusion module sfm raw rgb light eld feature generated encoder aim fusing available information address challenging problem light eld sod straightforward solution simply concatenate light eld feature produced different focal slice however drawback emerge approach first ignores relative contribution different focal slice nal result focal slice represent image focused different depth scene shown intuitively different focal slice different weight regarding salient object second direct concatenation operation seriously damage spatial correlation focal slice proper effective fusion strategy considered hence propose novel memory oriented spatial fusion module sfm address problem module introduce attention mechanism shown emphasize useful feature suppress unnecessary one focused blurred information procedure ned attm avgp ooling cid cid atti mean concatenation operation represent convolution operator convolution parameter layer avgp ooling mean global average pooling operation mean softmax function attm mean channel wise attention map weighted light eld feature cid layer cid denotes feature wise multiplication regarded sequence input corresponding consecutive time step fed convlstm structure gradually fimcellmo fimcellmo fimcellmo fimcellencoderthe illustration sfm convlstmcellscimthe illustration fim fimcellconvlstmcellscimmo fimcellpoolingsoftmax cid channel attentionscimmo sfm sfm sfm sfm block block block block block memory orienteddecoderpredictionrgbfocal stacksupervision cid cid cid cid cid cid abcdeelement wisemultiplicationelement wiseadditionfeature wisemultiplicationup icic icconvconvic cellcellcellcellgpmsupervision cid cid cid cid cid cid cic poolingconcatconv cid cid cid cid cid cid kllcsoftmaxgpmconcatenatedilated rate dilated rate dilated rate dilated rate cid cid cid cic cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid visual comparison ablation study mean rgb mean light eld data concatenation without weighting mean concatenation weighting mean convlstm fusion weighting represents gpm full sfm mean whole network without scim mean nal model spatial information accurately identifying salient object procedure ned whi wci whf wcf wxi cid wxf cid tanh wxc cid wxo cid tanh whc wco denotes hadamard product sigmoid function memory cell store earlier information model parameter learned gate memory cell hidden state tensor way step fused light eld feature effectively generated unique property light eld data make spontaneously suitable use convlstm feature fusion convlstm bene cial making better use spatial correlation multiple focal slice thanks powerful gate memory mechanism model enhances average mae performance nearly point proposed dataset lfsd dataset tab furthermore capture global contextual information different scale extend global perception module gpm top gpm ned conv cid convd denotes concatenation operation cid mean operation performed several time different dilation rate rate set denoted result returned parameter learned layer nal fused light eld feature multiple layer end add several intermediate supervision layer facilitate network convergence encourage explicit fusion light eld feature memory oriented feature integration module fim cient integration hierarchical deep feature signi cant pixel wise prediction task salient object detection semantic segmentation propose new memory oriented module novel perspective utilizes memory mechanism effectively tegrate multi level light eld feature top manner speci cally channel feature map con sidered feature detector design scene context integration table quantitative result ablation analysis network meaning index explained caption index module rgb weighting weighting sfm gpm sfm gpm fim scim fim scim lfsd hfut cid dut cid module scim shown utilizes memory information toper layer learn channel attention map update current light eld feature focusing important channel suppressing unnecessary one convlstm progressively integrates high level memory current elaborately ned input say high level feature abundant semantic information gradually summarized memory used guide selection low level spatial detail precise saliency prediction speci cally scim shown represents previous scene understanding hidden state convlstm time step mean fused light eld feature mth layer scim ned cid avgp ooling denote element wise addition multiplication respectively updated feature cid fed convlstm cell summarize spatial information historical memory current input cid use output block initial state convlstm scim step corresponding cid cid cid cid respectively output ned cid convlstm followed transition convolutional layer operation get nal saliency map calculation procedure similar equ replacing input however top structure may cause high level feature diluted trans mitted lower layer address problem inspired densenet link feature low high level way shown alleviate gradient vanish ing meanwhile encourage feature reuse nal light eld feature used set successively besides order guarantee time step convlstm explicitly learn important formation accurately identifying salient object add intermediate supervision internal output convlstm generally speaking intermediate supervision act instruction guide scim convlstm accurately lter non salient area retain salient area intermediate result illustrated full detail code made publicly available visual result intermediate supervi sion complex scene model gradu ally optimize saliency map produce precise prediction experiment datasets evaluate performance proposed network conduct experiment proposed dataset public light eld saliency datasets lfsd hfut dataset consists light eld sample randomly select sample training remaining sample testing detail found sec lfsd dataset contains light eld captured lytro camera dataset proposed pioneered use light eld solving challenging problem sod hfut hfut consists sample captured lytro camera challenging dataset real life scenario various distance sensor noise lighting condition sample lfsd hfut used testing evaluate generalization ability saliency model avoid tting augment training set ipping cropping rotating experiment setup evaluation metric adopt metric comprehensive evaluation including precision recall curve measure mean absolute error mae measure measure fimcellmo fimcellmo fimcellmo fimcellmo sfmmo sfmmo sfmmo sfmdecoderimagegt cid cid cid illustration baseline network rgb light eld data input correspond tab respectively term light eld input use concatenation without weighting strategy fuse light eld feature different focal slice conv block fairness intermediate supervision proposed network curve proposed method cnns based method obviously consistently outstanding approach universally agreed standard evaluating sod model well explained many literature due limited space detailed description implementation detail network implemented pytorch framework trained gtx gpu training test image uniformly resized network trained end end manner momentum weight decay learning rate set respectively training phrase use softmax entropy loss network trained standard sgd converges epoch batch size backbone network rgb focal stack stream initialized corresponding pre trained vgg net parameter initialized gaussian kernel ablation study effectiveness light field data tab detection result baseline network illustrated rgb data light eld data respectively numerical result measured measure mae demonstrate network light eld data outperforms rgb data visual comparison aforementioned network respectively indicates light eld data improve prediction performance challenging circumstance moreover conduct experiment repeating rgb input frame time way model architecture identical version input data quantitative result term measure mae focal slice rgb respectively con rms effectiveness focusness information spatial fusion module effectiveness sfm give evidence effectiveness sfm compare baseline network adding sfm signi cant improvement visually observed shown numerically sfm reduces mae performance nearly datasets conduct investigation provide internal inspection sfm gradual improvement add feature weighting mechanism convlstm integrator gpm sfm shown consistent assertion different contribution spatial correlation different focal slice bene cial sod gpm proved able adaptively detect object different scale quantitative result tab numerically accumulative accuracy gain component effectiveness fim fim proposed higher cognitive processing visually show uence addition fim observe considerable gain reduce mae shown tab achieved logical since high level feature gradually summarized memory used guide selection low level spatial detail fim result removing scim fim may lead false positive suggests scim effectively update original input according memory oriented scene understanding may greatly bias result decoder cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid inputoutput conv skip connectionskip connectionskip connectionrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecallrecallrecallprecisionprecisionprecisionourshfutlfsd table quantitative comparison light eld datasets best result shown boldface red green font respectively mean non deep learning mean available result type method year lf tpami mca tomm wsc cvpr dilf ijcai tip tanet mmci cvpr pca arxiv pdnet tcyb ctmf tip cdcp iccvw acsd icip nlpr eccv picanet cvpr cvpr pagrn eccv net ijcai iccv amulet iccv ucf cvpr nldf ds cvpr dhs cvpr mst cvpr bsca cvpr dsr iccv mae hfut mae lfsd mae limitation approach paper present deep learning based light eld saliency detection method deeply exploring comprehensively exploiting internal correlation focal slice demonstrate success method achieving state art datasets see work opening potential direction future study rst building big versatile dataset training validating different model present dataset training model testing model could bigger improving generalization ability model training direction developing computation cient memory cient method focal stack employed training process present rst deep learning based method light eld saliency detection lightweight model could potentially bene light eld data comparison state art compare result method one containing deep learning based method non deep learning one remarked light eld method lf mca wsc dilf rgb method tanet mmci pca pdnet ctmf cdcp acsd nlpr top ranking rgb method picanet pagr net amulet ucf nldf ds dhs mst bsca dsr fair comparison result competing method generated authorized code directly provided author quantitative evaluation quantitative result shown tab proposed model consistently achieves highest score datasets across evaluation metric important observation noted compared latest cnns based rgb sod method quantity training set method achieves signi cant advantage relatively small training set indicates light eld data signi cant promising show curve method outperform top ranking approach qualitative evaluation show selected representative sample result comparing method current state art method method able handle wide rage challenging scene including shown small object row similar foreground background row clutter background row dif cult scene row complex case see predicted result positively uenced light eld data proposed network light eld feature different focal slice effectively fused multi level global semantic information local detail cue suf ciently integrated visual comparison method top ranking cnns based method chal lenging case obviously model generate precise salient result even complex scene indicates method take full advantage light eld accurate saliency prediction conclusion paper develop novel memory oriented decoder tailored light eld saliency detection sfm resembles memory mechanism human fuse information effectively excavates various contribution spatial correlation different focal slice fim suf ciently integrates multi level feature leveraging high level memory guide low level selection additionally introduce scale light eld saliency dataset pave way future study experiment method achieves superior performance method including one especially complex scenario acknowledgement work supported national natural science foundation china fundamental research fund central university dut author grateful reviewer suggestion improving quality paper reference achanta hemami estrada sstrunk frequency tuned salient region detection conference computer vision pattern recognition cvpr page borji cheng jiang salient object detection survey arxiv preprint arxiv borji cheng jiang salient object detection benchmark ieee transaction processing tip borji sihite itti salient object detection benchmark european conference computer vision eccv page chen progressively complementarity aware fusion network rgb salient object detection conference computer vision pattern recognition cvpr page chen stream attention aware network rgb salient object detection ieee transaction processing tip chen multi modal fusion network multi scale multi path cross modal interaction rgb salient object detection pattern recognition cheng zhang mitra huang global contrast based salient region detection ieee transaction pattern analysis machine intelligence tpami volume page craye filliat goudou environment exploration object based visual saliency learning ieee international conference robotics automation icra page dai sun fcn object detection via region based fully convolutional network international conference neural information processing system nip page deng zhu qin han heng net recurrent residual nement network saliency detection international joint conference arti cial intelligence ijcai page ourstanetpcammcipagrnr netc simagegtpicanet lfsd pdnetamuletdssucfctmfnldf fan cheng liu borji structure measure new way evaluate foreground map international conference computer vision iccv page fan gong cao ren cheng borji enhanced alignment measure binary foreground map evaluation international joint conference arti cial intelligence ijcai page feng barnes mccarthy local background enclosure rgb salient object detection conference computer vision pattern recognition cvpr page gao mahadevan vasconcelos discriminant centersurround hypothesis bottom saliency international conference neural information processing system nip guo chen yang deep depth inference binocular monocular cue arxiv preprint arxiv han chen liu yan cnns based rgb saliency detection via cross view transfer multiview fusion ieee transaction system man cybernetics harel koch perona graph based visual saliency international conference neural information processing system nip page hariharan arbel girshick malik hypercolumns object segmentation grained localization conference computer vision pattern recognition cvpr page hong kwak han online tracking learning discriminative saliency map convolutional neural network international conference machine learning icml page hou cheng borji torr deeply supervised salient object detection short connection conference computer vision pattern recognition cvpr volume page huang liu van der maaten weinberger densely connected convolutional network conference computer vision pattern recognition cvpr page itti koch niebur model saliency based visual attention rapid scene analysis ieee transaction pattern analysis machine intelligence tpami jiang ling peng salient region detection ufo uniqueness focusness objectness international conference computer vision iccv page geng ren depth saliency based anisotropic center surround difference international conference processing icip page lee tai kim deep saliency encoded low level distance map high level feature conference computer vision pattern recognition cvpr page visual saliency based multiscale deep feature conference computer vision pattern recognition cvpr page ling saliency detection light eld conference computer vision pattern recognition cvpr page ling saliency detection light eld ieee transaction pattern analysis machine intelligence tpami zhang ruan yang saliency detection via dense sparse reconstruction international conference computer vision iccv page yang cheng liu shen contour knowledge transfer salient object detection european conference computer vision eccv page zhao wei yang zhuang ling wang deepsaliency multi task deep neural network model salient object detection ieee transaction processing tip hou koch rehg yuille secret salient object segmentation conference computer vision pattern recognition cvpr page lin milan shen reid nenet multi path nement network high resolution conference computer vision pattern recognition cvpr page semantic segmentation liu han dhsnet deep hierarchical saliency network salient object detection conference computer vision pattern recognition cvpr page liu han yang picanet learning pixel wise contextual attention saliency detection conference computer vision pattern recognition cvpr page luo mishra achkar eichel jodoin non local deep feature salient object detection conference computer vision pattern recognition cvpr page niu geng liu leveraging stereopsis saliency analysis conference computer vision pattern recognition cvpr page peng xiong rgbd salient object detection benchmark algorithm european conference computer vision eccv page perazzi henb pritch hornung saliency lters contrast based ltering salient region detection conference computer vision pattern recognition cvpr page qin wang saliency detection via cellular automaton conference computer vision pattern recognition cvpr page zhang tian tang yang rgbd salient object detection via deep fusion ieee transaction processing tip ren girshick sun faster cnn towards real time object detection region proposal network international conference neural information processing system nip volume page shao brady speci object retrieval based salient region pattern recognition shi chen wang yeung wong woo convolutional lstm network machine learning approach precipitation nowcasting international conference neural information processing system nip page simonyan zisserman deep convolutional network scale recognition international conference learning representation iclr smeulders chu cucchiara calderara dehghan shah visual tracking experimental survey ieee transaction pattern analysis machine intelligence tpami song lee depth estimation network dual defocused image different depth eld international conference processing icip page yang chien real time salient object detection minimum spanning tree conference computer vision pattern recognition cvpr page wang ruan yang deep network saliency detection via local estimation global search conference computer vision pattern recognition cvpr page wang zhu hiroaki chandraker efros ramamoorthi light eld dataset cnn architecture material recognition european conference computer vision eccv page wang lai shen ling salient object detection deep learning era depth survey arxiv preprint arxiv woo park lee kweon cbam convolutional block attention module european conference computer vision eccv page xie holistically nested edge detection international journal computer vision ijcv yeung hou chen chen chen chung light eld spatial super resolution deep cient spatial angular separable convolution ieee transaction processing tip sun weighted sparse coding framework saliency detection conference computer vision pattern recognition cvpr page zeiler fergus visualizing understanding convolutional network european conference computer vision eccv page zhang wang gao wang zhang saliency detection deeper investigation light eld international conference arti cial intelligence ijcai page zhang wang lin yang gao rui saliency detection light eld multi cue approach acm transaction multimedia computing communication application tomm zhang wang wang ruan amulet aggregating multi level convolutional feature salient object detection international conference computer vision iccv page zhang wang wang yin learning uncertain convolutional feature accurate saliency detection international conference computer vision iccv page zhang wang wang progressive attention guided recurrent network salient object detection conference computer vision pattern recognition cvpr page zhao ouyang wang saliency detection multi context deep learning conference computer vision pattern recognition cvpr page zhou liang zhang lumsdaine lin scale orientation aware epi patch learning light eld depth estimation international conference pattern recognition icpr page zhu cai huang pdnet prior model guided depth enhanced network salient object detection arxiv preprint arxiv zhu wang wang innovative salient object detection center dark channel prior international conference computer vision workshop iccvw page zhu guo wang roble kelly breaking spatio angular trade light eld super resolution via lstm modelling epipolar plane image arxiv preprint arxiv zhu liang wei sun saliency optimization robust background detection conference computer vision pattern recognition cvpr page', 'loaded dice trading bias variance order score function estimator reinforcement learning gregory farquhar university oxford shimon whiteson university oxford jakob foerster facebook research abstract gradient based method optimisation objective stochastic setting unknown intractable dynamic require estimator derivative derive objective automatic differentiation produce low variance unbiased estimator derivative order objective compatible arbitrary advantage estimator allows control bias variance order derivative function approximation furthermore propose method trade bias variance higher order derivative discounting impact distant causal dependency demonstrate correctness utility objective analytically tractable mdps meta reinforcement learning continuous control introduction stochastic setting reinforcement learning often impossible compute derivative objective depend unknown intractable distribution transition function environment case gradient based optimisation possible use stochastic gradient estimator great success domain found building estimator rst order derivative amenable automatic differentiation optimise parameter deep neural network fran ois lavet nonetheless number exciting application rst order derivative insuf cient meta learning multi agent learning often involve differentiating learning step gradient based learner finn stadie zintgraf foerster higher order optimisation method improve ciency furmston however estimating higher order derivative correctly low variance easily context automatic differentiation proven challenging foerster propose tool constructing estimator order derivative easy use avoid cumbersome manipulation otherwise required account dependency gradient estimate distribution sampled however formulation relies pure monte carlo estimate objective introducing unacceptable variance estimate rst higher order derivative limiting uptake method relying derivative meanwhile great stride made development estimator rst order derivative stochastic objective reinforcement learning use learned value function critic baseline extensively studied trade bias variance gradient estimator made explicit mixed objective combine monte carlo sample correspondence gregory farquhar conference neural information processing system neurips vancouver canada objective learned value function schulman technique create family advantage estimator used reduce variance accelerate credit assignment rst order optimisation applied full generality higher order derivative work derive objective differentiated number time produce correct estimator higher order derivative stochastic computation graph scgs markov property found sequence modeling unlike prior work objective fully compatible arbitrary choice advantage estimator approximate value function allows explicit trade offs bias variance order derivative estimate made known technique future advantage estimation method designed rst order derivative furthermore propose method trading bias variance higher order derivative discounting impact distant causal dependency empirically rst use small random mdps admit analytic solution estimator unbiased low variance perfect value function bias variance may exibly traded hyperparameters study objective challenging meta reinforcement learning problem simulated continuous control impact various parameter choice training demonstration code available http github com oxwhirl loaded dice handful additional line code needed implement objective existing codebase us higher order derivative background gradient estimator commonly faced objective form expectation random variable order calculate gradient expectation respect parameter interest must often employ gradient estimator gradient cannot computed exactly example reinforcement learning environment dynamic unknown form part objective expected return polyonymous likelihood ratio score function reinforce estimator given log expectation rh may estimated monte carlo sample drawn often independent second term dropped depends random variable may reparameterised depend deterministically may instead drop rst term see mohamed comprehensive review stochastic computation graph mdps stochastic computation graph scgs directed acyclic graph node determinsitic stochastic function edge indicate functional dependency schulman gradient estimator described may used estimate gradient objective sum cost node respect parameter schulman propose surrogate loss single objective produce desired gradient estimate differentiation weber apply advanced rst order gradient estimator scgs formalise markov property scgs allow exible powerful estimator originally developed context reinforcement learning applied describe estimator following subsection rst relevant subset scgs keep main body paper simple highlight important known use case method adopt notation reinforcement learning rather cumbersome notation generic scgs graph reinforcement learning describes markov decision process mdp begin initial state time timestep action sampled stochastic policy parameterised map state action add stochastic node graph state action pair lead reward next state process continues simple mdp graph shown gure many problem reward condition state rather state action consider episodic problem terminate step although result may extended non terminating case discounted reward cost node graph leading familiar reinforcement learning objective example scgs support new objective left right vanilla mdp mdp stochastic latent goal variable pomdp expected discounted sum reward cid trt expectation taken respect policy well unknown transition dynamic underlying mdp generalisation result hold slightly general class scgs well whose objective still sum reward time may number stochastic deterministic node corresponding timestep however node may uence future reward uence next timestep formally markov property state node exists directed path cid cid blocked none descendant nition weber class scgs capture broad class mdp like model gradient estimator advantage value function set node scg expectation objective stochas tic variable excluding set node reduce variance serving control variate baseline critic condition sampled value taken corresponding stochastic node sampled action difference critic baseline value function known advantage replaces sampled cost gradient estimator baseline value function affect variance gradient estimator weaver tao however learned imperfect critic value function result biased gradient estimator may trade bias variance different mixture sampled cost unbiased high variance learned critic value function biased low variance choice advantage estimator hyperparameters used tune bias variance resulting gradient estimator suit problem hand many way model advantage function popular simple family advantage estimator proposed schulman agae cid cid cid cid cid cid cid cid parameter trade bias variance formed sampled reward unbiased high variance agae us next sampled reward relies heavily estimated value function reducing variance cost bias higher order estimator construct higher order gradient estimator may recursively apply technique treating gradient estimate objective new scg foerster note several shortcoming surrogate loss approach schulman higher order derivative surrogate loss cannot differentiated produce correct higher order estimator even estimate produced surrogate loss cannot treated objective new scg surrogate loss severs dependency sampled cost sampling distribution address foerster introduce dice single objective may differentiated repeatedly automatic differentiation produce unbiased estimator derivative order dice objective reinforcement learning given cid cid indicates set stochastic node action occurring timestep earlier special operator act set stochastic node always evaluates special behaviour differentiation log cid zero derivative operator effect automates likelihood ratio trick differentiation expectation maintaining dependency trick applied computing higher order derivative notational convenience later derivation extend nition slightly ning operation empty set original version dice critical drawback compared state art method described estimating rst order derivative stochastic objective first mecha nism baseline reduce variance estimator higher order derivative mao liu subsequently independently suggest partial solution problem neither provide proof unbiasedness estimator beyond second order second dice estimator mao liu formulated way requires use monte carlo sampled cost without form permit use critic value function way make use full range possible advantage estimator exact calculation higher order derivative estimator dependence given reward previous action lead nested sum previous timesteps term tend high variance estimated data become small vicinity local optimum noted furmston rothfuss use observation propose simpli version dice objective dropping dependency jlv estimator biased higher rst order derivative rothfuss derive correct unbiased estimator order make use advantage estimation objective extend applicability beyond meta learning style maml finn next section introduce new objective may make use critic well baseline value function thereby allows bias variance order derivative traded choice advantage estimator furthermore introduce discounting past dependency allows smooth trade bias variance due high variance term identi furmston method dice objective cast sum reward dependency reward node stochastic cause captured use critic value function hand must use forward looking sum return possible graph maintains markov property ned section respect objective permit sequential decomposition cost node reward stochastic cause uenced action begin dice objective discounted sum reward given true objective expected discounted sum reward trajectory drawn policy cid cid trt cid simply take change variable cid second term relabeling dummy variable immediately back cid cid typical return cid cid cid cid cid cid cid cid cid cid cid last line used objective formulated term forward looking return capture depen since expression dencies sampling distribution dice objective applied restricted class scgs requisite markov property still guaranteed derivative unbiased estimator derivative true objective order proof original dice objective given foerster carry derivative omit following estimator clarity including however ensures convenient property objective still evaluates expectation true return introduce value function conditionally independent well derivative conditioned markov property scg equivalent conditional independence given consider expectation new form use conditional independence push expectation onto complete derivation please see supplementary material simply critic value function ned always evaluates zero cid cid cid cid cid cid cid cid cid cid furthermore baseline depend change expectation estimator shown standard derivation reproduced schulman reinforcement learning common use expected state value eat approximation optimal baseline estimator may use place reducing variance derived estimator term advantage recovers unbiased estimate derivative order cid cid cid practice common omit thus optimising undiscounted return still discounted advantage variance reduction tool see discussion thomas function approximation practice estimate advantage must made limited data inexact model critic value function due limited data model class misspeci cation inef cient learning introduce bias gradient estimator work schulman may use combination sampled cost estimated value form advantage estimator trade bias variance however thanks new estimator capture full dependency advantage sampling distribution trade offs may immediately applied higher order derivative approximate baseline value function affect estimator variance careful choice baseline may nonetheless great signi cance exploiting factorisation policy foerster formulation objective extends method well future advance advantage estimation rst order higher order derivative variance due higher order dependency correct form unbiased estimator us proper variance reduction strategy computing advantage may trade bias variance estimate higher order derivative arises due full history causal dependency particular propose set discount factor prior dependency limit horizon past action accounted estimate higher order derivative similarly way mdp discount factor reduces variance constraining horizon future must considered constrains far past consider causal dependency uence higher order derivative first note acting set stochastic node decomposes product implement discounting exponentially decaying past contribution cid cid cid cid cid cid cid cid cid cid cid cid may computed nal objective call loaded dice product log space action probability transforming convenient numerically stable sum algorithm show objective may easily computed episode algorithm compute loaded dice objective require trajectory state action log log deps deps end return cid accumulates nal objective cid accumulates weighted stochastic dependency cid dependency including cid dependency excluding operator log probability cid dependency weighted advantage cid applies function end function return exp stop gradient estimator resembles jlv although make use advantage may low variance biased regardless choice advantage estimator recover estimator unbiased advantage estimator unbiased intermediate value able trade bias variance demonstrate empirically section new form objective allows use reduce impact high variance term identi furmston rothfuss smooth way rather completely dropping term convergence increasing batch size unbiased order estimator dice dice baseline mao loaded dice lvc rothfuss low variance biased estimator experiment section empirically demonstrate correctness estimator absence function approximation bias variance estimator may traded choice advantage estimator approximate value function available use novel discount factor bias variance order derivative make initial analysis simple interpretable use small random mdps state action per state reward depend state mdps discounted value may calculated analytically follows state transition matrix induced mdp transition function cid tabular policy element given cid cid let initial state distribution vector probability distribution state time vector pst mean reward time pst vector per state reward finally cid cid cid trt differentiable wrt may easily computed automatic differentiation package detail code found supplementary material low variance unbiased order estimator show correlation estimated true derivative change function batch size third order compare original dice estimator loaded dice objective proposed mao incorporates baseline loaded dice use agae exact value function remain unbiased unbiased estimator converge true derivative suf ciently batch size however advantage estimator exact value function variance may dramatically reduced estimate converge much rapidly performance lvc rothfuss rst order match exactly estimator mao underperforms loaded dice use advantage higher order low variance biased expected low produce low variance estimate cost high bias effect hold order deriva tives high considers full past produce low bias high variance estimator low discount past first order gradient unaffected trading bias variance small mdp trading bias variance advantage estimation show bias standard deviation estimated derivative range inexact value function perturb true value function gaussian noise state emulate function approximation effect choice advantage estimator trade bias variance rst order order derivative trading bias variance discounting cause show bias standard deviation estimated derivative range isolate effect use exact value function absolute bias variance lower gure first order derivative unaffected expected however higher order derivative strongly affect bias variance resulting estimator outlier third order derivative guarantee monotonicity bias variance found outlier rarer second third order appearing artefact particular mdps meta reinforcement learning maml loaded dice apply new family estimator pair challenging meta reinforcement learning problem continuous control following work finn aim model agnostic meta learning maml learn good initialisation neural network policy single small number policy gradient update batch training episode policy achieves good performance task sampled distribution meta testing policy able adapt new task distribution maml theoretically sound original implementation neglected higher order dependency induced setting rothfuss stadie approach number task adapt policy inner loop policy gradient optimisation step outer loop initial parameter updated maximise return post adaptation policy outer loop optimisation depends post adaptation parameter depend gradient estimated inner loop important higher order term outer loop optimisation correct estimator inner loop optimisation therefore impact ciency overall meta training procedure well quality nal solution inner loop optimisation use novel objective range value sweep range xed sweep range best value found outer loop optimisation use vanilla policy gradient baseline outer loop could use gradient based policy optimisation algorithm choose simple version isolate extent impact inner loop estimator show result cheetahdir task high estimator high variance performance bad le impactful cheetahvel task note task episode short low value function simple linear function batch data finn factor would favor high higher variance return better value function relying heavily learned value function lower may effective trading bias variance meta reinforcement learning report mean standard error run post adaptation return smoothed moving average outer loop optimisation environment lead high variance unbiased version objective may valuable value function better used effectively mitigate variance cheetahvel noticeably faster learning achieved low non zero analysis furmston indicates magnitude higher order term discounted many case become small policy approach local optimum consistent empirical nding non zero may learn faster plateau similar level appendix show result antvel task important factor conclude loaded dice provides meaningful control higher order estimator signi cant impact realistic use case conclusion work derived theoretically sound objective apply general advantage function estimation order derivative reinforcement learning type sequential problem context function approximation objective unlocks ability trade bias variance higher order derivative importantly like underlying dice objective single objective generates estimator order derivative repeated automatic differentiation propose simple method discounting impact distant causal dependency estimation higher order derivative allows another axis trade bias variance empirically use small random mdps demonstrate behaviour bias variance higher order derivative estimate utility meta reinforcement learning excited application meta learning multi agent learning higher order optimi sation may made possible new objective future work wish revisit choice discounting heuristic method limit impact high variance term theoretical analysis may help identify context higher order dependency important optimisation finally may even possible meta learn hyperparameters acknowledgment thank maruan shedivat minqi jiang valuable discussion work supported epsrc cdt autonomous intelligent machine system project received funding european research council erc european union horizon research innovation programme grant agreement number reference chelsea finn pieter abbeel sergey levine model agnostic meta learning fast adaptation deep network proceeding international conference machine learning volume page jmlr org jakob foerster richard chen maruan shedivat shimon whiteson pieter abbeel igor mordatch proceeding international conference learning opponent learning awareness autonomous agent multiagent system page international foundation autonomous agent multiagent system jakob foerster gregory farquhar maruan shedivat tim rockt schel eric xing shimon whiteson dice nitely differentiable monte carlo estimator international conference machine learning page jakob foerster gregory farquhar triantafyllos afouras nantas nardelli shimon whiteson counterfac tual multi agent policy gradient thirty second aaai conference arti cial intelligence vincent fran ois lavet peter henderson riashat islam marc bellemare joelle pineau introduction deep reinforcement learning foundation trend cid machine learning michael gradient estimation handbook operation research management science thomas furmston guy lever david barber approximate newton method policy search markov decision process journal machine learning research hao liu richard socher caiming xiong taming maml cient unbiased meta reinforcement learning international conference machine learning page jingkai mao jakob foerster tim rockt schel maruan shedivat gregory farquhar shimon whiteson baseline order gradient estimation stochastic computation graph international conference machine learning page shakir mohamed mihaela rosca michael figurnov andriy mnih monte carlo gradient estimation machine learning arxiv preprint arxiv jonas rothfuss dennis lee ignasi clavera tamim asfour pieter abbeel promp proximal meta policy search arxiv preprint arxiv john schulman nicolas heess theophane weber pieter abbeel gradient estimation stochastic computation graph advance neural information processing system page john schulman philipp moritz sergey levine michael jordan pieter abbeel high dimensional continuous control generalized advantage estimation arxiv preprint arxiv bradly stadie yang rein houthooft chen yan duan yuhuai pieter abbeel ilya sutskever consideration learning explore via meta reinforcement learning arxiv preprint arxiv philip thomas bias natural actor critic algorithm international conference machine learning page lex weaver nigel tao optimal reward baseline gradient based reinforcement learning proceeding seventeenth conference uncertainty arti cial intelligence page morgan kaufmann publisher inc ophane weber nicolas heess lars buesing david silver credit assignment technique stochastic computation graph arxiv preprint arxiv luisa zintgraf kyriacos shiarlis vitaly kurin katja hofmann shimon whiteson caml fast context adaptation via meta learning international conference machine learning derivation value function formulation start cid objective cid cid cid cid evaluate objective taking expectation trajectory induced policy complete sequence state action reward convenience following derivation ned reward indexed next time step action taken ensures partial trajectory correctly keep reward action cause note krt depends expectation objective given cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid tjt cid note time step term form cid cid next use last step used markov property substituting obtain substitute back obtain cid cid cid cid cid cid cid cid cid cid cid cid cid putting together obtain nal form cid cid cid cid cid cid experimental detail random mdps use mdptoolbox example rand function pymdptoolbox generate random mdp transition function state action per state reward function state sampled use sampling stochastic estimator use batch rollouts length step unless batch size otherwise speci compute higher order derivative derivative rst parameter order save computa tion sweep use batch value simulate function approximation error analysis impact add gaussian noise standard deviation true value function maml experiment use following hyperparameters maml experiment parameter hidden layer size number layer task batch size meta batch size inner loop learning rate outer loop optimiser outer loop learning rate outer loop reward noise value trajectory task adam uniform timestep normalise advantage batch per task show additional experiment antvel mujoco task domain important factor trading bias variance ant velocity task report mean standard error run post adaptation return smoothed moving average outer loop optimisation', 'cortical microcircuit gated recurrent neural network rui ponte costa yannis assael centre neural circuit behaviour dept computer science dept physiology anatomy genetics university oxford oxford university oxford oxford rui costa cncb deepmind london yannis assael brendan shillingford dept computer science university oxford oxford deepmind london brendan shillingford nando freitas deepmind london nandodefreitas google com tim vogels centre neural circuit behaviour dept physiology anatomy genetics university oxford oxford tim vogels cncb abstract cortical circuit exhibit intricate recurrent architecture remarkably similar across different brain area stereotyped structure suggests existence common computational principle however principle remained largely elusive inspired gated memory network namely long short term memory network lstms introduce recurrent neural network information gated inhibitory cell subtractive sublstm propose natural mapping sublstms onto known canonical excitatory inhibitory cortical microcircuit empirical evaluation across sequential classi cation language modelling task show sublstm unit achieve similar performance lstm unit result suggest cortical circuit optimised solve complex contextual problem proposes novel view computational function overall work provides step towards unifying recurrent network used machine learning biological counterpart introduction last decade neuroscience research collected enormous amount data chitecture dynamic cortical circuit unveiling complex stereotypical structure across neocortex markram harris mrsic flogel jiang prevalent feature cortical net laminar organisation high degree recurrence even level local micro circuit douglas song harris mrsic flogel jiang another key feature cortical circuit detailed tight balance excitation inhibition received growing support author contributed equally work conference neural information processing system nip long beach usa experimental froemke xue froemke theoretical level van vreeswijk sompolinsky brunel vogels abbott hennequin however computational process facilitated architecture dynamic still elusive remains fundamental disconnect underlying biophysical network emergence intelligent complex behaviour arti cial recurrent neural network rnns hand crafted perform speci computation fact rnns recently proven successful solving complex task language modelling speech recognition perceptual task graf graf sutskever van den oord assael task input data contains information across multiple timescales need ltered processed according relevance ongoing presentation stimulus make dif cult learn separate meaningful stimulus background noise hochreiter pascanu rnns particular gated rnns solve problem maintaining representation relevant input sequence needed without interference new stimulus principle protected memory conserve past input thus allow back propagation error backwards time pascanu memory property rst successful type gated rnns named long short term memory network lstms hochreiter schmidhuber note architectural feature lstms overlap closely known cortical structure important difference regard mechanistic implementation gate cortical network lstms lstms gate control memory cell multiplicative factor biological network gate inhibitory neuron act rst approximation subtractively excitatory inhibitory current cancel linearly level postsynaptic membrane potential kandel gerstner moreover subtractive inhibitory mechanism must well balanced closely match excitatory input act gate input closed state without perturbing activity much inhibition previous model explored gating subtractive excitatory inhibitory balanced network vogels abbott kremkow without clear computational role hand predictive coding rnns feature studied bastos deneve machens without clear match state art machine learning network regarding previous neuroscienti interpretation lstms suggestion lstms model working memory different brain area prefrontal cortex basal ganglion hippocampus reilly frank krueger dayan cox dean marblestone hassabis bhalla without clear interpretation individual component lstms speci mapping known circuit propose map architecture function lstms directly onto cortical circuit gating provided lateral subtractive inhibition network potential exhibit excitation inhibition balance observed experiment douglas bastos harris mrsic flogel yield simpler gradient propagation multiplicative gating study dynamic empirical evaluation showing sublstms achieve similar performance lstms penn treebank wikitext language modelling task well pixelwise sequential mnist classi cation transferring functionality lstms biologically plausible network work provides testable hypothesis recently emerging technologically advanced experiment functionality entire cortical microcircuit biological motivation architecture lstm unit general feedforward structure aided additional recurrent memory controlled lateral gate remarkably similar columnar architecture cortical circuit see detailed neocortical schematic central element lstms similar rnns memory cell hypothesise implemented local recurrent network pyramidal cell layer line previous study showing relatively high level recurrence non random connectivity pyramidal cell layer douglas thomson song furthermore layer pyramidal network display rich activity relatively long time scale vivo barth sakata harris luczak van kerkoerle slice egorov wang consistent lstm like function strong evidence persistent neuronal activity higher cortical area goldman rakic sensory area huang van kerkoerle kornblith relatively speaking sensory area visual cortex exhibit sorter timescales higher brain area prefrontal cortex would expect given different temporal requirement brain area similar behaviour expected multi area layer lstms note longer time scale present super cial layer layer goldman rakic van kerkoerle suggesting possibility memory cell per cortical microcircuit slow memory decay network may controlled short york van rossum costa long term synaptic plasticity abbott nelson senn ster gerstner zenke costa recurrent excitatory synapsis gate protect given memory lstms mapped onto lateral inhibitory input cortical circuit propose similar lstms input gate implemented inhibitory neuron layer layer lateral inhibition consistent canonical view microcircuit douglas bastos harris mrsic flogel sparse sensory evoked response layer sakata harris harris mrsic flogel brain inhibition believed originate parvalbumin basket cell providing near exact balanced inhibitory counter signal given excitatory feedforward input froemke xue froemke excitatory inhibitory input thus cancel arriving signal ignored default consequently activity within downstream memory network remains largely unperturbed unless altered targeted modulation inhibitory activity harris mrsic flogel vogels abbott letzkus similarly memory cell affect output lstm activity unaccompanied congruent inhibition mapped onto layer layer layer microcircuit known project higher brain area harris mrsic flogel see lateral inhibition turned gate open subtractive neural integration presynaptic cell re neurotransmitter released synaptic terminal neurotrans mitter subsequently bound postsynaptic receptor prompt structural change ion channel allow electrically charged ion postsynaptic cell depending receptor type ion either increase depolarise decrease hyperpolarise postsynaptic membrane potential suf ciently depolarising excitatory input provided postsynaptic potential reach threshold stereotyped action potential spike kandel behaviour formalised circuit resistance capacitance follows ohm law yield standard leaky integrate neuron model gerstner kistler riexc riinh membrane time constant iexc iinh excitatory inhibitory hyperpolarizing synaptic input current respectively action potential initiated standard model brette gerstner gerstner membrane potential hit hard threshold modelled momentary pulse subsequent reset resting potential neuronal excitation inhibition opposite effect inhibitory input act linearly subtractively membrane potential leaky integrate model approximated level ring rate rate cid iexc iinh iexc iinh cid see input output response gerstner kistler used demonstrate impact subtractive gating contrast multi plicative gating ring rate approximation form basis gated rnn model similar subtrac tive behaviour input output function bottom moreover rate formulation allows cleaner comparison lstm unit use existing machine learning optimisation method could argued different form inhibition shunting inhibition counteracts excitatory input decreasing membrane resistance characteristic multiplicative gating effect membrane potential however analysed level output ring rate effect becomes subtractive holt koch prescott koninck consistent biological arti cial gated recurrent neural network example unit simpli cortical recurrent neural network sensory downstream input arrives pyramidal cell layer layer fed onto memory cell recurrently connected pyramidal cell layer memory decay decay time constant input onto layer balanced inhibitory basket cell balance represented diagonal equal connection output memory cell gated basket cell layer within area upstream brain area implementation following similar notation lstm unit input output subtractive gate dashed connection represent potential balance excitatory inhibitory input weight set lstm recurrent neural network cell see main text detail plot bellow illustrate different gating mode simple current based noisy leaky integrate neuron capped subtractive inhibition sigmoidal activation function subtractive gating sigmoidal activation function multiplicative gating output rate represents number spike per second biological circuit approach model framed ring rate level rather level membrane potential subtractive gated long short term memory lstm unit hochreiter schmidhuber greff access memory cell controlled input gate see time forget gate control decay memory output gate control whether content memory cell transmitted rest network lstm network consists many lstm unit containing memory cell input forget output gate lstm state described unit follows dynamic given middle column note leak controlled input recurrent unit may biologically unrealistic input output cellctxt itotztxt ftxt input output cellctxt itot ztfxt inputunit junit junit joutputl inpc inl pcsmemorycellfpclayer abcsublstmlstmcortical circuit output rate input baselineweak inh strong inh inh exc inh exc closed gatebaselineweak inh strong inh baselinestrong gatesubtractive gatingmultiplicative gatingweak gate lstm sublstm rht rht tanh rht rht tanh memory cell note multiplicative control input gate denotes element wise multiplication new weighted input given input vector recurrent input lstm unit respectively overall output lstm unit computed lstm network multiple layer million parameter weight bias typically trained stochastic gradient descent supervised setting parameter multiple gate allow network adapt information depending task hand particular enable writing memory cell controlled input gate adjusting timescale memory controlled forget gate exposing memory network controlled output gate combined effect gate make possible lstm unit capture temporal contextual dependency across multiple timescales introduce study new rnn unit sublstm sublstm unit mapping lstms onto known canonical excitatory inhibitory cortical microcircuit douglas song harris mrsic flogel similarly sublstms ned however gating subtractive rather multiplicative sublstm ned memory cell transformed input input gate model use simpli notion balance gating jth unit memory forgetting consider option controlled gate lstm unit rht biologically plausible learned simple decay referred result sublstm similarly input sublstm output gated subtractive output gate see equation evaluated different activation function sigmoidal transformation highest performance key difference gated rnns subtractive inhibitory gating potential balanced excitatory input respectively see detailed comparison different gating mode subtractive versus multiplicative gating rnns key difference sublstms lstms lie implementation gating mech anism lstms typically use multiplicative factor control amplitude input signal sublstms use biologically plausible interaction excitation inhibition important consequence subtractive gating potential improved gradient backwards towards input layer illustrate compare gradient sublstms lstms simple example first review derivative loss respect various component sublstm notation based greff notation represents derivative loss note consider version sublstms forget gate lstms sublstm another simple memory decay scalar ne memory timeconstant sublstm weight could optimised model decided keep number parameter minimum simplicity ease comparison lstms respect def dloss dht error layer chain rule comparison corresponding derivative lstm unit given tanh tanh tanh sigmoid activation function overlined variable etc pre activation value gate input transformation woxt roht output gate sublstm note compared lstm sublstms provide simpler gradient fewer multiplicative factor lstms weight input transformation updated according total number temporal step ellipsis abbreviates recurrent gradient path time containing path backwards time via simplicity analysis ignore recurrent connection lstm sublstm consider depth wise path network call tth timestep depth contribution derivative lstm slight abuse notation tanh input gate cid output gate tanh cid tanh derivative tanh notice either input output gate set zero closed corresponding contribution gradient zero network subtractive gating depth derivative contribution becomes cid cid sigmoid derivative case input output gate present subtractive gate sublstms directly impair error propagation result aim work fold first inspired cortical circuit aimed propose biological plausible implementation lstm unit would allow better understand cortical architecture dynamic compare performance sublstm unit lstms rst compared learning dynamic subtractive multiplicative network mathematically second step empirically compared sublstm sublstm lstm network task sequential mnist classi cation word level language modelling penn treebank marcus wikitext merity network weight initialised glorot initialisation glorot bengio lstm unit initial forget gate bias selected number unit sublstm number parameter held constant across experiment facilitate fair comparison lstms sublstms sequential mnist sequential mnist digit classi cation task digit mnist dataset presented rnn sequence pixel decompose mnist image pixel sequence step network optimised rmsprop momentum tieleman hinton learning rate hidden layer hidden unit result sublstms achieves similar result lstms result comparable previous result task rnns comparison lstm sublstm network sequential pixel pixel mnist hidden unit sample mnist dataset converted matrix pixel temporal sequence timesteps classi cation accuracy test set sublstm xed learned forget gate language modelling language modelling represents challenging task rnns short long term dependency rnn language model rnn lm model probability text autoregressively predicting sequence word timestep trained predict following word word model word sequence product conditional multinoulli distribution evaluate rnn lm measuring perplexity ned sequence word perplexity rst used penn treebank ptb dataset train model word level language modelling training validation test word vocabulary word rnns tested hidden layer backpropagation truncated step batch size optimise network used rmsprop momentum performed hyperparameter search validation set input output update dropout rate learning rate weight decay hyperparameter search done google vizier performs black box optimisation gaussian process bandit transfer learning table resulting hyperparameters table report perplexity test set golovin understand sublstms scale network size varied number hidden unit tested wikitext language modelling dataset based wikipedia article dataset twice ptb dataset training validation test word lstmsublstmfix sublstm testing accuracy seq feature larger vocabulary word therefore well suited evaluate model performance longer term dependency reduces likelihood tting datasets result sublstms achieve perplexity similar lstms table interestingly biological plausible version sublstm simple decay forget gate achieves performance similar better sublstms penn treebank ptb test perplexity wikitext test perplexity size sublstm sublstm lstm size sublstm sublstm lstm table language modelling word level test set perplexity penn treebank wikitext model layer sublstm us xed learned forget gate unit number unit sublstm chosen number parameter sub lstm facilitate fair comparison size indicates number unit number hidden unit sublstm selected number parameter lstm sublstm facilitating fair comparison conclusion future work cortical microcircuit exhibit complex stereotypical network architecture support rich dynamic computational power dynamic yet properly understood known excitatory inhibitory neuron type interact closely process sensory information great accuracy making sense interaction beyond scope contemporary experimental approach lstms hand well understood powerful tool contextual task structure map intriguingly well onto stereotyped connectivity cortical circuit analysed biologically constrained lstms sublstms could perform similarly well indeed model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table penn treebank hyperparameters model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table wikitext hyperparameters subtractively gated excitation inhibition recurrent neural network promise compared lstms benchmark sequence classi cation word level language modelling notable sublstms could outperform traditional counterpart yet hope work serve platform discus develop idea cortical function establish link relevant experimental work role excitatory inhibitory neuron contextual learning froemke froemke poort pakan kuchibhotla future work interesting study additional biological detail may affect performance next step aim include dale principle given neuron make either excitatory inhibitory connection stratum harvey naturally focus perplexing diversity inhibitory cell type markram behaviour shunting inhibition mixed subtractive divisive control doiron mejias boustani sur seybold overall given success multiplicative gated lstms insightful understand biological trick cortical network may give lstms performance boost acknowledgement would like thank everton agnes glar ehre gabor melis jake stroud helpful comment discussion supported sir henry dale fellowship wellcome trust royal society supported epsrc research council rcuk supported clarendon fund reference abbott nelson synaptic plasticity taming beast nature neuroscience assael shillingford whiteson freitas lipnet sentence level lipreading arxiv preprint arxiv barth curto luczak marguet harris population coding tone stimulus auditory cortex dynamic rate vector analysis european journal neuroscience bastos usrey adam mangun fry friston canonical microcircuit predictive coding neuron bhalla dendrite deep learning sequence hippocampus hippocampus although focus comparison lstms similar point would apply gated rnns gated recurrent unit chung brette gerstner adaptive exponential integrate model effective description neuronal activity journal neurophysiology brunel dynamic sparsely connected network excitatory inhibitory spiking neuron journal computational neuroscience chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling arxiv org costa froemke sjostrom van rossum uni pre postsynaptic long term plasticity enables reliable exible learning elife costa mizusaki sjostrom van rossum functional consequence pre postsynaptic expression synaptic plasticity philosophical transaction royal society london series biological science costa padamsey amour emptage froemke vogels synaptic transmission optimization predicts expression locus long term plasticity neuron costa sjostrom van rossum probabilistic inference short term synaptic plasticity neocortical microcircuit frontier computational neuroscience cox dean neural network neuroscience inspired computer vision current biology deneve machens cient code balanced network nature neuroscience doiron longtin berman maler subtractive divisive inhibition effect voltage dependent inhibitory conductance noise neural computation douglas koch mahowald martin suarez recurrent excitation neocortical circuit science douglas martin whitteridge canonical microcircuit neocortex neural computation egorov hamam frans hasselmo alonso graded persistent activity entorhinal cortex neuron nature boustani sur response dependent dynamic cell speci inhibition cortical network vivo nature communication froemke plasticity cortical excitatory inhibitory balance annual review neuroscience froemke merzenich schreiner synaptic memory trace cortical receptive eld plasticity nature gerstner kistler spiking neuron model single neuron population plasticity cambridge university press gerstner kistler naud paninski neuronal dynamic single neuron network model cognition cambridge university press glorot bengio understanding dif culty training deep feedforward neural network proceeding thirteenth international conference arti cial intelligence statistic page goldman rakic cellular basis working memory neuron golovin solnik moitra kochanski karro sculley google vizier service black box optimization proceeding acm sigkdd international conference knowledge discovery data mining page acm graf generating sequence recurrent neural network arxiv org graf mohamed hinton speech recognition deep recurrent neural network arxiv preprint arxiv greff srivastava koutn steunebrink schmidhuber lstm search space odyssey arxiv org harris mrsic flogel cortical connectivity sensory coding nature hassabis kumaran summer eld botvinick neuroscience inspired arti cial intelligence neuron hennequin agnes vogels inhibitory plasticity balance control codependence annual review neuroscience hennequin vogels gerstner optimal control transient dynamic balanced network support generation complex movement neuron hochreiter bengio frasconi schmidhuber gradient recurrent net dif culty learning long term dependency hochreiter schmidhuber long short term memory neural computation holt koch shunting inhibition divisive effect ring rate neural computation huang matysiak heil nig brosch king persistent neural activity auditory cortex related auditory working memory human nonhuman primate elife jiang shen cadwell berens sinz ecker patel tolias principle connectivity among morphologically ned cell type adult neocortex science aac aac kandel schwartz jessell siegelbaum principle neural science kornblith quiroga koch fried mormann persistent single neuron activity working memory human medial temporal lobe current biology kremkow aertsen kumar gating signal propagation spiking neural network balanced correlated excitation inhibition journal neuroscience krueger dayan flexible shaping learning small step help cognition kuchibhotla gill lindsay papadoyannis field sten miller froemke parallel processing cortical inhibition enables context dependent behavior nature neuroscience jaitly hinton simple way initialize recurrent network rectus linear unit arxiv org letzkus wolff thi disinhibition circuit mechanism associative learning memory neuron luczak mcnaughton harris packet based communication cortex nature review neuroscience marblestone wayne kording toward integration deep learning neuroscience frontier computational neuroscience marcus marcinkiewicz santorini building annotated corpus english penn treebank computational linguistics markram toledo rodriguez wang gupta silberberg interneurons neocortical inhibitory system nature review neuroscience mejias kappen longtin torres short term synaptic plasticity heterogeneity neural system merity xiong bradbury socher pointer sentinel mixture model arxiv org reilly frank making working memory work computational model learning prefrontal cortex basal ganglion neural computation pakan lowe dylda keemink currie coutts rochefort mrsic flogel behavioral state modulation inhibition context dependent cell type speci mouse visual cortex elife pascanu mikolov bengio dif culty training recurrent neural network arxiv org ster gerstner triplet spike model spike timing dependent plasticity journal neuroscience poort khan pachitariu nemri orsolic krupic bauza sahani keller mrsic flogel hofer learning enhances sensory multiple non sensory representation primary visual cortex neuron prescott koninck gain control ring rate shunting inhibition role synaptic noise dendritic saturation proc natl acad sci usa sakata harris laminar structure spontaneous sensory evoked population activity auditory cortex neuron senn markram tsodyks algorithm modifying neurotransmitter release probability based pre postsynaptic spike timing neural computation seybold phillips schreiner hasenstaub inhibitory action uni network integration neuron song str reigl nelson chklovskii highly nonrandom feature synaptic connectivity local cortical circuit plo biology stratum harvey dale principle brain research bulletin sutskever vinyals sequence sequence learning neural network arxiv org thomson west wang bannister synaptic connection small circuit involving excitatory inhibitory neuron layer adult rat cat neocortex triple intracellular recording biocytin labelling vitro cerebral cortex new york tieleman hinton lecture rmsprop divide gradient running average recent magnitude coursera neural network machine learning van den oord kalchbrenner vinyals espeholt graf kavukcuoglu conditional generation pixelcnn decoder arxiv org van kerkoerle self roelfsema layer speci city effect attention working memory activity primary visual cortex nature communication van vreeswijk sompolinsky chaos neuronal network balanced excitatory inhibitory activity science vogels abbott gating multiple signal detailed balance excitation inhibition spiking network nature neuroscience wang markram goodman berger goldman rakic heterogeneity pyramidal network medial prefrontal cortex nature publishing group xue atallah scanziani equalizing excitation inhibition ratio across visual cortical neuron nature york van rossum recurrent network short term synaptic depression journal computational neuroscience zenke agnes gerstner diverse synaptic plasticity mechanism orchestrated form retrieve memory spiking neural network nature communication', 'unsupervised learning view invariant action representation grad school integrative science engineering national university singapore national university singapore yongkang wong school computing singapore lijunnan nu edu yongkang wong nu edu junnan singapore zhao dept computer science engineering university minnesota minneapolis usa qzhao umn edu mohan kankanhalli school computing national university singapore singapore mohan comp nu edu abstract recent success human action recognition deep learning method mostly adopt supervised learning paradigm requires signi cant amount man ually labeled data achieve good performance however label collection expensive time consuming process work propose unsupervised learning framework exploit unlabeled data learn video representation different previous work video representation learning unsupervised learning task predict motion multiple target view video repre sentation source view learning extrapolate cross view motion representation capture view invariant motion dynamic discriminative action addition propose view adversarial training method enhance learning view invariant feature demonstrate effectiveness learned representation action recognition multiple datasets introduction recognizing human action video long standing research problem computer vision past year convolutional neural network cnns recurrent neural network rnns emerged state art learning framework action recognition however success existing supervised learning method primarily driven signi cant amount manually labeled data expensive time consuming collect tackle problem stream unsupervised method recently proposed leverage free unlabeled data representation learning key idea design surrogate task exploit inherent structure raw video formulate loss function train network work design surrogate task constructing future frame future motion others use temporal order video frame learn representation self supervised manner although promising result learned representation often view speci make le robust view change generally human action observed multiple view action appears quite different therefore important learn discriminative view invariant feature especially action recognition unknown unseen view human ability visualize conference neural information processing system neurips montr canada action look like different view human brain build view invariant action representation immediately hypothesize enabling deep network ability extrapolate action across different view encourage learn view invariant representation work propose unsupervised learning framework task construct motion multiple target view video representation source view argue order network infer cross view motion dynamic learned representation reside view invariant discriminative space action recognition view invariant representation learning cross view action recognition widely studied however existing method require access human pose information training others compromise discriminative power achieve view invariance focus inferring motion rather tracking body keypoints space time method learns recurrent encoder extract motion dynamic insensitive viewpoint change represent motion calculated rgb data contribution work follows propose unsupervised framework effectively learn view invariant video representation predict motion sequence multiple view learned representation extracted cnn rnn based encoder decoded multiple sequence ows cnn decoder framework trained jointly minimizing several loss propose view adversarial training encourage view invariant feature learning video different view mapped shared subspace view classi cannot discriminate shared representation enforced contain meaningful motion information use decoder demonstrate effectiveness learned representation cross subject cross view action recognition task experiment various input modality including rgb depth method outperforms state art unsupervised method across multiple datasets related work unsupervised representation learning deep network shown dominant performance various computer vision task fully supervised training paradigm requires vast amount human labeled data inherent limitation highlight importance unsupervised learning leverage unlabeled data learn feature representation past year unsupervised learning method extensively studied deep learning method deep boltzmann machine auto encoders unsupervised representation learning proven useful several supervised task pedestrian detection object detection classi cation video domain line recent work unsupervised representation learning rst line work exploit temporal structure video learn visual representation sequence veri cation sequence sorting task second line work based frame reconstruction ranzato proposed rnn model predict missing frame future frame input video sequence srivastava extended framework lstm encoder decoder model reconstruct input sequence predict future sequence representation learning mostly capture semantic feature luo proposed unsupervised learning framework predicts future motion pair consecutive frame learned representation promising result supervised action recognition however previous work often learn view speci representation sensitive viewpoint change action recognition rgb action recognition action recognition rgb video long standing problem detailed survey found recent approach shown great progress eld generally divided category rst category focus designing handcrafted feature video representation successful example improved dense trajectory feature combined fisher vector encoding second category us deep network jointly learn feature representation classi simonyan zisserman proposed stream proposed unsupervised representation learning framework sequence input frame encoder generates sequence feature representation timestep representation used cross view decoder reconstruction decoder view classi multiple loss term jointly minimized encoder learn generate view invariant representation capture motion dynamic cnns extract spatial motion representation video frame optical ows rnn based architecture proposed model temporal information however deep network training requires amount human labeled data cnns pre trained imagenet commonly adopted backbone facilitate training avoid tting rgb action recognition since rst work action recognition depth map researcher proposed method action recognition extract feature multi modal data including depth rgb skeleton recently wang used scene calculated rgb data input action recognition state art method rgb action recognition report human level performance well established datasets msr dailyactivity however show big performance gap human existing method challenging ntu rgbd dataset contains signi cantly subject viewpoint action class background view invariant feature representation particularly challenging aspect action recognition recognize action varied unknown unseen view referred cross view action recognition literature performance existing method drop sharply viewpoint change due inherent view dependence feature used method tackle problem researcher proposed method learn representation invariant viewpoint change method create spatial temporal representation insensitive view variation method view independent latent space feature extracted different view directly comparable example rahmani used deep network project dense trajectory feature different view canonical view however previous method require access human pose information mocap data skeleton training others limited discriminative power moreover existing method skeleton based method shown effective performance cross view evaluation ntu rgbd dataset method goal unsupervised learning method learn video representation capture view invariant motion dynamic achieve training model us representation predict sequence motion multiple view motion represented dense scene ows calculated primal dual method rgb data learned representation used discriminative motion feature action recognition section rst present unsupervised learning framework followed action recognition method convbilstm convbilstm convbilstm cnndeconv encodercross view decoderdeconv reconstruction decoder view classifiergrl learning framework lxview lrecon lcls size work set overview learning framework illustrated end end deep network consists component encoder cross view decoder reconstruction decoder view classi parameterized respectively goal minimize following loss weight balance interaction loss term lxview cross view prediction loss lrecon reconstruction loss lcls view classi cation loss applied adversarial setting enhance view invariance loss term involves encoder component next explain component detail encoder let denote available view captured action encoder parameter ized take input sequence frame view denoted encodes sequence low dimensionality feature embeddings speci cally frame rst use downsampling cnn denoted conv extract low dimensionality feature size directional convolutional lstm denoted bilstm run sequence extracted conv feature timestep bilstm generates feature map size forward pas backward pas feature map concatenated channel wise form encoding compared vanilla lstms convolutional lstms replace fully connected transforma tions spatial convolution preserve spatial information intermediate representation perform much better vanilla lstms moreover directional lstm aggregate information previous frame future frame help generate richer representa tions compared lstm encoder encodes frame proposed encoder generate encoding longer sequence embodies discriminative motion dynamic action work set sequence length cross view decoder goal cross view decoder predict view cid given encoding dif cult decoder zero information view therefore give additional input decoder contains view speci information input depth map view timestep serf anchor inform decoder spatial con guration order predict view decoder still requires view invariant motion dynamic speci cally rst use cnn extract feature size extracted channel wise feature size use feature concatenated upsampling cnn denoted deconv perform spatial upsampling deconv consists fractionally strided convolutional layer batch normalization layer relu activation observe batch normalization critical optimize network let denote output cross view decoder timestep want minimize mean squared error view timestep inferring directly xview cid cid cid cid cid cid cid sequence anchor depth frame sequence ows since want learn video representation used predict motion multiple view deploy multiple cross view decoder shared parameter view therefore cross view prediction loss cid lxview xview cid reconstruction decoder goal decoder reconstruct given encoding view learning reconstruction help encoder extract basic motion used together cross view decoder enhances learning view invariant motion dynamic architecture reconstruction decoder deconv module similar cross view decoder number input channel rst layer adapted let output reconstruction decoder timestep reconstruction loss lrecon cid cid cid cid cid cid cid view classi propose view adversarial training encourages encoder learn video representation invariant view change draw inspiration domain adversarial training aim learning feature indiscriminate respect shift domain proposed view adversarial training achieved adding view classi connected encoder gradient reversal layer grl view classi try predict view encoded representation belongs whereas encoder try confuse view classi generating view invariant representation map encoding timestep probability formally view classi distribution possible view learning grl adversarial optimized increase ability discriminate encoding different view grl revers sign gradient ows back result encoder parameter learning representation reduces view classi cation accuracy essentially minimize cross entropy loss view classi cation task respect maximize respect therefore view classi cation loss sum cross entropy loss entire sequence cid log cid cid lcls ground truth view input view classi consists fully connected layer softmax layer since encoding convolutional feature rst attened vector go view classi action recognition use encoder unsupervised learning action recognition given learned representa tions sequence frame apply action classi action classi simple fully connected layer take attened vector input output score possible action class nal score sequence average score timestep action classi trained cross entropy loss training consider scenario cid scratch randomly initialize weight encoder train entire model scratch tune initialize encoder learned weight tune action recognition keep pre trained encoder xed train action classi test time uniformly sequence video sequence length average score across sampled sequence get class score video experiment unsupervised representation learning implementation detail conv encoder depth cnn cross view decoder employ resnet architecture nal convolution layer add convolutional layer reduce feature size number input channel rst convolutional layer adapted according input modality note cnn pre trained imagenet bilstm use convolutional lters size convolution input hidden state initialize weight following method training use mini batch size train model adam optimizer initial learning rate weight decay decrease learning rate half every step mini batch avoid table cross view prediction error ntu rgb dataset method proposed method lrecon lcls proposed method lcls proposed method cross subject rgb depth flow cross view depth flow rgb walking towards sitting example sequence depth input ows visualized rgb image upper row ground truth ows lower row predicted ows blue box denotes source view reconstruction whereas red box denotes cross view prediction model estimate raw motion multiple view distracting prediction task activate view adversarial training step weight loss term set determined via cross validation order effectively predict motion want describe motion low dimensional signal hence apply spatial downsampling ows calculating mean non overlapping patch resulting map multiplied keep proper scale become ground truth dataset use ntu rgb dataset unsupervised representation learning dataset consists video action class captured subject camera viewpoint viewpoint divided main view based horizontal angle camera respect subject front view left side view right side view left side degree view right side degree view view form view set used experiment action sequence simultaneously captured camera view time evaluation set standard evaluation protocol action recognition ntu rgb dataset cross subject evaluation cross view evaluation following conduct unsupervised learning experiment experiment ensure encoder trained test sample supervised learning setting cross subject evaluation follow training testing split cross view evaluation sample camera used training camera testing since need least camera unsupervised task randomly divide supervised training set ratio unsupervised training test use cross view prediction loss lxview evaluation metric table action recognition accuracy ntu rgb dataset method scratch tune lrecon view adversarial tune view adversarial tune cross subject rgb depth flow cross view depth rgb flow table comparison state art method action recognition ntu rgb dataset method hog super normal vector hon shuf learn luo lie group ftp dynamic skeleton hbrnn layer lstm lstm gca lstm ensemble lstm depth skeleton lstm modality cross subject cross view depth skeleton flow quanti performance model predict motion across different view experiment input modality rgb depth result table show quantitative result unsupervised prediction task order demonstrate effect different component loss term evaluate different variant proposed framework first train encoder cross view decoder denoted proposed method lrecon lcls add reconstruction decoder reconstruction loss denoted proposed method lcls finally add view adversarial training view classi cation loss form proposed method across input modality reconstruction view adversarial training improve cross view prediction performance comparing different input modality achieves lowest lxview expected contains view invariant motion information show qualitative example prediction depth input pair row upper row ground truth ows whereas lower row ows predicted decoder model show ability estimate raw motion multiple view encoded representation action recognition ntu rgb implementation detail experiment setting described section train model adam optimizer mini batch size learning rate weight decay set learning rate encoder tune scratch decay learning rate half every step tune since training converges faster half learning rate every step result table show classi cation accuracy cross subject cross view action recognition input modality across modality supervised learning scratch table cross subject action recognition accu racy msrdailyactivity dataset table cross view action recognition accuracy northwestern ucla dataset method actionlet ensemble hon mst aog snv hopc luo scratch tune accuracy method actionlet ensemble hankelets mst aog hopc nktm luo scratch tune accuracy lowest accuracy unsupervised learned representation training linear action classi signi cantly increase accuracy fine tuning encoder improve performance remove view adversarial training unsupervised framework accuracy would decrease especially cross view recognition among input modality input achieves highest accuracy agrees unsupervised learning flow input modality higher accuracy cross view recognition compared cross subject recognition support observation view invariant modality comparison state art table compare method state art method ntu rgb dataset rst group method use depth input second group method use skeleton input implement unsupervised learning method italic report classi cation accuracy directly cite result report map rather accuracy implementation achieve similar map depth input proposed method outperforms previous method increase accu racy signi cant cross view recognition show learned representation invariant viewpoint change input method achieves comparable performance skeleton based method however skeleton higher level feature robust viewpoint change moreover method higher cross view accuracy us explicit coordinate system transformation achieve view invariance transfer learning action recognition section perform transfer learning task use unsupervised learned representa tions action recognition datasets new domain different subject environment viewpoint perform cross subject evaluation msr dailyactivity dataset cross view evaluation northwestern ucla multiviewaction dataset experiment scratch tune setting depth modality input msr dailyactivity dataset dataset contains video action performed subject follow experimental setting video half subject training data video rest half test data northwestern ucla multiviewaction dataset dataset contains video action performed subject captured camera different view follow use video rst view training video third view test result table result comparison state art method datasets training deep model scratch give poor performance unsupervised learned representation increase accuracy margin method outperforms previous unsupervised method achieves comparable performance skeleton based method marked depth based method marked use carefully hand craft feature demonstrates learned representation generalize across domain conclusion work propose unsupervised learning framework leverage unlabeled video data multiple view learn view invariant video representation capture motion dynamic learn video representation representation source view predict ows multiple target view propose view adversarial training enhance view invariance learned representation train unsupervised framework ntu rgb dataset demonstrate effectiveness learned representation cross subject cross view action recognition task across multiple datasets proposed unsupervised learning framework naturally extended beyond action future work intend extend framework view invariant representation learning task gesture recognition person identi cation addition consider generative adversarial network gan multi view data generation acknowledgment research supported national research foundation prime minister singapore strategic capability research centre funding initiative reference bengio lamblin popovici larochelle greedy layer wise training deep network nip page bengio laufer alain yosinski deep generative stochastic network trainable backprop icml page carreira zisserman quo vadis action recognition new model kinetics dataset cvpr page cheng wan saudagar namuduri buckle advance human action recognition survey arxiv preprint arxiv doersch gupta efros unsupervised visual representation learning context prediction iccv page donahue hendricks guadarrama rohrbach venugopalan darrell saenko long term recurrent convolutional network visual recognition description cvpr page wang wang hierarchical recurrent neural network skeleton based action recognition cvpr page fernando bilen gavves gould self supervised video representation learning odd network cvpr page ganin lempitsky unsupervised domain adaptation backpropagation icml page ganin ustinova ajakan germain larochelle laviolette marchand lempitsky domain adversarial training neural network jmlr gidaris singh komodakis unsupervised representation learning predicting rotation iclr goodfellow pouget abadie mirza warde farley ozair courville bengio generative adversarial net nip page haque peng luo alahi yeung towards viewpoint invariant human pose estimation eccv page zhang ren sun delving deep rectus er surpassing human level performance imagenet classi cation iccv page zhang ren sun deep residual learning recognition cvpr page zheng lai zhang jointly learning heterogeneous feature rgb activity recognition cvpr page ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift icml page isik tacchetti poggio fast invariant representation human action visual system journal neurophysiology jaimez souiai jim nez cremers primal dual framework real time dense rgb scene icra page kingma adam method stochastic optimization iclr kong ding deeply learned view invariant feature cross view action recognition ieee trans processing building high level feature scale unsupervised learning icassp page lee huang singh yang unsupervised representation learning sorting sequence iccv page lee kim kang lee ensemble deep learning skeleton based action recognition temporal sliding lstm network iccv page camp sznaier cross view activity recognition hankelets cvpr page wong zhao kankanhalli attention transfer web image video recognition acm multimedia page wong zhao kankanhalli dual glance model deciphering social relationship iccv page zickler discriminative virtual view cross view action recognition cvpr page zhang liu action recognition based bag point cvpr page liu shahroudy wang spatio temporal lstm trust gate human action recognition eccv page liu wang duan kot global context aware attention lstm network action recognition cvpr page jia tang range depth feature action recognition cvpr page luo peng huang alahi fei fei unsupervised learning long term motion dynamic video cvpr page misra zitnick hebert shuf learn unsupervised learning temporal order veri cation eccv page hausknecht vijayanarasimhan vinyals monga toderici beyond short snippet deep network video classi cation cvpr page noroozi pirsiavash favaro representation learning learning count iccv page ohn bar trivedi joint angle similarity hog action recognition cvpr workshop page oneata verbeek schmid action event recognition sher vector compact feature set iccv page oreifej liu hon histogram oriented normal activity recognition depth sequence cvpr page parameswaran chellappa view invariance human action recognition ijcv patraucean handa cipolla spatio temporal video autoencoder differentiable memory iclr workshop rahmani bennamoun learning action recognition model depth skeleton video iccv page rahmani mahmood huynh mian histogram oriented principal component cross view action recognition ieee tpami rahmani mian action recognition novel viewpoint cvpr page rahmani mian shah learning deep model human action recognition novel viewpoint ieee tpami ranzato szlam bruna mathieu collobert chopra video language modeling baseline generative model natural video arxiv preprint arxiv salakhutdinov hinton deep boltzmann machine aistats page sermanet kavukcuoglu chintala lecun pedestrian detection unsupervised multi stage feature learning cvpr page shahroudy liu wang ntu rgb scale dataset human activity analysis cvpr page shahroudy gong wang deep multimodal feature analysis action recognition rgb video ieee tpami simonyan zisserman stream convolutional network action recognition video nip page springenberg dosovitskiy brox riedmiller striving simplicity convolutional net arxiv preprint arxiv srivastava mansimov salakhutdinov unsupervised learning video representation lstms icml page vemulapalli arrate chellappa human action recognition representing skeleton point lie group cvpr page vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders icml page wang schmid action recognition improved trajectory iccv page wang liu yuan mining actionlet ensemble action recognition depth camera cvpr page wang liu yuan learning actionlet ensemble human action recognition ieee tpami wang nie xia zhu cross view action modeling learning recognition cvpr page wang gao zhang tang ogunbona scene action map new representation rgb based action recognition convolutional neural network cvpr page wang gupta transitive invariance self supervised visual representation learning iccv page yang tian super normal vector activity recognition depth sequence cvpr page zhang lan xing zeng xue zheng view adaptive recurrent neural network high performance human action recognition skeleton data iccv page zhang wang xiao zhou liu shi cross view action recognition via continuous virtual path cvpr page']\n"
          ]
        }
      ],
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "print(\"target word: {}, context word: {}\".format(inverse_vocab[target_word], inverse_vocab[context_word]))\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:02.090155Z",
          "iopub.status.busy": "2021-05-11T17:14:02.089388Z",
          "iopub.status.idle": "2021-05-11T17:14:02.099805Z",
          "shell.execute_reply": "2021-05-11T17:14:02.100526Z"
        },
        "papermill": {
          "duration": 0.050268,
          "end_time": "2021-05-11T17:14:02.100713",
          "exception": false,
          "start_time": "2021-05-11T17:14:02.050445",
          "status": "completed"
        },
        "tags": [],
        "id": "technological-complex"
      },
      "outputs": [],
      "source": [
        "# Add a dimension so you can use concatenation (on the next step).\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "# Concat positive context word with negative sampled words.\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:02.176685Z",
          "iopub.status.busy": "2021-05-11T17:14:02.175318Z",
          "iopub.status.idle": "2021-05-11T17:14:02.181390Z",
          "shell.execute_reply": "2021-05-11T17:14:02.182466Z"
        },
        "papermill": {
          "duration": 0.048419,
          "end_time": "2021-05-11T17:14:02.182716",
          "exception": false,
          "start_time": "2021-05-11T17:14:02.134297",
          "status": "completed"
        },
        "tags": [],
        "id": "romance-rochester",
        "outputId": "bd9409c1-082e-418d-a0ae-07eccbea1650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_index    : 71\n",
            "target_word     : metric learning temporal sequence alignment damien garreau en damien garreau en emi lajugie inria remi lajugie inria sylvain arlot cnrs sylvain arlot en francis bach inria francis bach inria abstract paper propose learn mahalanobis distance perform alignment multivariate time series learning example task time series true alignment known cast alignment problem structured prediction task propose realistic loss alignment optimization tractable provide experiment real data audio audio context learning similarity measure lead improvement performance alignment task propose use metric learning framework perform feature selection basic audio feature build combination better alignment performance introduction problem aligning temporal sequence ubiquitous application ranging bioinformat ic audio processing goal align similar time series global structure local temporal difference alignment algorithm rely similar ity measure good metric crucial especially high dimensional setting feature signal irrelevant alignment task goal paper learn similarity measure annotated example order improve relevance alignment example context music information retrieval alignment used different case audio audio alignment audio score alignment rst case goal match audio interpretation piece potentially different rythm whereas audio score alignment focus matching audio signal symbolic representation score second case attempt learn annotated data measure performing alignment joder propose generative model context keshet learn measure discriminative setting similarly keshet use discriminative loss learn measure work focus audio audio alignment context set authorized alignment much larger explicitly cast problem structured prediction task solve shelf stochastic optimization technique proper signi cant adjustment particular term loss idea alignment relevant community speech recognition since pioneering work sakoe chiba contributed equally sierra project team epartement informatique ecole normale sup erieure cnrs inria en need metric learning go far beyond unsupervised partitioning problem weinberger saul proposed margin framework learning metric nearest neighbour algorithm based set must link must link constraint lajugie proposed use margin framework learn mahalanobis metric context partitioning problem since structured svm proposed tsochantaridis taskar successfully used solve many learning problem instance learn weight graph matching metric ranking task used learn graph structure graph cut make following contribution cast learning mahalanobis metric context alignment structured prediction problem real musical datasets metric improves performance alignment algo rithms high level feature propose use metric learning framework learn combination basic audio feature get good alignment performance experimentally standard hamming loss although tractable computationnally permit learn relevant similarity measure real world setting propose new loss closer true evaluation loss alignment leading tractable learning task derive cient frank wolfe based algorithm deal new loss loss solves issue encountered hamming loss matricial formulation alignment problem notation paper consider alignment problem multivariate time series sharing dimension possibly different length namely rta rtb refer row ata btb column vector denote pair signal let rta arbitrary pairwise nity matrix associated pair encodes nity note framework extended case multivariate signal different dimension long well ned goal alignment task non decreasing sequence index length max match time index time series time maximal index time series way cid satis matching beginning matching ending type move given binary matrix every otherwise denote set matrix uniquely determined example given vertical move matrix mean signal waiting whereas horizontal mean waiting diagonal move mean move together sense time reference warped known alignment task cast following linear program set cid max goal learn form nity matrix learned alignment obtained optimization problem referred decoding model dynamic time warping given nity matrix associated pair signal nding alignment solves done ciently tatb example valid alignment encoded matrix red upper triangle grey zone corresponds area loss ab blue lower one dynamic programming algorithm often referred dynamic time warping algorithm described alg supplementary material various additional constraint may used dynamic time warping algorithm could easily add alg cardinality set huge corresponds number path rectangular grid southwest northeast corner vertical horizontal diagonal move allowed nition delannoy number noted go nity mahalanobis metric many application see pair nity matrix computed cid cid paper propose learn metric compare instead plain euclidean metric parametrized matrix set semi nite positive matrix use corresponding mahalanobis metric compute pairwise nity cid note decoding maximization linear function parameter max cid max cid joint feature map cid cid cid learning metric assume given pair training instance goal matrix predicted alignment close groundtruth example well unseen example rst loss alignment order quantify proximity alignment see necessary fully labelled instance mean pair need exact alignment partial alignment might dealt alternating metric learning constrained alignment loss alignment framework alignment encoded matrix thus interested func tions cid frobenius norm ned cid cid hamming loss simple loss matrix frobenius norm difference turn unnormalized hamming loss valued matrix matrix ned cid cid cid cid cid cid cid cid cid cid vector coordinate equal last line come fact value make hamming loss loss often used structured prediction task audio score setting keshet use modi version loss average number time difference alignment greater xed threshold loss easy optimize since linear parametrization alignement problem optimal audio audio alignment indeed major drawback hamming loss alignment xed length depends number crossing alignment path easily cid cid much closer see important notice often case length signal grows area loss natural loss computed mean distance beween path depicted matrix loss corresponds area path matrix represented grey zone formally put min area loss mean audio literature loss sometimes called mean absolute deviation loss noted ab unfortunately general alignment problem ab linear matrix context alignment sequence different nature signal reference thus index sequence ned increasing audio partition alignment problem loss linear argument precisely introduce matrix lta rta lower triangular one including diagonal write loss min cid cid lta cid lta cid cid lta cid lta cid lta kyk cid easy see lta cid cid prove loss corresponds area loss special case let alignment vertical move unique ykj ltay lta exactly area curve determined path experiment use ab evaluation training approximation area loss many real world applica tions meaningful loss ass quality alignment area loss shown experiment hamming loss suf cient simple situation allows learn metric lead good alignment performance term area loss challenging datasets work see sec due fact alignment close term area loss suffer big hamming loss thus natural extend formulation matrix start symmetrizing formulation overcome problem overpenalization vertical horizontal move couple binary matrix symmetrized area loss cid cid cid lta cid cid cid cid ltb cid cid ltb cid lta ltay cid cid ltb cid cid cid cid ltb cid ltay cid cid cid real world bach chorale dataset represented groundtruth alignment together others term hamming loss alignment far groundtruth whereas area loss structured prediction setting described sec depicted alignment called violated constraint namely output loss augmented decoding step see sec propose make loss concave convex hull denote let introduce max cid max largest eigenvalue binary matrix cid cid cid cid dtb cid cid cid lta dta dta cid dta ltb cid dtb ltay cid cid ltb cid ltb cid cid cid get concave function coincides cid empirical loss minimization recall given alignment example xed loss cid goal solve following minimization problem cid cid cid argmax cid cid min convex regularizer preventing tting cid cid margin approach section describe margin approach solve surrogate problem untractable shown decoding task maximum linear function parameter aim predicting output discrete space space potential alignment respect constraint learning thus fall structured prediction framework hinge loss convex surrogate max cid cid cid cid cid cid cid completeness experiment try set matrix minimal trace dominate cid solving semide nite program sdp report associated note matrix could chosen particular since matrix pointwise positive matrix diag cid loss concave cid tatb violated constraint hamming lossmost violated constraint lsgroundruth alignment evaluation usually referred loss augmented decoding see cid argmax elementary computation cid argmin cid cid cid cid aim solving following problem sometimes called margin rescaled problem rta cid cid cid cid cid cid min cid cid max cid hamming loss case notice joint feature map linear thus take loss linear rst argument cid instance hamming loss loss augmented decoding maximization linear function space solve ciently dynamic programming algorithm see sec supplementary material way plugging hamming loss lead convex structured prediction problem problem solved standard technique cutting plane method stochastic gradient descent block coordinate frank wolfe dual note adapted standard unconstrained optimization method setting cid cid cid cid min optimization symmetrized area loss symmetrized area loss concave rst argument thus problem min max form deriving dual straightforward detail found supplementary material plug symmetrized area loss cid sal ned problem dual following form cid cid denote convex hull set cartesian product training example set note recover similar since sal loss concave aforementioned problem convex problem quadratic program compact set thus use frank wolfe algorithm note similar proposed lacoste julien additional term due concavity loss cid experiment applied method task learning good similarity measure aligning audio sig nals eld researcher spent lot effort designing well suited meaningful feature problem combining feature aligning temporal sequence still challenging simplicity took diagonal experiment dataset kirchhoff lerch dataset description first applied method dataset kirchhoff lerch dataset pair aligned example arti cially created stretching original audio signal way groundtruth alignment known thus data fall setting precise description dataset found pair stretched along different tempo curve signal made music divided frame hopsize thus leading typical length signal setting keep feature simple implement known perform well alignment task mfcc labeled spectral atness spectral centroid spectral spread maximum envelope max power level frame pow see detail computation feature normalize feature subtracting median value dividing standard deviation median audio data subject outlier comparison performance individual feature learned metric error bar performance learned metric determined best worst perfor mance different experiment denotes learned combination method best mfcc combination experiment conducted following experiment individual feature perform alignment dynamic time warping algorithm evaluate performance single feature term loss typically used ass performance setting report result experiment plug data method hamming loss learn linear positive com bination feature reported thus combining feature dataset yield better performance considering single feature completeness conducted experiment standard rst mfccs coef cients rst second order derivative feature result competed best learned combination handcrafted feature namely term ab loss perform second note result slightly worse best single handcrafted feature better best mfcc coef cient used feature baseline compared uniform combination handcrafted feature metric identity matrix result chart ab second individual value ranging second second chorale dataset dataset bach dataset consists ten bach chorale small quadriphonic piece chorale midi reference corresponding score basically representation partition alignment midi le audio given thus converted midi le audio following classically done alignment see way fall audio audio framework technique apply piece music approximately long leading similar signal length experiment use feature sec depicted optimization hamming loss performs poorly dataset fact best individual feature performance far better performance learned thus metric learning practical hamming loss performs much worse best single feature conducted learning experiment symetrized area loss cid resulting learned parameter far better learned hamming loss get performance similar best feature note feature handcrafted reaching performance hard task training instance already challenging http music northwestern edu data bach html wmpowm scm srsfm maxssm ab performance algorithm chorale dataset left right best single feature best learned combination feature symmetrized area loss cid best combination mfcc sal obtained via sdp see footnote section best combination mfcc derivative learned cid best combination mfccs derivative learned hamming loss best combination feature hamming loss depicted learned parameter loss augmented decoding performed either area known structured svm represents violated constraint see violated constraint hamming loss lead align ment totally unrelated groundtruth alignment whereas symmetrized area loss far closer much discriminative feature selection last conducted feature selection experiment datasets starting low level feature namely leading mfccs coef cients rst derivative learn linear combination achieves good alignment performance term area loss note little musical prior knowledge put moreover either improve best handcrafted feature dataset perform similarly datasets performance learned combination handcrafted feature performed similarly combination mfccs coef cients conclusion paper presented structured prediction framework learning metric tempo ral alignment problem able combine hand crafted feature well building automat ically new state art feature basic low level information little expert knowledge technically made possible considering loss beyond usual hamming loss typically used practical within structured prediction framework linear output representation present work may extended several way main consider case partial information alignment available often case music bioinformatics application note similarly lajugie simple alternating optimiza tion metric learning constrained alignment provide simple rst solution could probably improved upon acknowledgement author acknowledge support european research council sierra project gargantua project funded mastodon program cnrs airbus foundation phd fellowship thanks piotr bojanowski helpful discussion warm thanks arshia cont philippe cuvillier sharing knowledge audio processing holger kirchhoff alexander lerch dataset ab reference aach church aligning gene expression time series time warping algorithm bioinfor matics banderier schwer delannoy number journal statistical planning inference caetano mcauley cheng smola learning graph matching ieee trans pami cont schwarz schnell raphael evaluation real time audio score alignment proc ismir cuturi vert birkenes matsui kernel time series based global alignment proc icassp volume page ieee dixon widmer match music alignment tool chest proc ismir page frank wolfe algorithm quadratic programming naval research logistics quarterly gold morgan elli speech audio signal processing processing perception speech music john wiley son hamming error detecting error correcting code bell system technical journal dannenberg tzanetakis polyphonic audio matching alignment music retrieval computer science department page joachim finley cutting plane training structural svms machine learning joder essid richard learning optimal feature polyphonic audio score alignment ieee trans audio speech language processing keshet shalev shwartz singer chazan margin algorithm speech phoneme music score alignment ieee transaction audio speech language processing kirchhoff lerch evaluation feature audio audio alignment journal new music research lacoste julien jaggi schmidt pletscher block coordinate frank wolfe optimization structural svms proc icml lajugie bach arlot margin metric learning constrained partitioning problem proc icml mcfee lanckriet metric learning rank proc icml page uller information retrieval music motion springer sakoe chiba dynamic programming algorithm optimization spoken word recognition acoustic speech signal processing ieee transaction shalev shwartz singer srebro cotter pegasos primal estimated sub gradient solver svm mathematical programming szummer kohli hoiem learning crfs graph cut proc cvpr taskar koller guestrin max margin markov network adv nip thompson plewniak poch balibase benchmark alignment database evaluation multiple alignment program bioinformatics torres cabada nieto exact formula number alignment dna sequence mitochondrial dna tsochantaridis joachim hofmann altun singer margin method structured interdependent output variable journal machine learning research weinberger saul distance metric learning margin nearest neighbor classi cation journal machine learning research\n",
            "context_indices : [70 16  4 40  8]\n",
            "context_words   : ['resnets provably better linear predictor department computer science applied mathematics ohad shamir weizmann institute science rehovot israel ohad shamir weizmann abstract residual network resnet standard deep neural net architecture state art performance across numerous application main premise resnets allow training layer focus tting residual previous layer output target output thus expect trained network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather getting stuck arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual unit indeed exhibit behavior sense optimization landscape contains local minimum value obtained linear predictor namely layer network notably minimal assumption precise network architecture data distribution loss function used provide quantitative analysis approximate stationary point problem finally certain tweak architecture training network standard stochastic gradient descent achieves objective value close better linear predictor introduction residual network resnets popular class arti cial neural network providing state art performance across numerous application kim xie xiong unlike vanilla feedforward neural network resnets characterized skip connection output layer directly added output following layer mathematically whereas feedforward neural network expressed stacking layer form input output pair tunable parameter function resnets built residual unit form xed function fact common let identity case unit take form intuitively mean layer training focus tting residual target given rather particular adding depth harm performance since effectively eliminate layer tuning zero function due property residual network proven effective training extremely deep network hundred layer despite widespread empirical success rigorous theoretical understanding training residual network limited recent theoretical work optimization deep learning conference neural information processing system neurips montr canada soltanolkotabi yun soudry hoffer brutzkus safran shamir lee name example focused simpler feedforward architecture capture property residual network recent result consider residual like element see discussion related work generally apply standard architecture particular aware theoretical justi cation basic premise resnets namely architecture allows adding layer without harming performance problem training neural network involves solving highly non convex problem local search procedure thus even though deeper residual network express shallower one clear training process indeed converge network better perhaps attempt train residual network gradient based method might hit poor local minimum worse error obtained shallower network question main motivation work secondary motivation several recent result yun safran shamir liang demonstrate spurious local minimum value larger global minimum exist general training neural network even fairly strong assumption thus instead aiming demonstrating minimum exist might good true realistic network perhaps consider modest goal showing minimum exist certain non trivial level set level set correspond instance optimal value attainable shallower network without additional residual layer paper study question considering competitiveness simple residual network composed arbitrarily deep nonlinear residual unit linear output layer respect linear predictor equivalently layer network speci cally consider optimization problem associated training residual network general non convex complicated structure nevertheless prove optimization landscape local minimum value higher achieved linear predictor data word run local search procedure reach local minimum assured solution worse best obtainable linear predictor importantly fairly minimal assumption residual unit assumption data distribution linear separability assumption loss function used besides smoothness convexity network output satis loss used practice addition provide quantitative analysis show every point close stationary certain direction see sec precise nition poly worse xed linear predictor result geometric nature explain later necessarily imply standard gradient based method indeed converge desirable solution example since iterates might diverge nevertheless provide algorithmic showing residual architecture changed bit standard stochastic gradient descent sgd procedure predictor similar better best linear predictor relies simple perhaps unexpected reduction setting online learning might independent interest supplementary material paper contains proof appendix discussion result generalized vector valued output appendix related work far know existing rigorous theoretical result residual network pertain linear network combine linear residual unit form although network used practice capture important aspect non convexity associated training residual network particular hardt showed linear residual network squared loss spurious local minimum namely every local minimum global recently bartlett proved convergence result gradient descent problem assuming input isotropic target linear mapping symmetric positive nite showing similar result non linear network mentioned hardt major open problem paper focus non linear residual unit consider local minimum level set term setting perhaps work closest liang considers network written cid hidden layer network arbitrary possibly deeper network technical assumption data distribution activation used network size assuming certain classi cation loss author prove training objective benign sense network corresponding local minimum zero classi cation error however author point architecture different standard resnets would require nal tunable layer combine output result provably hold architecture moreover technical assumption non trivial apply standard activation loss relu activation logistic loss require speci condition data linear separability certain low rank structure contrast study standard residual unit make minimal assumption network data distribution loss used side prove result local minimum certain level set rather point finally idea studying stationary point non convex optimization problem reference level set explored work setting quite different setting preliminary start word basic notation terminology generally use bold faced letter denote vector assumed column form capital letter denote matrix function cid cid refers euclidean norm vector spectral norm matrix unless speci otherwise cid cid matrix denotes frobenius norm always upper bound spectral norm matrix vec refers entry written long vector according canonical order given function euclidean space denotes gradient denotes hessian point domain function local minimum cid cid open neighborhood finally use standard notation hide constant let poly referto expression polynomial consider residual network architecture consisting residual unit composed linear output layer scalar output cid cid make assumption structure overall depth network computes except last layer tunable linear transformation namely matrix necessarily square parameter condition follows full pre activation structure proposed empirically found best performing residual unit architecture commonly used practice tensorflow depart structure fully tunable rather convolution facilitate simplify theoretical study assumption given network output cid cid parameterized vector matrix possibly complicated function parame terized remark bias note model easily incorporate bias namely predictor form cid cid tunable standard trick augmenting additional coordinate whose value always assuming output vector additional coordinate value since result depend data geometry speci would affected modi cation assume network trained respect data distribution average training set loss function cid network prediction target value thus consider optimization problem min cid cid cid cid see appendix discussion result generalized network vector valued output unconstrained objective main focus paper general objective convex easily spurious local minimum saddle point result make explicit assumption distribution structure loss assume throughout paper following assumption loss cid twice differentiable convex assumption mild satis standard loss logistic loss squared loss smoothed hinge loss etc note assumption twice differentiable respect particular function ned xed twice differentiable emphasize throughout paper assume necessarily differentiable respect indeed represents network non differentiable operator relu max function cannot expect differentiable everywhere considering derivative think input long vector euclidean space order speci vec vector matrix discussed introduction wish compare objective value obtained linear predictor speci cally use notation flin cid cid cid cid denote expected loss linear predictor parameterized vector assumption function convex twice differentiable finally introduce following class point behave approximately like local minimum respect term rst derivative nition sopsp let open subset domain lipschitz second order partial stationary point sopsp cid cid min importantly note local minimum must sopsp local minimum differentiable function hence cid cid min nition directly generalizes well known notion second order stationary point sosp mccormick nesterov polyak jin ned function twice differentiable parameter fact nition sopsp equivalent requiring sosp need use general nition assuming differentiable interestingly sosp general class point non convex optimization gradient based method shown converge poly iteration competitiveness linear predictor main result thm corollary proven stage first point cid cid cid lower bounded term suboptimality respect best linear predictor thm consider case point suboptimal respect best linear predictor either cid cid strictly positive min strictly negative thm thus building nition sopsp previous section point suboptimal compared linear predictor local minimum theorem point cid vector dimension cid cid cid flin cid cid cid cid cid cid cid cid cid cid theorem implies point objective value larger linear predictor flin unless partial derivative respect namely non zero cannot stationary point respect local minimum proof theorem appears supplementary material relies following key lemma shall state roughly sketch proof lemma fix cid vector size matrix cid cid cid cid vec cid flin word inner product gradient carefully chosen vector lower bounded suboptimality compared linear predictor particular point suboptimal gradient cannot zero cid cid cid vec cid cid cid proof sketch lemma cid vec cid cid cid cid cid cid cid cid vec let cid equal cid cid careful technical calculation reveals expression cid cid cid cid cid cid cid cid turn equal cid cid cid cid cid cid cid recalling nition cid noting convexity cid follows lower bounded cid cid cid cid cid cid flin cid cid cid analyze case following theorem cid cid cid cid cid cid min cid min flin cid cid cid cid cid cid min cid cid cid cid min denotes minimal eigenvalue symmetric matrix combining theorem following main theorem fix positive suppose convex open subset domain max cid cid cid cid lipschitz lipschitz lipschitz respectively cid cid sopsp min cid cid flin poly note poly term hide dependency linear individual factor see proof supplementary material exact expression discussed sec local minimum must correspond sopsp hence theorem implies point minw cid cid flin long satis lipschitz continuity assumption nite bounded subset domain since hold arrived following corollary corollary suppose bounded subset hold lipschitz continuous every cal minimum satis domain inf flin word objective spurious local minimum value smallest attainable linear predictor remark generalization vector valued output consider generalization setting network vector valued output namely cid matrix loss cid taking vector valued argument convex cross entropy loss general setting possible prove variant thm similar proof technique see appendix however clear prove analog thm hence thm leave question future research effect norm regularization thm implies sopsp must value much worse obtained linear predictor moreover discussed sec point closely related second order stationary point gradient based method known converge quickly point jin thus tempting claim method indeed network competitive linear predictor unfortunately fundamental catch bound thm depends norm point via cid cid cid cid arbitrarily bad norm suf ciently word thm guarantee point sopsp good long far away origin dynamic gradient method iterates remain bounded domain least suf ciently slowly increasing norm would issue however priori guaranteed would case since optimization problem unconstrained assuming anything structure could parameter diverge meaningful algorithmic derived thm course option dependence cid cid cid cid artifact analysis sopsp competitive linear predictor regardless norm however following example show case example fix suppose scalar squared loss dependence parameter cid objective equivalently written see leftmost plot gradient hessian equal cid cid cid cid left right contour plot superimposed constraint cid cid inside circle axis corresponds axis corresponds exhibit spurious local minimum bottom left quadrant domain best viewed color particular gradient hessian equal arbitrarily close small enough however objective value cid cid respectively point equal cid cid flin remark example gradient hessian uniformly bounded lipschitz constant euclidean space however lipschitz constant bounded numerical constant includes stationary point studied construction indicates problem indeed lie norm unbounded lipschitz constant derivative standard approach ensure iterates remain bounded add regularization namely optimize min regularization term penalizing norm unfortunately alter objective might introduce new spurious local minimum exist graphically illustrated plot example without regularization form whereas stationary point either global minimum along valley corresponding saddle point regularization created new spurious local minimum around intuitively regularization make objective value increase well valley global minimum regularization choice lead phenomenon similar issue occur impose hard constraint namely optimize min constrained domain illustrates optimization problem spurious local minimum inside constrained domain course way issue making regularization parameter suf ciently small domain suf ciently regularization come effect cid cid suf ciently however correct choice depends run problem simply xed example change time priori guarantee chosen thus clear xed choice regularization would work lead gradient based method good local minimum success sgd assuming skip connection output discussed challenge getting algorithmic previous section possible assuming architecture network changed bit concretely instead network architecture cid cid consider architecture parameterized vector new objective written cid cid cid cid cid cid cid cid cid cid architecture corresponds skip connection directly network output rather nal linear output layer similar spirit skip connection studied liang except layer nonlinear network instead linear cid component follows consider standard stochastic gradient descent sgd algorithm train network fixing step size convex parameter domain initialize point randomly data point underlying data distribution perform denote euclidean projection set cid cid cid note always differentiable respect assume simplicity differentiable respect simply cid still easily veri hold use notation flin cid arbitrary vector cid cid cid cid cid cid denote expected loss linear predictor parameterized following theorem establishes mild condition running stochastic gradient descent suf ciently many iteration result network competitive xed linear predictor theorem suppose domain satis following positive constant closed convex set clidean space namely cartesian product support data distribution cid cid cid lipschitz bounded absolute value cid cid cid cid cid suppose perform iteration stochastic gradient descent described step size satis probability least iterates min flin cid cid log cid proof relies technically straightforward perhaps unexpected reduction adversarial online learning appears supplementary material roughly speaking idea stochastic gradient descent procedure equivalent online gradient descent respect sequence function ned iterates even though iterates change unexpected complicated way strong guarantee online learning allow sequence function rather arbitrary allow obtain theorem acknowledgement thank anonymous nip reviewer helpful comment research supported part european research council erc grant reference peter bartlett david helmbold philip long gradient descent identity initialization ciently learns positive nite linear transformation deep residual network arxiv preprint arxiv alon brutzkus amir globerson eran malach shai shalev shwartz sgd learns parameterized network provably generalize linearly separable data arxiv preprint arxiv simon jason lee power parametrization neural network quadratic activation arxiv preprint arxiv simon jason lee yuandong tian barnabas poczos aarti singh gradient descent learns hidden layer cnn afraid spurious local minimum arxiv preprint arxiv rong tengyu optimization landscape tensor decomposition advance neural information processing system page rong jason lee tengyu learning hidden layer neural network landscape design arxiv preprint arxiv moritz hardt tengyu identity matter deep learning arxiv preprint arxiv elad hazan introduction online convex optimization foundation trend cid optimization kaiming xiangyu zhang shaoqing ren jian sun deep residual learning recognition proceeding ieee conference computer vision pattern recognition page kaiming xiangyu zhang shaoqing ren jian sun identity mapping deep residual network european conference computer vision page springer chi jin rong praneeth netrapalli sham kakade michael jordan escape saddle point ciently arxiv preprint arxiv jiwon kim jung kwon lee kyoung lee accurate super resolution deep convolutional network proceeding ieee conference computer vision pattern recognition page shiyu liang ruoyu sun yixuan srikant understanding loss surface neural network binary classi cation arxiv preprint arxiv garth mccormick modi cation armijo step size rule negative curvature mathematical programming yurii nesterov boris polyak cubic regularization newton method global performance mathematical programming itay safran ohad shamir spurious local minimum common layer relu neural network arxiv preprint arxiv shai shalev shwartz online learning online convex optimization foundation trend cid machine learning mahdi soltanolkotabi adel javanmard jason lee theoretical insight optimization landscape parameterized shallow neural network arxiv preprint arxiv daniel soudry elad hoffer exponentially vanishing sub optimal local minimum multilayer neural network arxiv preprint arxiv saining xie ross girshick piotr doll zhuowen kaiming aggregated residual transformation deep neural network computer vision pattern recognition cvpr ieee conference page ieee wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig microsoft conversational speech recognition system acoustic speech signal processing icassp ieee international conference page ieee chulhee yun suvrit sra ali jadbabaie critical view global optimality deep learning arxiv preprint arxiv martin zinkevich online convex programming generalized nitesimal gradient ascent proceeding international conference machine learning icml page', 'memory oriented decoder light field salient object detection miao zhang jingjing wei yongri piao huchuan dalian university technology china miaozhang dlut edu lijingjing jiwei mail dlut edu yrpiao lhchuan dlut edu abstract light eld data demonstrated favor many task computer vision existing work light eld saliency detection still rely hand crafted feature paper present deep learning based method novel memory oriented decoder tailored light eld saliency detection goal deeply explore comprehensively exploit internal correlation focal slice accurate prediction designing feature fusion integration mechanism success method demonstrated achieving state art datasets present problem way accessible member community provide scale light eld dataset facilitates comparison across algorithm code dataset made publicly available http github com oiplab dut molf introduction salient object detection sod ability identify visually distinctive object despite substantial appearance similarity scene fundamental task attracted lot interest due importance various application visual tracking object recognition segmentation retrieval robot navigation existing method categorized rgb rgb light eld saliency detection based input data type method achieved great success long dominant eld saliency detection however saliency detection method may suffer false positive come challenging scene shown reason twofold first traditional method underlie many prior knowledge violation highly pose risk complex scene second deep learning based method subject feature extracted limited rgb data containing much special information rgb data light eld data saliency detection attracted lot attention depth map providing scene layout improve saliency accuracy extent however mediocre quality depth map heavily jeopardize accuracy saliency detection light eld provides image scene array viewpoint spread extent lens aperture different view used produce stack focal slice containing abundant spatial parallax information well accurate depth information object scene furthermore focusness strongest information allowing human observer instantly understand order object arranged along depth scene light eld data demonstrated favor many application computer vision depth estimation super resolution material recognition due denotes equal contribution prof piao corresponding author conference neural information processing system neurips vancouver canada left challenging scene similar foreground background complex back ground transparent object low intensity environment right light eld data focal slice focus different depth level green box red dot represents different focus position observation bene cial cient foreground background separation show model saliency result mean ground truth unique property light eld shown promising prospect saliency detection however deep learning based light eld method missing contemporary study saliency detection strong reason believe introducing cnn framework light eld saliency detection important aspect method sod order incorporate cnn framework light eld accurate sod key issue needed considered first solve ciency training data second effectively properly fuse light eld feature generated different focal slice third comprehensively integrate multi level feature paper leverage idea light eld confront challenge better adapt network fuse feature focal slice may neither want ignore contribution corresponding focal slice salient object happens focus destroy spatial correlation different focal slice therefore propose novel memory oriented spatial fusion module sfm resemble memory mechanism human fuse information understand scene going piece information emphasizing relevant one hand integration fused feature used higher cognitive processing therefore propose sophisticated multi level integration mechanism top manner high level feature used guide low level feature selection namely memory oriented feature integration module fim previous information referred memory used channel attention update current light eld feature important unnecessary feature distinguishable summary main contribution follows introduce scale light led saliency dataset sample con tains focus focal stack focal slice depth map corresponding ground truth genuinely hoping could pave way light eld sod enable advanced research development propose novel memory oriented decoder tailored light eld sod feature fusion mechanism sfm feature integration mechanism fim enable accurate prediction work best knowledge rst exploitation unique focal slice light eld data deep learning based saliency detection extensive experiment light eld datasets method achieves consis tently superior performance state art approach related work salient object detection early work saliency detection mainly rely hand crafted feature prior knowledge color contrast background prior recently utilization cnns sod achieved appealing performance adopt cnn extract multi scale feature predict saliency super pixel wang propose cnns integrate local super pixel estimation global search sod zhao utilize independent cnns extract global local context lee combine low level distant map high level semantic feature deep cnns sod rgb complex scenario method achieve better performance suffer time consuming computation injure spatial information input image afterwords liu han rst generate coarse saliency map detail step step hou introduce short connection multiple side output based hed architecture zhang integrate multi level feature multiple resolution combine accurate prediction luo propose simpli cnn combine local global information design loss penalize boundary error zhang liu introduce attention mechanism guide feature integration deng design residual nement block learn complementary saliency information intermediate prediction transfer contour knowledge saliency detection without manual saliency mask detailed survey sod found sod depth image uent spatial information act complementary cue saliency detection peng regard depth data channel input feed multi stage saliency detection model feng present saliency method based anisotropic center surround difference local background enclosure zhu propose center dark channel prior rgb sod use hand crafted feature train cnn achieve better performance tradition method stream model used process rgb depth map separately cross modal feature combined jointly make prediction due limited training set trained stage wise manner chen design progressive fusion network fuse cross modal multi level feature predict saliency map chen propose stream network extract rgb feature use attention mechanism adaptively select complement zhu use scale rgb datasets pre train prior model employ depth induced feature enhance network previous work light eld sod shown promising prospect especially complex scenario report saliency detection approach light eld data propose rst light eld saliency dataset lfsd zhang propose saliency method based depth contrast focusness based background prior effectiveness superiority light eld property introduce weighted sparse coding structure handling heterogenous type input data zhang integrate multiple visual cue light eld image detect salient region however deep learning based light eld method still infancy many issue yet explored light field dataset remedy data ciency problem introduce scale light eld saliency dataset selected high quality sample captured lytro illum camera decode light eld format lytro desktop light eld consists focus focal stack focal slice focusing different depth depth corresponding manually labeled ground truth focal stack resembles human perception eye eye dynamically refocus different focal slice determine saliency show sample light eld proposed dataset observation bene cial cient foreground background separation annotation volunteer asked draw rectangle attractive object collect sample choosing image consensus manually label salient object focus commonly used segmentation tool supplying easy understand dataset hope promote research make sod problem accessible familiar eld proposed light eld saliency dataset provides unique focal slice used support training need deep neural network dataset consists indoor outdoor scene captured surrounding environment daily life ce supermarket campus street besides dataset contains many challenging scene shown similar foreground background complex background transparent object multiple object low intensity environment proposed network overall architecture adopt widely utilized vgg net backbone architecture drop last pooling layer fully connected layer reserve convolutional block better task overall architecture proposed network contains encoder memory oriented decoder shown encoder rgb fed stream generate raw rgb feature focal slice fed another stream generate light eld feature abundant spacial information simplicity illustrate single encoder represents stream simultaneously suggested conv block block might shallow make reliable prediction hereby perform decoder deeper layer block block speci cally given rgb focal slice size denote output last block represents feature generated rgb stream represents index focal slice represents last convolution block memory oriented spatial fusion module sfm raw rgb light eld feature generated encoder aim fusing available information address challenging problem light eld sod straightforward solution simply concatenate light eld feature produced different focal slice however drawback emerge approach first ignores relative contribution different focal slice nal result focal slice represent image focused different depth scene shown intuitively different focal slice different weight regarding salient object second direct concatenation operation seriously damage spatial correlation focal slice proper effective fusion strategy considered hence propose novel memory oriented spatial fusion module sfm address problem module introduce attention mechanism shown emphasize useful feature suppress unnecessary one focused blurred information procedure ned attm avgp ooling cid cid atti mean concatenation operation represent convolution operator convolution parameter layer avgp ooling mean global average pooling operation mean softmax function attm mean channel wise attention map weighted light eld feature cid layer cid denotes feature wise multiplication regarded sequence input corresponding consecutive time step fed convlstm structure gradually fimcellmo fimcellmo fimcellmo fimcellencoderthe illustration sfm convlstmcellscimthe illustration fim fimcellconvlstmcellscimmo fimcellpoolingsoftmax cid channel attentionscimmo sfm sfm sfm sfm block block block block block memory orienteddecoderpredictionrgbfocal stacksupervision cid cid cid cid cid cid abcdeelement wisemultiplicationelement wiseadditionfeature wisemultiplicationup icic icconvconvic cellcellcellcellgpmsupervision cid cid cid cid cid cid cic poolingconcatconv cid cid cid cid cid cid kllcsoftmaxgpmconcatenatedilated rate dilated rate dilated rate dilated rate cid cid cid cic cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid visual comparison ablation study mean rgb mean light eld data concatenation without weighting mean concatenation weighting mean convlstm fusion weighting represents gpm full sfm mean whole network without scim mean nal model spatial information accurately identifying salient object procedure ned whi wci whf wcf wxi cid wxf cid tanh wxc cid wxo cid tanh whc wco denotes hadamard product sigmoid function memory cell store earlier information model parameter learned gate memory cell hidden state tensor way step fused light eld feature effectively generated unique property light eld data make spontaneously suitable use convlstm feature fusion convlstm bene cial making better use spatial correlation multiple focal slice thanks powerful gate memory mechanism model enhances average mae performance nearly point proposed dataset lfsd dataset tab furthermore capture global contextual information different scale extend global perception module gpm top gpm ned conv cid convd denotes concatenation operation cid mean operation performed several time different dilation rate rate set denoted result returned parameter learned layer nal fused light eld feature multiple layer end add several intermediate supervision layer facilitate network convergence encourage explicit fusion light eld feature memory oriented feature integration module fim cient integration hierarchical deep feature signi cant pixel wise prediction task salient object detection semantic segmentation propose new memory oriented module novel perspective utilizes memory mechanism effectively tegrate multi level light eld feature top manner speci cally channel feature map con sidered feature detector design scene context integration table quantitative result ablation analysis network meaning index explained caption index module rgb weighting weighting sfm gpm sfm gpm fim scim fim scim lfsd hfut cid dut cid module scim shown utilizes memory information toper layer learn channel attention map update current light eld feature focusing important channel suppressing unnecessary one convlstm progressively integrates high level memory current elaborately ned input say high level feature abundant semantic information gradually summarized memory used guide selection low level spatial detail precise saliency prediction speci cally scim shown represents previous scene understanding hidden state convlstm time step mean fused light eld feature mth layer scim ned cid avgp ooling denote element wise addition multiplication respectively updated feature cid fed convlstm cell summarize spatial information historical memory current input cid use output block initial state convlstm scim step corresponding cid cid cid cid respectively output ned cid convlstm followed transition convolutional layer operation get nal saliency map calculation procedure similar equ replacing input however top structure may cause high level feature diluted trans mitted lower layer address problem inspired densenet link feature low high level way shown alleviate gradient vanish ing meanwhile encourage feature reuse nal light eld feature used set successively besides order guarantee time step convlstm explicitly learn important formation accurately identifying salient object add intermediate supervision internal output convlstm generally speaking intermediate supervision act instruction guide scim convlstm accurately lter non salient area retain salient area intermediate result illustrated full detail code made publicly available visual result intermediate supervi sion complex scene model gradu ally optimize saliency map produce precise prediction experiment datasets evaluate performance proposed network conduct experiment proposed dataset public light eld saliency datasets lfsd hfut dataset consists light eld sample randomly select sample training remaining sample testing detail found sec lfsd dataset contains light eld captured lytro camera dataset proposed pioneered use light eld solving challenging problem sod hfut hfut consists sample captured lytro camera challenging dataset real life scenario various distance sensor noise lighting condition sample lfsd hfut used testing evaluate generalization ability saliency model avoid tting augment training set ipping cropping rotating experiment setup evaluation metric adopt metric comprehensive evaluation including precision recall curve measure mean absolute error mae measure measure fimcellmo fimcellmo fimcellmo fimcellmo sfmmo sfmmo sfmmo sfmdecoderimagegt cid cid cid illustration baseline network rgb light eld data input correspond tab respectively term light eld input use concatenation without weighting strategy fuse light eld feature different focal slice conv block fairness intermediate supervision proposed network curve proposed method cnns based method obviously consistently outstanding approach universally agreed standard evaluating sod model well explained many literature due limited space detailed description implementation detail network implemented pytorch framework trained gtx gpu training test image uniformly resized network trained end end manner momentum weight decay learning rate set respectively training phrase use softmax entropy loss network trained standard sgd converges epoch batch size backbone network rgb focal stack stream initialized corresponding pre trained vgg net parameter initialized gaussian kernel ablation study effectiveness light field data tab detection result baseline network illustrated rgb data light eld data respectively numerical result measured measure mae demonstrate network light eld data outperforms rgb data visual comparison aforementioned network respectively indicates light eld data improve prediction performance challenging circumstance moreover conduct experiment repeating rgb input frame time way model architecture identical version input data quantitative result term measure mae focal slice rgb respectively con rms effectiveness focusness information spatial fusion module effectiveness sfm give evidence effectiveness sfm compare baseline network adding sfm signi cant improvement visually observed shown numerically sfm reduces mae performance nearly datasets conduct investigation provide internal inspection sfm gradual improvement add feature weighting mechanism convlstm integrator gpm sfm shown consistent assertion different contribution spatial correlation different focal slice bene cial sod gpm proved able adaptively detect object different scale quantitative result tab numerically accumulative accuracy gain component effectiveness fim fim proposed higher cognitive processing visually show uence addition fim observe considerable gain reduce mae shown tab achieved logical since high level feature gradually summarized memory used guide selection low level spatial detail fim result removing scim fim may lead false positive suggests scim effectively update original input according memory oriented scene understanding may greatly bias result decoder cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid inputoutput conv skip connectionskip connectionskip connectionrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecall precision oursamuletc sctmfdfdhsdssmmcinldfpagrnpcapdnetpicanetr nettanetucfrecallrecallrecallprecisionprecisionprecisionourshfutlfsd table quantitative comparison light eld datasets best result shown boldface red green font respectively mean non deep learning mean available result type method year lf tpami mca tomm wsc cvpr dilf ijcai tip tanet mmci cvpr pca arxiv pdnet tcyb ctmf tip cdcp iccvw acsd icip nlpr eccv picanet cvpr cvpr pagrn eccv net ijcai iccv amulet iccv ucf cvpr nldf ds cvpr dhs cvpr mst cvpr bsca cvpr dsr iccv mae hfut mae lfsd mae limitation approach paper present deep learning based light eld saliency detection method deeply exploring comprehensively exploiting internal correlation focal slice demonstrate success method achieving state art datasets see work opening potential direction future study rst building big versatile dataset training validating different model present dataset training model testing model could bigger improving generalization ability model training direction developing computation cient memory cient method focal stack employed training process present rst deep learning based method light eld saliency detection lightweight model could potentially bene light eld data comparison state art compare result method one containing deep learning based method non deep learning one remarked light eld method lf mca wsc dilf rgb method tanet mmci pca pdnet ctmf cdcp acsd nlpr top ranking rgb method picanet pagr net amulet ucf nldf ds dhs mst bsca dsr fair comparison result competing method generated authorized code directly provided author quantitative evaluation quantitative result shown tab proposed model consistently achieves highest score datasets across evaluation metric important observation noted compared latest cnns based rgb sod method quantity training set method achieves signi cant advantage relatively small training set indicates light eld data signi cant promising show curve method outperform top ranking approach qualitative evaluation show selected representative sample result comparing method current state art method method able handle wide rage challenging scene including shown small object row similar foreground background row clutter background row dif cult scene row complex case see predicted result positively uenced light eld data proposed network light eld feature different focal slice effectively fused multi level global semantic information local detail cue suf ciently integrated visual comparison method top ranking cnns based method chal lenging case obviously model generate precise salient result even complex scene indicates method take full advantage light eld accurate saliency prediction conclusion paper develop novel memory oriented decoder tailored light eld saliency detection sfm resembles memory mechanism human fuse information effectively excavates various contribution spatial correlation different focal slice fim suf ciently integrates multi level feature leveraging high level memory guide low level selection additionally introduce scale light eld saliency dataset pave way future study experiment method achieves superior performance method including one especially complex scenario acknowledgement work supported national natural science foundation china fundamental research fund central university dut author grateful reviewer suggestion improving quality paper reference achanta hemami estrada sstrunk frequency tuned salient region detection conference computer vision pattern recognition cvpr page borji cheng jiang salient object detection survey arxiv preprint arxiv borji cheng jiang salient object detection benchmark ieee transaction processing tip borji sihite itti salient object detection benchmark european conference computer vision eccv page chen progressively complementarity aware fusion network rgb salient object detection conference computer vision pattern recognition cvpr page chen stream attention aware network rgb salient object detection ieee transaction processing tip chen multi modal fusion network multi scale multi path cross modal interaction rgb salient object detection pattern recognition cheng zhang mitra huang global contrast based salient region detection ieee transaction pattern analysis machine intelligence tpami volume page craye filliat goudou environment exploration object based visual saliency learning ieee international conference robotics automation icra page dai sun fcn object detection via region based fully convolutional network international conference neural information processing system nip page deng zhu qin han heng net recurrent residual nement network saliency detection international joint conference arti cial intelligence ijcai page ourstanetpcammcipagrnr netc simagegtpicanet lfsd pdnetamuletdssucfctmfnldf fan cheng liu borji structure measure new way evaluate foreground map international conference computer vision iccv page fan gong cao ren cheng borji enhanced alignment measure binary foreground map evaluation international joint conference arti cial intelligence ijcai page feng barnes mccarthy local background enclosure rgb salient object detection conference computer vision pattern recognition cvpr page gao mahadevan vasconcelos discriminant centersurround hypothesis bottom saliency international conference neural information processing system nip guo chen yang deep depth inference binocular monocular cue arxiv preprint arxiv han chen liu yan cnns based rgb saliency detection via cross view transfer multiview fusion ieee transaction system man cybernetics harel koch perona graph based visual saliency international conference neural information processing system nip page hariharan arbel girshick malik hypercolumns object segmentation grained localization conference computer vision pattern recognition cvpr page hong kwak han online tracking learning discriminative saliency map convolutional neural network international conference machine learning icml page hou cheng borji torr deeply supervised salient object detection short connection conference computer vision pattern recognition cvpr volume page huang liu van der maaten weinberger densely connected convolutional network conference computer vision pattern recognition cvpr page itti koch niebur model saliency based visual attention rapid scene analysis ieee transaction pattern analysis machine intelligence tpami jiang ling peng salient region detection ufo uniqueness focusness objectness international conference computer vision iccv page geng ren depth saliency based anisotropic center surround difference international conference processing icip page lee tai kim deep saliency encoded low level distance map high level feature conference computer vision pattern recognition cvpr page visual saliency based multiscale deep feature conference computer vision pattern recognition cvpr page ling saliency detection light eld conference computer vision pattern recognition cvpr page ling saliency detection light eld ieee transaction pattern analysis machine intelligence tpami zhang ruan yang saliency detection via dense sparse reconstruction international conference computer vision iccv page yang cheng liu shen contour knowledge transfer salient object detection european conference computer vision eccv page zhao wei yang zhuang ling wang deepsaliency multi task deep neural network model salient object detection ieee transaction processing tip hou koch rehg yuille secret salient object segmentation conference computer vision pattern recognition cvpr page lin milan shen reid nenet multi path nement network high resolution conference computer vision pattern recognition cvpr page semantic segmentation liu han dhsnet deep hierarchical saliency network salient object detection conference computer vision pattern recognition cvpr page liu han yang picanet learning pixel wise contextual attention saliency detection conference computer vision pattern recognition cvpr page luo mishra achkar eichel jodoin non local deep feature salient object detection conference computer vision pattern recognition cvpr page niu geng liu leveraging stereopsis saliency analysis conference computer vision pattern recognition cvpr page peng xiong rgbd salient object detection benchmark algorithm european conference computer vision eccv page perazzi henb pritch hornung saliency lters contrast based ltering salient region detection conference computer vision pattern recognition cvpr page qin wang saliency detection via cellular automaton conference computer vision pattern recognition cvpr page zhang tian tang yang rgbd salient object detection via deep fusion ieee transaction processing tip ren girshick sun faster cnn towards real time object detection region proposal network international conference neural information processing system nip volume page shao brady speci object retrieval based salient region pattern recognition shi chen wang yeung wong woo convolutional lstm network machine learning approach precipitation nowcasting international conference neural information processing system nip page simonyan zisserman deep convolutional network scale recognition international conference learning representation iclr smeulders chu cucchiara calderara dehghan shah visual tracking experimental survey ieee transaction pattern analysis machine intelligence tpami song lee depth estimation network dual defocused image different depth eld international conference processing icip page yang chien real time salient object detection minimum spanning tree conference computer vision pattern recognition cvpr page wang ruan yang deep network saliency detection via local estimation global search conference computer vision pattern recognition cvpr page wang zhu hiroaki chandraker efros ramamoorthi light eld dataset cnn architecture material recognition european conference computer vision eccv page wang lai shen ling salient object detection deep learning era depth survey arxiv preprint arxiv woo park lee kweon cbam convolutional block attention module european conference computer vision eccv page xie holistically nested edge detection international journal computer vision ijcv yeung hou chen chen chen chung light eld spatial super resolution deep cient spatial angular separable convolution ieee transaction processing tip sun weighted sparse coding framework saliency detection conference computer vision pattern recognition cvpr page zeiler fergus visualizing understanding convolutional network european conference computer vision eccv page zhang wang gao wang zhang saliency detection deeper investigation light eld international conference arti cial intelligence ijcai page zhang wang lin yang gao rui saliency detection light eld multi cue approach acm transaction multimedia computing communication application tomm zhang wang wang ruan amulet aggregating multi level convolutional feature salient object detection international conference computer vision iccv page zhang wang wang yin learning uncertain convolutional feature accurate saliency detection international conference computer vision iccv page zhang wang wang progressive attention guided recurrent network salient object detection conference computer vision pattern recognition cvpr page zhao ouyang wang saliency detection multi context deep learning conference computer vision pattern recognition cvpr page zhou liang zhang lumsdaine lin scale orientation aware epi patch learning light eld depth estimation international conference pattern recognition icpr page zhu cai huang pdnet prior model guided depth enhanced network salient object detection arxiv preprint arxiv zhu wang wang innovative salient object detection center dark channel prior international conference computer vision workshop iccvw page zhu guo wang roble kelly breaking spatio angular trade light eld super resolution via lstm modelling epipolar plane image arxiv preprint arxiv zhu liang wei sun saliency optimization robust background detection conference computer vision pattern recognition cvpr page', 'loaded dice trading bias variance order score function estimator reinforcement learning gregory farquhar university oxford shimon whiteson university oxford jakob foerster facebook research abstract gradient based method optimisation objective stochastic setting unknown intractable dynamic require estimator derivative derive objective automatic differentiation produce low variance unbiased estimator derivative order objective compatible arbitrary advantage estimator allows control bias variance order derivative function approximation furthermore propose method trade bias variance higher order derivative discounting impact distant causal dependency demonstrate correctness utility objective analytically tractable mdps meta reinforcement learning continuous control introduction stochastic setting reinforcement learning often impossible compute derivative objective depend unknown intractable distribution transition function environment case gradient based optimisation possible use stochastic gradient estimator great success domain found building estimator rst order derivative amenable automatic differentiation optimise parameter deep neural network fran ois lavet nonetheless number exciting application rst order derivative insuf cient meta learning multi agent learning often involve differentiating learning step gradient based learner finn stadie zintgraf foerster higher order optimisation method improve ciency furmston however estimating higher order derivative correctly low variance easily context automatic differentiation proven challenging foerster propose tool constructing estimator order derivative easy use avoid cumbersome manipulation otherwise required account dependency gradient estimate distribution sampled however formulation relies pure monte carlo estimate objective introducing unacceptable variance estimate rst higher order derivative limiting uptake method relying derivative meanwhile great stride made development estimator rst order derivative stochastic objective reinforcement learning use learned value function critic baseline extensively studied trade bias variance gradient estimator made explicit mixed objective combine monte carlo sample correspondence gregory farquhar conference neural information processing system neurips vancouver canada objective learned value function schulman technique create family advantage estimator used reduce variance accelerate credit assignment rst order optimisation applied full generality higher order derivative work derive objective differentiated number time produce correct estimator higher order derivative stochastic computation graph scgs markov property found sequence modeling unlike prior work objective fully compatible arbitrary choice advantage estimator approximate value function allows explicit trade offs bias variance order derivative estimate made known technique future advantage estimation method designed rst order derivative furthermore propose method trading bias variance higher order derivative discounting impact distant causal dependency empirically rst use small random mdps admit analytic solution estimator unbiased low variance perfect value function bias variance may exibly traded hyperparameters study objective challenging meta reinforcement learning problem simulated continuous control impact various parameter choice training demonstration code available http github com oxwhirl loaded dice handful additional line code needed implement objective existing codebase us higher order derivative background gradient estimator commonly faced objective form expectation random variable order calculate gradient expectation respect parameter interest must often employ gradient estimator gradient cannot computed exactly example reinforcement learning environment dynamic unknown form part objective expected return polyonymous likelihood ratio score function reinforce estimator given log expectation rh may estimated monte carlo sample drawn often independent second term dropped depends random variable may reparameterised depend deterministically may instead drop rst term see mohamed comprehensive review stochastic computation graph mdps stochastic computation graph scgs directed acyclic graph node determinsitic stochastic function edge indicate functional dependency schulman gradient estimator described may used estimate gradient objective sum cost node respect parameter schulman propose surrogate loss single objective produce desired gradient estimate differentiation weber apply advanced rst order gradient estimator scgs formalise markov property scgs allow exible powerful estimator originally developed context reinforcement learning applied describe estimator following subsection rst relevant subset scgs keep main body paper simple highlight important known use case method adopt notation reinforcement learning rather cumbersome notation generic scgs graph reinforcement learning describes markov decision process mdp begin initial state time timestep action sampled stochastic policy parameterised map state action add stochastic node graph state action pair lead reward next state process continues simple mdp graph shown gure many problem reward condition state rather state action consider episodic problem terminate step although result may extended non terminating case discounted reward cost node graph leading familiar reinforcement learning objective example scgs support new objective left right vanilla mdp mdp stochastic latent goal variable pomdp expected discounted sum reward cid trt expectation taken respect policy well unknown transition dynamic underlying mdp generalisation result hold slightly general class scgs well whose objective still sum reward time may number stochastic deterministic node corresponding timestep however node may uence future reward uence next timestep formally markov property state node exists directed path cid cid blocked none descendant nition weber class scgs capture broad class mdp like model gradient estimator advantage value function set node scg expectation objective stochas tic variable excluding set node reduce variance serving control variate baseline critic condition sampled value taken corresponding stochastic node sampled action difference critic baseline value function known advantage replaces sampled cost gradient estimator baseline value function affect variance gradient estimator weaver tao however learned imperfect critic value function result biased gradient estimator may trade bias variance different mixture sampled cost unbiased high variance learned critic value function biased low variance choice advantage estimator hyperparameters used tune bias variance resulting gradient estimator suit problem hand many way model advantage function popular simple family advantage estimator proposed schulman agae cid cid cid cid cid cid cid cid parameter trade bias variance formed sampled reward unbiased high variance agae us next sampled reward relies heavily estimated value function reducing variance cost bias higher order estimator construct higher order gradient estimator may recursively apply technique treating gradient estimate objective new scg foerster note several shortcoming surrogate loss approach schulman higher order derivative surrogate loss cannot differentiated produce correct higher order estimator even estimate produced surrogate loss cannot treated objective new scg surrogate loss severs dependency sampled cost sampling distribution address foerster introduce dice single objective may differentiated repeatedly automatic differentiation produce unbiased estimator derivative order dice objective reinforcement learning given cid cid indicates set stochastic node action occurring timestep earlier special operator act set stochastic node always evaluates special behaviour differentiation log cid zero derivative operator effect automates likelihood ratio trick differentiation expectation maintaining dependency trick applied computing higher order derivative notational convenience later derivation extend nition slightly ning operation empty set original version dice critical drawback compared state art method described estimating rst order derivative stochastic objective first mecha nism baseline reduce variance estimator higher order derivative mao liu subsequently independently suggest partial solution problem neither provide proof unbiasedness estimator beyond second order second dice estimator mao liu formulated way requires use monte carlo sampled cost without form permit use critic value function way make use full range possible advantage estimator exact calculation higher order derivative estimator dependence given reward previous action lead nested sum previous timesteps term tend high variance estimated data become small vicinity local optimum noted furmston rothfuss use observation propose simpli version dice objective dropping dependency jlv estimator biased higher rst order derivative rothfuss derive correct unbiased estimator order make use advantage estimation objective extend applicability beyond meta learning style maml finn next section introduce new objective may make use critic well baseline value function thereby allows bias variance order derivative traded choice advantage estimator furthermore introduce discounting past dependency allows smooth trade bias variance due high variance term identi furmston method dice objective cast sum reward dependency reward node stochastic cause captured use critic value function hand must use forward looking sum return possible graph maintains markov property ned section respect objective permit sequential decomposition cost node reward stochastic cause uenced action begin dice objective discounted sum reward given true objective expected discounted sum reward trajectory drawn policy cid cid trt cid simply take change variable cid second term relabeling dummy variable immediately back cid cid typical return cid cid cid cid cid cid cid cid cid cid cid last line used objective formulated term forward looking return capture depen since expression dencies sampling distribution dice objective applied restricted class scgs requisite markov property still guaranteed derivative unbiased estimator derivative true objective order proof original dice objective given foerster carry derivative omit following estimator clarity including however ensures convenient property objective still evaluates expectation true return introduce value function conditionally independent well derivative conditioned markov property scg equivalent conditional independence given consider expectation new form use conditional independence push expectation onto complete derivation please see supplementary material simply critic value function ned always evaluates zero cid cid cid cid cid cid cid cid cid cid furthermore baseline depend change expectation estimator shown standard derivation reproduced schulman reinforcement learning common use expected state value eat approximation optimal baseline estimator may use place reducing variance derived estimator term advantage recovers unbiased estimate derivative order cid cid cid practice common omit thus optimising undiscounted return still discounted advantage variance reduction tool see discussion thomas function approximation practice estimate advantage must made limited data inexact model critic value function due limited data model class misspeci cation inef cient learning introduce bias gradient estimator work schulman may use combination sampled cost estimated value form advantage estimator trade bias variance however thanks new estimator capture full dependency advantage sampling distribution trade offs may immediately applied higher order derivative approximate baseline value function affect estimator variance careful choice baseline may nonetheless great signi cance exploiting factorisation policy foerster formulation objective extends method well future advance advantage estimation rst order higher order derivative variance due higher order dependency correct form unbiased estimator us proper variance reduction strategy computing advantage may trade bias variance estimate higher order derivative arises due full history causal dependency particular propose set discount factor prior dependency limit horizon past action accounted estimate higher order derivative similarly way mdp discount factor reduces variance constraining horizon future must considered constrains far past consider causal dependency uence higher order derivative first note acting set stochastic node decomposes product implement discounting exponentially decaying past contribution cid cid cid cid cid cid cid cid cid cid cid cid may computed nal objective call loaded dice product log space action probability transforming convenient numerically stable sum algorithm show objective may easily computed episode algorithm compute loaded dice objective require trajectory state action log log deps deps end return cid accumulates nal objective cid accumulates weighted stochastic dependency cid dependency including cid dependency excluding operator log probability cid dependency weighted advantage cid applies function end function return exp stop gradient estimator resembles jlv although make use advantage may low variance biased regardless choice advantage estimator recover estimator unbiased advantage estimator unbiased intermediate value able trade bias variance demonstrate empirically section new form objective allows use reduce impact high variance term identi furmston rothfuss smooth way rather completely dropping term convergence increasing batch size unbiased order estimator dice dice baseline mao loaded dice lvc rothfuss low variance biased estimator experiment section empirically demonstrate correctness estimator absence function approximation bias variance estimator may traded choice advantage estimator approximate value function available use novel discount factor bias variance order derivative make initial analysis simple interpretable use small random mdps state action per state reward depend state mdps discounted value may calculated analytically follows state transition matrix induced mdp transition function cid tabular policy element given cid cid let initial state distribution vector probability distribution state time vector pst mean reward time pst vector per state reward finally cid cid cid trt differentiable wrt may easily computed automatic differentiation package detail code found supplementary material low variance unbiased order estimator show correlation estimated true derivative change function batch size third order compare original dice estimator loaded dice objective proposed mao incorporates baseline loaded dice use agae exact value function remain unbiased unbiased estimator converge true derivative suf ciently batch size however advantage estimator exact value function variance may dramatically reduced estimate converge much rapidly performance lvc rothfuss rst order match exactly estimator mao underperforms loaded dice use advantage higher order low variance biased expected low produce low variance estimate cost high bias effect hold order deriva tives high considers full past produce low bias high variance estimator low discount past first order gradient unaffected trading bias variance small mdp trading bias variance advantage estimation show bias standard deviation estimated derivative range inexact value function perturb true value function gaussian noise state emulate function approximation effect choice advantage estimator trade bias variance rst order order derivative trading bias variance discounting cause show bias standard deviation estimated derivative range isolate effect use exact value function absolute bias variance lower gure first order derivative unaffected expected however higher order derivative strongly affect bias variance resulting estimator outlier third order derivative guarantee monotonicity bias variance found outlier rarer second third order appearing artefact particular mdps meta reinforcement learning maml loaded dice apply new family estimator pair challenging meta reinforcement learning problem continuous control following work finn aim model agnostic meta learning maml learn good initialisation neural network policy single small number policy gradient update batch training episode policy achieves good performance task sampled distribution meta testing policy able adapt new task distribution maml theoretically sound original implementation neglected higher order dependency induced setting rothfuss stadie approach number task adapt policy inner loop policy gradient optimisation step outer loop initial parameter updated maximise return post adaptation policy outer loop optimisation depends post adaptation parameter depend gradient estimated inner loop important higher order term outer loop optimisation correct estimator inner loop optimisation therefore impact ciency overall meta training procedure well quality nal solution inner loop optimisation use novel objective range value sweep range xed sweep range best value found outer loop optimisation use vanilla policy gradient baseline outer loop could use gradient based policy optimisation algorithm choose simple version isolate extent impact inner loop estimator show result cheetahdir task high estimator high variance performance bad le impactful cheetahvel task note task episode short low value function simple linear function batch data finn factor would favor high higher variance return better value function relying heavily learned value function lower may effective trading bias variance meta reinforcement learning report mean standard error run post adaptation return smoothed moving average outer loop optimisation environment lead high variance unbiased version objective may valuable value function better used effectively mitigate variance cheetahvel noticeably faster learning achieved low non zero analysis furmston indicates magnitude higher order term discounted many case become small policy approach local optimum consistent empirical nding non zero may learn faster plateau similar level appendix show result antvel task important factor conclude loaded dice provides meaningful control higher order estimator signi cant impact realistic use case conclusion work derived theoretically sound objective apply general advantage function estimation order derivative reinforcement learning type sequential problem context function approximation objective unlocks ability trade bias variance higher order derivative importantly like underlying dice objective single objective generates estimator order derivative repeated automatic differentiation propose simple method discounting impact distant causal dependency estimation higher order derivative allows another axis trade bias variance empirically use small random mdps demonstrate behaviour bias variance higher order derivative estimate utility meta reinforcement learning excited application meta learning multi agent learning higher order optimi sation may made possible new objective future work wish revisit choice discounting heuristic method limit impact high variance term theoretical analysis may help identify context higher order dependency important optimisation finally may even possible meta learn hyperparameters acknowledgment thank maruan shedivat minqi jiang valuable discussion work supported epsrc cdt autonomous intelligent machine system project received funding european research council erc european union horizon research innovation programme grant agreement number reference chelsea finn pieter abbeel sergey levine model agnostic meta learning fast adaptation deep network proceeding international conference machine learning volume page jmlr org jakob foerster richard chen maruan shedivat shimon whiteson pieter abbeel igor mordatch proceeding international conference learning opponent learning awareness autonomous agent multiagent system page international foundation autonomous agent multiagent system jakob foerster gregory farquhar maruan shedivat tim rockt schel eric xing shimon whiteson dice nitely differentiable monte carlo estimator international conference machine learning page jakob foerster gregory farquhar triantafyllos afouras nantas nardelli shimon whiteson counterfac tual multi agent policy gradient thirty second aaai conference arti cial intelligence vincent fran ois lavet peter henderson riashat islam marc bellemare joelle pineau introduction deep reinforcement learning foundation trend cid machine learning michael gradient estimation handbook operation research management science thomas furmston guy lever david barber approximate newton method policy search markov decision process journal machine learning research hao liu richard socher caiming xiong taming maml cient unbiased meta reinforcement learning international conference machine learning page jingkai mao jakob foerster tim rockt schel maruan shedivat gregory farquhar shimon whiteson baseline order gradient estimation stochastic computation graph international conference machine learning page shakir mohamed mihaela rosca michael figurnov andriy mnih monte carlo gradient estimation machine learning arxiv preprint arxiv jonas rothfuss dennis lee ignasi clavera tamim asfour pieter abbeel promp proximal meta policy search arxiv preprint arxiv john schulman nicolas heess theophane weber pieter abbeel gradient estimation stochastic computation graph advance neural information processing system page john schulman philipp moritz sergey levine michael jordan pieter abbeel high dimensional continuous control generalized advantage estimation arxiv preprint arxiv bradly stadie yang rein houthooft chen yan duan yuhuai pieter abbeel ilya sutskever consideration learning explore via meta reinforcement learning arxiv preprint arxiv philip thomas bias natural actor critic algorithm international conference machine learning page lex weaver nigel tao optimal reward baseline gradient based reinforcement learning proceeding seventeenth conference uncertainty arti cial intelligence page morgan kaufmann publisher inc ophane weber nicolas heess lars buesing david silver credit assignment technique stochastic computation graph arxiv preprint arxiv luisa zintgraf kyriacos shiarlis vitaly kurin katja hofmann shimon whiteson caml fast context adaptation via meta learning international conference machine learning derivation value function formulation start cid objective cid cid cid cid evaluate objective taking expectation trajectory induced policy complete sequence state action reward convenience following derivation ned reward indexed next time step action taken ensures partial trajectory correctly keep reward action cause note krt depends expectation objective given cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid tjt cid note time step term form cid cid next use last step used markov property substituting obtain substitute back obtain cid cid cid cid cid cid cid cid cid cid cid cid cid putting together obtain nal form cid cid cid cid cid cid experimental detail random mdps use mdptoolbox example rand function pymdptoolbox generate random mdp transition function state action per state reward function state sampled use sampling stochastic estimator use batch rollouts length step unless batch size otherwise speci compute higher order derivative derivative rst parameter order save computa tion sweep use batch value simulate function approximation error analysis impact add gaussian noise standard deviation true value function maml experiment use following hyperparameters maml experiment parameter hidden layer size number layer task batch size meta batch size inner loop learning rate outer loop optimiser outer loop learning rate outer loop reward noise value trajectory task adam uniform timestep normalise advantage batch per task show additional experiment antvel mujoco task domain important factor trading bias variance ant velocity task report mean standard error run post adaptation return smoothed moving average outer loop optimisation', 'cortical microcircuit gated recurrent neural network rui ponte costa yannis assael centre neural circuit behaviour dept computer science dept physiology anatomy genetics university oxford oxford university oxford oxford rui costa cncb deepmind london yannis assael brendan shillingford dept computer science university oxford oxford deepmind london brendan shillingford nando freitas deepmind london nandodefreitas google com tim vogels centre neural circuit behaviour dept physiology anatomy genetics university oxford oxford tim vogels cncb abstract cortical circuit exhibit intricate recurrent architecture remarkably similar across different brain area stereotyped structure suggests existence common computational principle however principle remained largely elusive inspired gated memory network namely long short term memory network lstms introduce recurrent neural network information gated inhibitory cell subtractive sublstm propose natural mapping sublstms onto known canonical excitatory inhibitory cortical microcircuit empirical evaluation across sequential classi cation language modelling task show sublstm unit achieve similar performance lstm unit result suggest cortical circuit optimised solve complex contextual problem proposes novel view computational function overall work provides step towards unifying recurrent network used machine learning biological counterpart introduction last decade neuroscience research collected enormous amount data chitecture dynamic cortical circuit unveiling complex stereotypical structure across neocortex markram harris mrsic flogel jiang prevalent feature cortical net laminar organisation high degree recurrence even level local micro circuit douglas song harris mrsic flogel jiang another key feature cortical circuit detailed tight balance excitation inhibition received growing support author contributed equally work conference neural information processing system nip long beach usa experimental froemke xue froemke theoretical level van vreeswijk sompolinsky brunel vogels abbott hennequin however computational process facilitated architecture dynamic still elusive remains fundamental disconnect underlying biophysical network emergence intelligent complex behaviour arti cial recurrent neural network rnns hand crafted perform speci computation fact rnns recently proven successful solving complex task language modelling speech recognition perceptual task graf graf sutskever van den oord assael task input data contains information across multiple timescales need ltered processed according relevance ongoing presentation stimulus make dif cult learn separate meaningful stimulus background noise hochreiter pascanu rnns particular gated rnns solve problem maintaining representation relevant input sequence needed without interference new stimulus principle protected memory conserve past input thus allow back propagation error backwards time pascanu memory property rst successful type gated rnns named long short term memory network lstms hochreiter schmidhuber note architectural feature lstms overlap closely known cortical structure important difference regard mechanistic implementation gate cortical network lstms lstms gate control memory cell multiplicative factor biological network gate inhibitory neuron act rst approximation subtractively excitatory inhibitory current cancel linearly level postsynaptic membrane potential kandel gerstner moreover subtractive inhibitory mechanism must well balanced closely match excitatory input act gate input closed state without perturbing activity much inhibition previous model explored gating subtractive excitatory inhibitory balanced network vogels abbott kremkow without clear computational role hand predictive coding rnns feature studied bastos deneve machens without clear match state art machine learning network regarding previous neuroscienti interpretation lstms suggestion lstms model working memory different brain area prefrontal cortex basal ganglion hippocampus reilly frank krueger dayan cox dean marblestone hassabis bhalla without clear interpretation individual component lstms speci mapping known circuit propose map architecture function lstms directly onto cortical circuit gating provided lateral subtractive inhibition network potential exhibit excitation inhibition balance observed experiment douglas bastos harris mrsic flogel yield simpler gradient propagation multiplicative gating study dynamic empirical evaluation showing sublstms achieve similar performance lstms penn treebank wikitext language modelling task well pixelwise sequential mnist classi cation transferring functionality lstms biologically plausible network work provides testable hypothesis recently emerging technologically advanced experiment functionality entire cortical microcircuit biological motivation architecture lstm unit general feedforward structure aided additional recurrent memory controlled lateral gate remarkably similar columnar architecture cortical circuit see detailed neocortical schematic central element lstms similar rnns memory cell hypothesise implemented local recurrent network pyramidal cell layer line previous study showing relatively high level recurrence non random connectivity pyramidal cell layer douglas thomson song furthermore layer pyramidal network display rich activity relatively long time scale vivo barth sakata harris luczak van kerkoerle slice egorov wang consistent lstm like function strong evidence persistent neuronal activity higher cortical area goldman rakic sensory area huang van kerkoerle kornblith relatively speaking sensory area visual cortex exhibit sorter timescales higher brain area prefrontal cortex would expect given different temporal requirement brain area similar behaviour expected multi area layer lstms note longer time scale present super cial layer layer goldman rakic van kerkoerle suggesting possibility memory cell per cortical microcircuit slow memory decay network may controlled short york van rossum costa long term synaptic plasticity abbott nelson senn ster gerstner zenke costa recurrent excitatory synapsis gate protect given memory lstms mapped onto lateral inhibitory input cortical circuit propose similar lstms input gate implemented inhibitory neuron layer layer lateral inhibition consistent canonical view microcircuit douglas bastos harris mrsic flogel sparse sensory evoked response layer sakata harris harris mrsic flogel brain inhibition believed originate parvalbumin basket cell providing near exact balanced inhibitory counter signal given excitatory feedforward input froemke xue froemke excitatory inhibitory input thus cancel arriving signal ignored default consequently activity within downstream memory network remains largely unperturbed unless altered targeted modulation inhibitory activity harris mrsic flogel vogels abbott letzkus similarly memory cell affect output lstm activity unaccompanied congruent inhibition mapped onto layer layer layer microcircuit known project higher brain area harris mrsic flogel see lateral inhibition turned gate open subtractive neural integration presynaptic cell re neurotransmitter released synaptic terminal neurotrans mitter subsequently bound postsynaptic receptor prompt structural change ion channel allow electrically charged ion postsynaptic cell depending receptor type ion either increase depolarise decrease hyperpolarise postsynaptic membrane potential suf ciently depolarising excitatory input provided postsynaptic potential reach threshold stereotyped action potential spike kandel behaviour formalised circuit resistance capacitance follows ohm law yield standard leaky integrate neuron model gerstner kistler riexc riinh membrane time constant iexc iinh excitatory inhibitory hyperpolarizing synaptic input current respectively action potential initiated standard model brette gerstner gerstner membrane potential hit hard threshold modelled momentary pulse subsequent reset resting potential neuronal excitation inhibition opposite effect inhibitory input act linearly subtractively membrane potential leaky integrate model approximated level ring rate rate cid iexc iinh iexc iinh cid see input output response gerstner kistler used demonstrate impact subtractive gating contrast multi plicative gating ring rate approximation form basis gated rnn model similar subtrac tive behaviour input output function bottom moreover rate formulation allows cleaner comparison lstm unit use existing machine learning optimisation method could argued different form inhibition shunting inhibition counteracts excitatory input decreasing membrane resistance characteristic multiplicative gating effect membrane potential however analysed level output ring rate effect becomes subtractive holt koch prescott koninck consistent biological arti cial gated recurrent neural network example unit simpli cortical recurrent neural network sensory downstream input arrives pyramidal cell layer layer fed onto memory cell recurrently connected pyramidal cell layer memory decay decay time constant input onto layer balanced inhibitory basket cell balance represented diagonal equal connection output memory cell gated basket cell layer within area upstream brain area implementation following similar notation lstm unit input output subtractive gate dashed connection represent potential balance excitatory inhibitory input weight set lstm recurrent neural network cell see main text detail plot bellow illustrate different gating mode simple current based noisy leaky integrate neuron capped subtractive inhibition sigmoidal activation function subtractive gating sigmoidal activation function multiplicative gating output rate represents number spike per second biological circuit approach model framed ring rate level rather level membrane potential subtractive gated long short term memory lstm unit hochreiter schmidhuber greff access memory cell controlled input gate see time forget gate control decay memory output gate control whether content memory cell transmitted rest network lstm network consists many lstm unit containing memory cell input forget output gate lstm state described unit follows dynamic given middle column note leak controlled input recurrent unit may biologically unrealistic input output cellctxt itotztxt ftxt input output cellctxt itot ztfxt inputunit junit junit joutputl inpc inl pcsmemorycellfpclayer abcsublstmlstmcortical circuit output rate input baselineweak inh strong inh inh exc inh exc closed gatebaselineweak inh strong inh baselinestrong gatesubtractive gatingmultiplicative gatingweak gate lstm sublstm rht rht tanh rht rht tanh memory cell note multiplicative control input gate denotes element wise multiplication new weighted input given input vector recurrent input lstm unit respectively overall output lstm unit computed lstm network multiple layer million parameter weight bias typically trained stochastic gradient descent supervised setting parameter multiple gate allow network adapt information depending task hand particular enable writing memory cell controlled input gate adjusting timescale memory controlled forget gate exposing memory network controlled output gate combined effect gate make possible lstm unit capture temporal contextual dependency across multiple timescales introduce study new rnn unit sublstm sublstm unit mapping lstms onto known canonical excitatory inhibitory cortical microcircuit douglas song harris mrsic flogel similarly sublstms ned however gating subtractive rather multiplicative sublstm ned memory cell transformed input input gate model use simpli notion balance gating jth unit memory forgetting consider option controlled gate lstm unit rht biologically plausible learned simple decay referred result sublstm similarly input sublstm output gated subtractive output gate see equation evaluated different activation function sigmoidal transformation highest performance key difference gated rnns subtractive inhibitory gating potential balanced excitatory input respectively see detailed comparison different gating mode subtractive versus multiplicative gating rnns key difference sublstms lstms lie implementation gating mech anism lstms typically use multiplicative factor control amplitude input signal sublstms use biologically plausible interaction excitation inhibition important consequence subtractive gating potential improved gradient backwards towards input layer illustrate compare gradient sublstms lstms simple example first review derivative loss respect various component sublstm notation based greff notation represents derivative loss note consider version sublstms forget gate lstms sublstm another simple memory decay scalar ne memory timeconstant sublstm weight could optimised model decided keep number parameter minimum simplicity ease comparison lstms respect def dloss dht error layer chain rule comparison corresponding derivative lstm unit given tanh tanh tanh sigmoid activation function overlined variable etc pre activation value gate input transformation woxt roht output gate sublstm note compared lstm sublstms provide simpler gradient fewer multiplicative factor lstms weight input transformation updated according total number temporal step ellipsis abbreviates recurrent gradient path time containing path backwards time via simplicity analysis ignore recurrent connection lstm sublstm consider depth wise path network call tth timestep depth contribution derivative lstm slight abuse notation tanh input gate cid output gate tanh cid tanh derivative tanh notice either input output gate set zero closed corresponding contribution gradient zero network subtractive gating depth derivative contribution becomes cid cid sigmoid derivative case input output gate present subtractive gate sublstms directly impair error propagation result aim work fold first inspired cortical circuit aimed propose biological plausible implementation lstm unit would allow better understand cortical architecture dynamic compare performance sublstm unit lstms rst compared learning dynamic subtractive multiplicative network mathematically second step empirically compared sublstm sublstm lstm network task sequential mnist classi cation word level language modelling penn treebank marcus wikitext merity network weight initialised glorot initialisation glorot bengio lstm unit initial forget gate bias selected number unit sublstm number parameter held constant across experiment facilitate fair comparison lstms sublstms sequential mnist sequential mnist digit classi cation task digit mnist dataset presented rnn sequence pixel decompose mnist image pixel sequence step network optimised rmsprop momentum tieleman hinton learning rate hidden layer hidden unit result sublstms achieves similar result lstms result comparable previous result task rnns comparison lstm sublstm network sequential pixel pixel mnist hidden unit sample mnist dataset converted matrix pixel temporal sequence timesteps classi cation accuracy test set sublstm xed learned forget gate language modelling language modelling represents challenging task rnns short long term dependency rnn language model rnn lm model probability text autoregressively predicting sequence word timestep trained predict following word word model word sequence product conditional multinoulli distribution evaluate rnn lm measuring perplexity ned sequence word perplexity rst used penn treebank ptb dataset train model word level language modelling training validation test word vocabulary word rnns tested hidden layer backpropagation truncated step batch size optimise network used rmsprop momentum performed hyperparameter search validation set input output update dropout rate learning rate weight decay hyperparameter search done google vizier performs black box optimisation gaussian process bandit transfer learning table resulting hyperparameters table report perplexity test set golovin understand sublstms scale network size varied number hidden unit tested wikitext language modelling dataset based wikipedia article dataset twice ptb dataset training validation test word lstmsublstmfix sublstm testing accuracy seq feature larger vocabulary word therefore well suited evaluate model performance longer term dependency reduces likelihood tting datasets result sublstms achieve perplexity similar lstms table interestingly biological plausible version sublstm simple decay forget gate achieves performance similar better sublstms penn treebank ptb test perplexity wikitext test perplexity size sublstm sublstm lstm size sublstm sublstm lstm table language modelling word level test set perplexity penn treebank wikitext model layer sublstm us xed learned forget gate unit number unit sublstm chosen number parameter sub lstm facilitate fair comparison size indicates number unit number hidden unit sublstm selected number parameter lstm sublstm facilitating fair comparison conclusion future work cortical microcircuit exhibit complex stereotypical network architecture support rich dynamic computational power dynamic yet properly understood known excitatory inhibitory neuron type interact closely process sensory information great accuracy making sense interaction beyond scope contemporary experimental approach lstms hand well understood powerful tool contextual task structure map intriguingly well onto stereotyped connectivity cortical circuit analysed biologically constrained lstms sublstms could perform similarly well indeed model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table penn treebank hyperparameters model hidden unit input dropout output dropout update dropout lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm lstm sublstm sublstm learning rate weight decay table wikitext hyperparameters subtractively gated excitation inhibition recurrent neural network promise compared lstms benchmark sequence classi cation word level language modelling notable sublstms could outperform traditional counterpart yet hope work serve platform discus develop idea cortical function establish link relevant experimental work role excitatory inhibitory neuron contextual learning froemke froemke poort pakan kuchibhotla future work interesting study additional biological detail may affect performance next step aim include dale principle given neuron make either excitatory inhibitory connection stratum harvey naturally focus perplexing diversity inhibitory cell type markram behaviour shunting inhibition mixed subtractive divisive control doiron mejias boustani sur seybold overall given success multiplicative gated lstms insightful understand biological trick cortical network may give lstms performance boost acknowledgement would like thank everton agnes glar ehre gabor melis jake stroud helpful comment discussion supported sir henry dale fellowship wellcome trust royal society supported epsrc research council rcuk supported clarendon fund reference abbott nelson synaptic plasticity taming beast nature neuroscience assael shillingford whiteson freitas lipnet sentence level lipreading arxiv preprint arxiv barth curto luczak marguet harris population coding tone stimulus auditory cortex dynamic rate vector analysis european journal neuroscience bastos usrey adam mangun fry friston canonical microcircuit predictive coding neuron bhalla dendrite deep learning sequence hippocampus hippocampus although focus comparison lstms similar point would apply gated rnns gated recurrent unit chung brette gerstner adaptive exponential integrate model effective description neuronal activity journal neurophysiology brunel dynamic sparsely connected network excitatory inhibitory spiking neuron journal computational neuroscience chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling arxiv org costa froemke sjostrom van rossum uni pre postsynaptic long term plasticity enables reliable exible learning elife costa mizusaki sjostrom van rossum functional consequence pre postsynaptic expression synaptic plasticity philosophical transaction royal society london series biological science costa padamsey amour emptage froemke vogels synaptic transmission optimization predicts expression locus long term plasticity neuron costa sjostrom van rossum probabilistic inference short term synaptic plasticity neocortical microcircuit frontier computational neuroscience cox dean neural network neuroscience inspired computer vision current biology deneve machens cient code balanced network nature neuroscience doiron longtin berman maler subtractive divisive inhibition effect voltage dependent inhibitory conductance noise neural computation douglas koch mahowald martin suarez recurrent excitation neocortical circuit science douglas martin whitteridge canonical microcircuit neocortex neural computation egorov hamam frans hasselmo alonso graded persistent activity entorhinal cortex neuron nature boustani sur response dependent dynamic cell speci inhibition cortical network vivo nature communication froemke plasticity cortical excitatory inhibitory balance annual review neuroscience froemke merzenich schreiner synaptic memory trace cortical receptive eld plasticity nature gerstner kistler spiking neuron model single neuron population plasticity cambridge university press gerstner kistler naud paninski neuronal dynamic single neuron network model cognition cambridge university press glorot bengio understanding dif culty training deep feedforward neural network proceeding thirteenth international conference arti cial intelligence statistic page goldman rakic cellular basis working memory neuron golovin solnik moitra kochanski karro sculley google vizier service black box optimization proceeding acm sigkdd international conference knowledge discovery data mining page acm graf generating sequence recurrent neural network arxiv org graf mohamed hinton speech recognition deep recurrent neural network arxiv preprint arxiv greff srivastava koutn steunebrink schmidhuber lstm search space odyssey arxiv org harris mrsic flogel cortical connectivity sensory coding nature hassabis kumaran summer eld botvinick neuroscience inspired arti cial intelligence neuron hennequin agnes vogels inhibitory plasticity balance control codependence annual review neuroscience hennequin vogels gerstner optimal control transient dynamic balanced network support generation complex movement neuron hochreiter bengio frasconi schmidhuber gradient recurrent net dif culty learning long term dependency hochreiter schmidhuber long short term memory neural computation holt koch shunting inhibition divisive effect ring rate neural computation huang matysiak heil nig brosch king persistent neural activity auditory cortex related auditory working memory human nonhuman primate elife jiang shen cadwell berens sinz ecker patel tolias principle connectivity among morphologically ned cell type adult neocortex science aac aac kandel schwartz jessell siegelbaum principle neural science kornblith quiroga koch fried mormann persistent single neuron activity working memory human medial temporal lobe current biology kremkow aertsen kumar gating signal propagation spiking neural network balanced correlated excitation inhibition journal neuroscience krueger dayan flexible shaping learning small step help cognition kuchibhotla gill lindsay papadoyannis field sten miller froemke parallel processing cortical inhibition enables context dependent behavior nature neuroscience jaitly hinton simple way initialize recurrent network rectus linear unit arxiv org letzkus wolff thi disinhibition circuit mechanism associative learning memory neuron luczak mcnaughton harris packet based communication cortex nature review neuroscience marblestone wayne kording toward integration deep learning neuroscience frontier computational neuroscience marcus marcinkiewicz santorini building annotated corpus english penn treebank computational linguistics markram toledo rodriguez wang gupta silberberg interneurons neocortical inhibitory system nature review neuroscience mejias kappen longtin torres short term synaptic plasticity heterogeneity neural system merity xiong bradbury socher pointer sentinel mixture model arxiv org reilly frank making working memory work computational model learning prefrontal cortex basal ganglion neural computation pakan lowe dylda keemink currie coutts rochefort mrsic flogel behavioral state modulation inhibition context dependent cell type speci mouse visual cortex elife pascanu mikolov bengio dif culty training recurrent neural network arxiv org ster gerstner triplet spike model spike timing dependent plasticity journal neuroscience poort khan pachitariu nemri orsolic krupic bauza sahani keller mrsic flogel hofer learning enhances sensory multiple non sensory representation primary visual cortex neuron prescott koninck gain control ring rate shunting inhibition role synaptic noise dendritic saturation proc natl acad sci usa sakata harris laminar structure spontaneous sensory evoked population activity auditory cortex neuron senn markram tsodyks algorithm modifying neurotransmitter release probability based pre postsynaptic spike timing neural computation seybold phillips schreiner hasenstaub inhibitory action uni network integration neuron song str reigl nelson chklovskii highly nonrandom feature synaptic connectivity local cortical circuit plo biology stratum harvey dale principle brain research bulletin sutskever vinyals sequence sequence learning neural network arxiv org thomson west wang bannister synaptic connection small circuit involving excitatory inhibitory neuron layer adult rat cat neocortex triple intracellular recording biocytin labelling vitro cerebral cortex new york tieleman hinton lecture rmsprop divide gradient running average recent magnitude coursera neural network machine learning van den oord kalchbrenner vinyals espeholt graf kavukcuoglu conditional generation pixelcnn decoder arxiv org van kerkoerle self roelfsema layer speci city effect attention working memory activity primary visual cortex nature communication van vreeswijk sompolinsky chaos neuronal network balanced excitatory inhibitory activity science vogels abbott gating multiple signal detailed balance excitation inhibition spiking network nature neuroscience wang markram goodman berger goldman rakic heterogeneity pyramidal network medial prefrontal cortex nature publishing group xue atallah scanziani equalizing excitation inhibition ratio across visual cortical neuron nature york van rossum recurrent network short term synaptic depression journal computational neuroscience zenke agnes gerstner diverse synaptic plasticity mechanism orchestrated form retrieve memory spiking neural network nature communication', 'unsupervised learning view invariant action representation grad school integrative science engineering national university singapore national university singapore yongkang wong school computing singapore lijunnan nu edu yongkang wong nu edu junnan singapore zhao dept computer science engineering university minnesota minneapolis usa qzhao umn edu mohan kankanhalli school computing national university singapore singapore mohan comp nu edu abstract recent success human action recognition deep learning method mostly adopt supervised learning paradigm requires signi cant amount man ually labeled data achieve good performance however label collection expensive time consuming process work propose unsupervised learning framework exploit unlabeled data learn video representation different previous work video representation learning unsupervised learning task predict motion multiple target view video repre sentation source view learning extrapolate cross view motion representation capture view invariant motion dynamic discriminative action addition propose view adversarial training method enhance learning view invariant feature demonstrate effectiveness learned representation action recognition multiple datasets introduction recognizing human action video long standing research problem computer vision past year convolutional neural network cnns recurrent neural network rnns emerged state art learning framework action recognition however success existing supervised learning method primarily driven signi cant amount manually labeled data expensive time consuming collect tackle problem stream unsupervised method recently proposed leverage free unlabeled data representation learning key idea design surrogate task exploit inherent structure raw video formulate loss function train network work design surrogate task constructing future frame future motion others use temporal order video frame learn representation self supervised manner although promising result learned representation often view speci make le robust view change generally human action observed multiple view action appears quite different therefore important learn discriminative view invariant feature especially action recognition unknown unseen view human ability visualize conference neural information processing system neurips montr canada action look like different view human brain build view invariant action representation immediately hypothesize enabling deep network ability extrapolate action across different view encourage learn view invariant representation work propose unsupervised learning framework task construct motion multiple target view video representation source view argue order network infer cross view motion dynamic learned representation reside view invariant discriminative space action recognition view invariant representation learning cross view action recognition widely studied however existing method require access human pose information training others compromise discriminative power achieve view invariance focus inferring motion rather tracking body keypoints space time method learns recurrent encoder extract motion dynamic insensitive viewpoint change represent motion calculated rgb data contribution work follows propose unsupervised framework effectively learn view invariant video representation predict motion sequence multiple view learned representation extracted cnn rnn based encoder decoded multiple sequence ows cnn decoder framework trained jointly minimizing several loss propose view adversarial training encourage view invariant feature learning video different view mapped shared subspace view classi cannot discriminate shared representation enforced contain meaningful motion information use decoder demonstrate effectiveness learned representation cross subject cross view action recognition task experiment various input modality including rgb depth method outperforms state art unsupervised method across multiple datasets related work unsupervised representation learning deep network shown dominant performance various computer vision task fully supervised training paradigm requires vast amount human labeled data inherent limitation highlight importance unsupervised learning leverage unlabeled data learn feature representation past year unsupervised learning method extensively studied deep learning method deep boltzmann machine auto encoders unsupervised representation learning proven useful several supervised task pedestrian detection object detection classi cation video domain line recent work unsupervised representation learning rst line work exploit temporal structure video learn visual representation sequence veri cation sequence sorting task second line work based frame reconstruction ranzato proposed rnn model predict missing frame future frame input video sequence srivastava extended framework lstm encoder decoder model reconstruct input sequence predict future sequence representation learning mostly capture semantic feature luo proposed unsupervised learning framework predicts future motion pair consecutive frame learned representation promising result supervised action recognition however previous work often learn view speci representation sensitive viewpoint change action recognition rgb action recognition action recognition rgb video long standing problem detailed survey found recent approach shown great progress eld generally divided category rst category focus designing handcrafted feature video representation successful example improved dense trajectory feature combined fisher vector encoding second category us deep network jointly learn feature representation classi simonyan zisserman proposed stream proposed unsupervised representation learning framework sequence input frame encoder generates sequence feature representation timestep representation used cross view decoder reconstruction decoder view classi multiple loss term jointly minimized encoder learn generate view invariant representation capture motion dynamic cnns extract spatial motion representation video frame optical ows rnn based architecture proposed model temporal information however deep network training requires amount human labeled data cnns pre trained imagenet commonly adopted backbone facilitate training avoid tting rgb action recognition since rst work action recognition depth map researcher proposed method action recognition extract feature multi modal data including depth rgb skeleton recently wang used scene calculated rgb data input action recognition state art method rgb action recognition report human level performance well established datasets msr dailyactivity however show big performance gap human existing method challenging ntu rgbd dataset contains signi cantly subject viewpoint action class background view invariant feature representation particularly challenging aspect action recognition recognize action varied unknown unseen view referred cross view action recognition literature performance existing method drop sharply viewpoint change due inherent view dependence feature used method tackle problem researcher proposed method learn representation invariant viewpoint change method create spatial temporal representation insensitive view variation method view independent latent space feature extracted different view directly comparable example rahmani used deep network project dense trajectory feature different view canonical view however previous method require access human pose information mocap data skeleton training others limited discriminative power moreover existing method skeleton based method shown effective performance cross view evaluation ntu rgbd dataset method goal unsupervised learning method learn video representation capture view invariant motion dynamic achieve training model us representation predict sequence motion multiple view motion represented dense scene ows calculated primal dual method rgb data learned representation used discriminative motion feature action recognition section rst present unsupervised learning framework followed action recognition method convbilstm convbilstm convbilstm cnndeconv encodercross view decoderdeconv reconstruction decoder view classifiergrl learning framework lxview lrecon lcls size work set overview learning framework illustrated end end deep network consists component encoder cross view decoder reconstruction decoder view classi parameterized respectively goal minimize following loss weight balance interaction loss term lxview cross view prediction loss lrecon reconstruction loss lcls view classi cation loss applied adversarial setting enhance view invariance loss term involves encoder component next explain component detail encoder let denote available view captured action encoder parameter ized take input sequence frame view denoted encodes sequence low dimensionality feature embeddings speci cally frame rst use downsampling cnn denoted conv extract low dimensionality feature size directional convolutional lstm denoted bilstm run sequence extracted conv feature timestep bilstm generates feature map size forward pas backward pas feature map concatenated channel wise form encoding compared vanilla lstms convolutional lstms replace fully connected transforma tions spatial convolution preserve spatial information intermediate representation perform much better vanilla lstms moreover directional lstm aggregate information previous frame future frame help generate richer representa tions compared lstm encoder encodes frame proposed encoder generate encoding longer sequence embodies discriminative motion dynamic action work set sequence length cross view decoder goal cross view decoder predict view cid given encoding dif cult decoder zero information view therefore give additional input decoder contains view speci information input depth map view timestep serf anchor inform decoder spatial con guration order predict view decoder still requires view invariant motion dynamic speci cally rst use cnn extract feature size extracted channel wise feature size use feature concatenated upsampling cnn denoted deconv perform spatial upsampling deconv consists fractionally strided convolutional layer batch normalization layer relu activation observe batch normalization critical optimize network let denote output cross view decoder timestep want minimize mean squared error view timestep inferring directly xview cid cid cid cid cid cid cid sequence anchor depth frame sequence ows since want learn video representation used predict motion multiple view deploy multiple cross view decoder shared parameter view therefore cross view prediction loss cid lxview xview cid reconstruction decoder goal decoder reconstruct given encoding view learning reconstruction help encoder extract basic motion used together cross view decoder enhances learning view invariant motion dynamic architecture reconstruction decoder deconv module similar cross view decoder number input channel rst layer adapted let output reconstruction decoder timestep reconstruction loss lrecon cid cid cid cid cid cid cid view classi propose view adversarial training encourages encoder learn video representation invariant view change draw inspiration domain adversarial training aim learning feature indiscriminate respect shift domain proposed view adversarial training achieved adding view classi connected encoder gradient reversal layer grl view classi try predict view encoded representation belongs whereas encoder try confuse view classi generating view invariant representation map encoding timestep probability formally view classi distribution possible view learning grl adversarial optimized increase ability discriminate encoding different view grl revers sign gradient ows back result encoder parameter learning representation reduces view classi cation accuracy essentially minimize cross entropy loss view classi cation task respect maximize respect therefore view classi cation loss sum cross entropy loss entire sequence cid log cid cid lcls ground truth view input view classi consists fully connected layer softmax layer since encoding convolutional feature rst attened vector go view classi action recognition use encoder unsupervised learning action recognition given learned representa tions sequence frame apply action classi action classi simple fully connected layer take attened vector input output score possible action class nal score sequence average score timestep action classi trained cross entropy loss training consider scenario cid scratch randomly initialize weight encoder train entire model scratch tune initialize encoder learned weight tune action recognition keep pre trained encoder xed train action classi test time uniformly sequence video sequence length average score across sampled sequence get class score video experiment unsupervised representation learning implementation detail conv encoder depth cnn cross view decoder employ resnet architecture nal convolution layer add convolutional layer reduce feature size number input channel rst convolutional layer adapted according input modality note cnn pre trained imagenet bilstm use convolutional lters size convolution input hidden state initialize weight following method training use mini batch size train model adam optimizer initial learning rate weight decay decrease learning rate half every step mini batch avoid table cross view prediction error ntu rgb dataset method proposed method lrecon lcls proposed method lcls proposed method cross subject rgb depth flow cross view depth flow rgb walking towards sitting example sequence depth input ows visualized rgb image upper row ground truth ows lower row predicted ows blue box denotes source view reconstruction whereas red box denotes cross view prediction model estimate raw motion multiple view distracting prediction task activate view adversarial training step weight loss term set determined via cross validation order effectively predict motion want describe motion low dimensional signal hence apply spatial downsampling ows calculating mean non overlapping patch resulting map multiplied keep proper scale become ground truth dataset use ntu rgb dataset unsupervised representation learning dataset consists video action class captured subject camera viewpoint viewpoint divided main view based horizontal angle camera respect subject front view left side view right side view left side degree view right side degree view view form view set used experiment action sequence simultaneously captured camera view time evaluation set standard evaluation protocol action recognition ntu rgb dataset cross subject evaluation cross view evaluation following conduct unsupervised learning experiment experiment ensure encoder trained test sample supervised learning setting cross subject evaluation follow training testing split cross view evaluation sample camera used training camera testing since need least camera unsupervised task randomly divide supervised training set ratio unsupervised training test use cross view prediction loss lxview evaluation metric table action recognition accuracy ntu rgb dataset method scratch tune lrecon view adversarial tune view adversarial tune cross subject rgb depth flow cross view depth rgb flow table comparison state art method action recognition ntu rgb dataset method hog super normal vector hon shuf learn luo lie group ftp dynamic skeleton hbrnn layer lstm lstm gca lstm ensemble lstm depth skeleton lstm modality cross subject cross view depth skeleton flow quanti performance model predict motion across different view experiment input modality rgb depth result table show quantitative result unsupervised prediction task order demonstrate effect different component loss term evaluate different variant proposed framework first train encoder cross view decoder denoted proposed method lrecon lcls add reconstruction decoder reconstruction loss denoted proposed method lcls finally add view adversarial training view classi cation loss form proposed method across input modality reconstruction view adversarial training improve cross view prediction performance comparing different input modality achieves lowest lxview expected contains view invariant motion information show qualitative example prediction depth input pair row upper row ground truth ows whereas lower row ows predicted decoder model show ability estimate raw motion multiple view encoded representation action recognition ntu rgb implementation detail experiment setting described section train model adam optimizer mini batch size learning rate weight decay set learning rate encoder tune scratch decay learning rate half every step tune since training converges faster half learning rate every step result table show classi cation accuracy cross subject cross view action recognition input modality across modality supervised learning scratch table cross subject action recognition accu racy msrdailyactivity dataset table cross view action recognition accuracy northwestern ucla dataset method actionlet ensemble hon mst aog snv hopc luo scratch tune accuracy method actionlet ensemble hankelets mst aog hopc nktm luo scratch tune accuracy lowest accuracy unsupervised learned representation training linear action classi signi cantly increase accuracy fine tuning encoder improve performance remove view adversarial training unsupervised framework accuracy would decrease especially cross view recognition among input modality input achieves highest accuracy agrees unsupervised learning flow input modality higher accuracy cross view recognition compared cross subject recognition support observation view invariant modality comparison state art table compare method state art method ntu rgb dataset rst group method use depth input second group method use skeleton input implement unsupervised learning method italic report classi cation accuracy directly cite result report map rather accuracy implementation achieve similar map depth input proposed method outperforms previous method increase accu racy signi cant cross view recognition show learned representation invariant viewpoint change input method achieves comparable performance skeleton based method however skeleton higher level feature robust viewpoint change moreover method higher cross view accuracy us explicit coordinate system transformation achieve view invariance transfer learning action recognition section perform transfer learning task use unsupervised learned representa tions action recognition datasets new domain different subject environment viewpoint perform cross subject evaluation msr dailyactivity dataset cross view evaluation northwestern ucla multiviewaction dataset experiment scratch tune setting depth modality input msr dailyactivity dataset dataset contains video action performed subject follow experimental setting video half subject training data video rest half test data northwestern ucla multiviewaction dataset dataset contains video action performed subject captured camera different view follow use video rst view training video third view test result table result comparison state art method datasets training deep model scratch give poor performance unsupervised learned representation increase accuracy margin method outperforms previous unsupervised method achieves comparable performance skeleton based method marked depth based method marked use carefully hand craft feature demonstrates learned representation generalize across domain conclusion work propose unsupervised learning framework leverage unlabeled video data multiple view learn view invariant video representation capture motion dynamic learn video representation representation source view predict ows multiple target view propose view adversarial training enhance view invariance learned representation train unsupervised framework ntu rgb dataset demonstrate effectiveness learned representation cross subject cross view action recognition task across multiple datasets proposed unsupervised learning framework naturally extended beyond action future work intend extend framework view invariant representation learning task gesture recognition person identi cation addition consider generative adversarial network gan multi view data generation acknowledgment research supported national research foundation prime minister singapore strategic capability research centre funding initiative reference bengio lamblin popovici larochelle greedy layer wise training deep network nip page bengio laufer alain yosinski deep generative stochastic network trainable backprop icml page carreira zisserman quo vadis action recognition new model kinetics dataset cvpr page cheng wan saudagar namuduri buckle advance human action recognition survey arxiv preprint arxiv doersch gupta efros unsupervised visual representation learning context prediction iccv page donahue hendricks guadarrama rohrbach venugopalan darrell saenko long term recurrent convolutional network visual recognition description cvpr page wang wang hierarchical recurrent neural network skeleton based action recognition cvpr page fernando bilen gavves gould self supervised video representation learning odd network cvpr page ganin lempitsky unsupervised domain adaptation backpropagation icml page ganin ustinova ajakan germain larochelle laviolette marchand lempitsky domain adversarial training neural network jmlr gidaris singh komodakis unsupervised representation learning predicting rotation iclr goodfellow pouget abadie mirza warde farley ozair courville bengio generative adversarial net nip page haque peng luo alahi yeung towards viewpoint invariant human pose estimation eccv page zhang ren sun delving deep rectus er surpassing human level performance imagenet classi cation iccv page zhang ren sun deep residual learning recognition cvpr page zheng lai zhang jointly learning heterogeneous feature rgb activity recognition cvpr page ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift icml page isik tacchetti poggio fast invariant representation human action visual system journal neurophysiology jaimez souiai jim nez cremers primal dual framework real time dense rgb scene icra page kingma adam method stochastic optimization iclr kong ding deeply learned view invariant feature cross view action recognition ieee trans processing building high level feature scale unsupervised learning icassp page lee huang singh yang unsupervised representation learning sorting sequence iccv page lee kim kang lee ensemble deep learning skeleton based action recognition temporal sliding lstm network iccv page camp sznaier cross view activity recognition hankelets cvpr page wong zhao kankanhalli attention transfer web image video recognition acm multimedia page wong zhao kankanhalli dual glance model deciphering social relationship iccv page zickler discriminative virtual view cross view action recognition cvpr page zhang liu action recognition based bag point cvpr page liu shahroudy wang spatio temporal lstm trust gate human action recognition eccv page liu wang duan kot global context aware attention lstm network action recognition cvpr page jia tang range depth feature action recognition cvpr page luo peng huang alahi fei fei unsupervised learning long term motion dynamic video cvpr page misra zitnick hebert shuf learn unsupervised learning temporal order veri cation eccv page hausknecht vijayanarasimhan vinyals monga toderici beyond short snippet deep network video classi cation cvpr page noroozi pirsiavash favaro representation learning learning count iccv page ohn bar trivedi joint angle similarity hog action recognition cvpr workshop page oneata verbeek schmid action event recognition sher vector compact feature set iccv page oreifej liu hon histogram oriented normal activity recognition depth sequence cvpr page parameswaran chellappa view invariance human action recognition ijcv patraucean handa cipolla spatio temporal video autoencoder differentiable memory iclr workshop rahmani bennamoun learning action recognition model depth skeleton video iccv page rahmani mahmood huynh mian histogram oriented principal component cross view action recognition ieee tpami rahmani mian action recognition novel viewpoint cvpr page rahmani mian shah learning deep model human action recognition novel viewpoint ieee tpami ranzato szlam bruna mathieu collobert chopra video language modeling baseline generative model natural video arxiv preprint arxiv salakhutdinov hinton deep boltzmann machine aistats page sermanet kavukcuoglu chintala lecun pedestrian detection unsupervised multi stage feature learning cvpr page shahroudy liu wang ntu rgb scale dataset human activity analysis cvpr page shahroudy gong wang deep multimodal feature analysis action recognition rgb video ieee tpami simonyan zisserman stream convolutional network action recognition video nip page springenberg dosovitskiy brox riedmiller striving simplicity convolutional net arxiv preprint arxiv srivastava mansimov salakhutdinov unsupervised learning video representation lstms icml page vemulapalli arrate chellappa human action recognition representing skeleton point lie group cvpr page vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders icml page wang schmid action recognition improved trajectory iccv page wang liu yuan mining actionlet ensemble action recognition depth camera cvpr page wang liu yuan learning actionlet ensemble human action recognition ieee tpami wang nie xia zhu cross view action modeling learning recognition cvpr page wang gao zhang tang ogunbona scene action map new representation rgb based action recognition convolutional neural network cvpr page wang gupta transitive invariance self supervised visual representation learning iccv page yang tian super normal vector activity recognition depth sequence cvpr page zhang lan xing zeng xue zheng view adaptive recurrent neural network high performance human action recognition skeleton data iccv page zhang wang xiao zhou liu shi cross view action recognition via continuous virtual path cvpr page']\n",
            "label           : [1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:02.256740Z",
          "iopub.status.busy": "2021-05-11T17:14:02.255857Z",
          "iopub.status.idle": "2021-05-11T17:14:02.260265Z",
          "shell.execute_reply": "2021-05-11T17:14:02.260742Z"
        },
        "papermill": {
          "duration": 0.044533,
          "end_time": "2021-05-11T17:14:02.260946",
          "exception": false,
          "start_time": "2021-05-11T17:14:02.216413",
          "status": "completed"
        },
        "tags": [],
        "id": "hindu-straight",
        "outputId": "fd430d9a-7074-47d3-b324-09ba3f1a9801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target  : tf.Tensor(71, shape=(), dtype=int32)\n",
            "context : tf.Tensor([70 16  4 40  8], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "print(\"target  :\", target)\n",
        "print(\"context :\", context)\n",
        "print(\"label   :\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.033815,
          "end_time": "2021-05-11T17:14:02.329874",
          "exception": false,
          "start_time": "2021-05-11T17:14:02.296059",
          "status": "completed"
        },
        "tags": [],
        "id": "another-directive"
      },
      "source": [
        "### Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:14:02.411067Z",
          "iopub.status.busy": "2021-05-11T17:14:02.410144Z",
          "iopub.status.idle": "2021-05-11T17:14:02.413941Z",
          "shell.execute_reply": "2021-05-11T17:14:02.413374Z"
        },
        "papermill": {
          "duration": 0.049678,
          "end_time": "2021-05-11T17:14:02.414078",
          "exception": false,
          "start_time": "2021-05-11T17:14:02.364400",
          "status": "completed"
        },
        "tags": [],
        "id": "advisory-allen"
      },
      "outputs": [],
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "        context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "        # Build context and label vectors (for one target word)\n",
        "        negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "        context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "        label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "        # Append each element from the training example to global lists.\n",
        "        targets.append(target_word)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "\n",
        "    return targets, contexts, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:15:10.840649Z",
          "iopub.status.busy": "2021-05-11T17:15:10.839854Z",
          "iopub.status.idle": "2021-05-11T17:15:10.872253Z",
          "shell.execute_reply": "2021-05-11T17:15:10.871529Z"
        },
        "papermill": {
          "duration": 0.07928,
          "end_time": "2021-05-11T17:15:10.872397",
          "exception": false,
          "start_time": "2021-05-11T17:15:10.793117",
          "status": "completed"
        },
        "tags": [],
        "id": "obvious-pursuit"
      },
      "outputs": [],
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:15:20.988281Z",
          "iopub.status.busy": "2021-05-11T17:15:20.987559Z",
          "iopub.status.idle": "2021-05-11T17:15:37.796351Z",
          "shell.execute_reply": "2021-05-11T17:15:37.795788Z"
        },
        "papermill": {
          "duration": 16.872926,
          "end_time": "2021-05-11T17:15:37.796490",
          "exception": false,
          "start_time": "2021-05-11T17:15:20.923564",
          "status": "completed"
        },
        "tags": [],
        "id": "compliant-mandate"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(docs)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "train_seq_padd = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-11T17:15:37.886465Z",
          "iopub.status.busy": "2021-05-11T17:15:37.885666Z",
          "iopub.status.idle": "2021-05-11T17:15:37.891341Z",
          "shell.execute_reply": "2021-05-11T17:15:37.890605Z"
        },
        "papermill": {
          "duration": 0.054557,
          "end_time": "2021-05-11T17:15:37.891488",
          "exception": false,
          "start_time": "2021-05-11T17:15:37.836931",
          "status": "completed"
        },
        "tags": [],
        "id": "floating-rouge",
        "outputId": "b92e762e-26a7-44ec-a93f-9296f438d2ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 523  257  685 3939 2317 2318  185    1 3940  492    1 6181  187    1\n",
            "  702 6181  187    1 6181  492    1 6181  492    1 5539    1  617  185\n",
            "    1    1  360 4705 8478  791  185 2100    1 1486    1    1  360  539\n",
            "  257  685  340   12  147  279 1015 2497  508 1139 1211 1487  396  257\n",
            "  685    9  626  193  499  326 2241  218   24   44    3  685  540   83\n",
            "  249  381  326  175  618  832  863  224  523  257  685    9  981 1488\n",
            "   18  257    9  403  136 6182  601   96 2101 1625  923  685   19  114\n",
            "  462  218 1386 1016 2498   79   88  176  967  548  271  218  325  218\n",
            "  619  235  250 3729 1788   53  669   48]\n"
          ]
        }
      ],
      "source": [
        "print(train_seq_padd[0])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}